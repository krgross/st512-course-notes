<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Multiple regression | ST 512 course notes</title>
  <meta name="description" content="Course notes for ST 512, Statistical Methods for Researchers II." />
  <meta name="generator" content="bookdown 0.30 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Multiple regression | ST 512 course notes" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Course notes for ST 512, Statistical Methods for Researchers II." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Multiple regression | ST 512 course notes" />
  
  <meta name="twitter:description" content="Course notes for ST 512, Statistical Methods for Researchers II." />
  

<meta name="author" content="Kevin Gross" />


<meta name="date" content="2023-02-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="simple-linear-regression.html"/>
<link rel="next" href="non-linear-regression-models.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#philosophy"><i class="fa fa-check"></i>Philosophy</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#scope-and-coverage"><i class="fa fa-check"></i>Scope and coverage</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#mathematical-level"><i class="fa fa-check"></i>Mathematical level</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#computing"><i class="fa fa-check"></i>Computing</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#format-of-the-notes"><i class="fa fa-check"></i>Format of the notes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments-and-license"><i class="fa fa-check"></i>Acknowledgments and license</a></li>
</ul></li>
<li class="part"><span><b>Part I: Regression modeling</b></span></li>
<li class="chapter" data-level="1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>1</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-basics-of-slr"><i class="fa fa-check"></i><b>1.1</b> The basics of SLR</a></li>
<li class="chapter" data-level="1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#least-squares-estimation"><i class="fa fa-check"></i><b>1.2</b> Least-squares estimation</a></li>
<li class="chapter" data-level="1.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#inference-for-the-slope"><i class="fa fa-check"></i><b>1.3</b> Inference for the slope</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#standard-errors"><i class="fa fa-check"></i><b>1.3.1</b> Standard errors</a></li>
<li class="chapter" data-level="1.3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#confidence-intervals"><i class="fa fa-check"></i><b>1.3.2</b> Confidence intervals</a></li>
<li class="chapter" data-level="1.3.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#statistical-hypothesis-tests"><i class="fa fa-check"></i><b>1.3.3</b> Statistical hypothesis tests</a></li>
<li class="chapter" data-level="1.3.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#inference-for-the-intercept"><i class="fa fa-check"></i><b>1.3.4</b> Inference for the intercept</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#sums-of-squares-decomposition-and-r2"><i class="fa fa-check"></i><b>1.4</b> Sums of squares decomposition and <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="1.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#fitting-the-slr-model-in-r"><i class="fa fa-check"></i><b>1.5</b> Fitting the SLR model in R</a></li>
<li class="chapter" data-level="1.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#diagnostic-plots"><i class="fa fa-check"></i><b>1.6</b> Diagnostic plots</a></li>
<li class="chapter" data-level="1.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#consequences-of-violating-model-assumptions-and-possible-fixes"><i class="fa fa-check"></i><b>1.7</b> Consequences of violating model assumptions, and possible fixes</a></li>
<li class="chapter" data-level="1.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#prediction-with-regression-models"><i class="fa fa-check"></i><b>1.8</b> Prediction with regression models</a></li>
<li class="chapter" data-level="1.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#regression-design"><i class="fa fa-check"></i><b>1.9</b> Regression design</a></li>
<li class="chapter" data-level="1.10" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#centering-the-predictor"><i class="fa fa-check"></i><b>1.10</b> <span class="math inline">\(^\star\)</span>Centering the predictor</a></li>
<li class="chapter" data-level="" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#appendix-regression-models-in-sas-proc-reg"><i class="fa fa-check"></i>Appendix: Regression models in SAS PROC REG</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#multiple-regression-basics"><i class="fa fa-check"></i><b>2.1</b> Multiple regression basics</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="multiple-regression.html"><a href="multiple-regression.html#the-multiple-regression-model"><i class="fa fa-check"></i><b>2.1.1</b> The multiple regression model</a></li>
<li class="chapter" data-level="2.1.2" data-path="multiple-regression.html"><a href="multiple-regression.html#interpreting-partial-regression-coefficients."><i class="fa fa-check"></i><b>2.1.2</b> Interpreting partial regression coefficients.</a></li>
<li class="chapter" data-level="2.1.3" data-path="multiple-regression.html"><a href="multiple-regression.html#visualizing-a-multiple-regression-model"><i class="fa fa-check"></i><b>2.1.3</b> Visualizing a multiple regression model</a></li>
<li class="chapter" data-level="2.1.4" data-path="multiple-regression.html"><a href="multiple-regression.html#statistical-inference-for-partial-regression-coefficients"><i class="fa fa-check"></i><b>2.1.4</b> Statistical inference for partial regression coefficients</a></li>
<li class="chapter" data-level="2.1.5" data-path="multiple-regression.html"><a href="multiple-regression.html#prediction"><i class="fa fa-check"></i><b>2.1.5</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#F-test"><i class="fa fa-check"></i><b>2.2</b> <span class="math inline">\(F\)</span>-tests for several regression coefficients</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#model-utility-tests"><i class="fa fa-check"></i><b>2.2.1</b> Model utility tests</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="multiple-regression.html"><a href="multiple-regression.html#categorical-predictors"><i class="fa fa-check"></i><b>2.3</b> Categorical predictors</a></li>
<li class="chapter" data-level="2.4" data-path="multiple-regression.html"><a href="multiple-regression.html#regression-interactions"><i class="fa fa-check"></i><b>2.4</b> Interactions between predictors</a></li>
<li class="chapter" data-level="2.5" data-path="multiple-regression.html"><a href="multiple-regression.html#collinearity"><i class="fa fa-check"></i><b>2.5</b> (Multi-)Collinearity</a></li>
<li class="chapter" data-level="2.6" data-path="multiple-regression.html"><a href="multiple-regression.html#variable-selection-choosing-the-best-model"><i class="fa fa-check"></i><b>2.6</b> Variable selection: Choosing the best model</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="multiple-regression.html"><a href="multiple-regression.html#model-selection-and-inference"><i class="fa fa-check"></i><b>2.6.1</b> Model selection and inference</a></li>
<li class="chapter" data-level="2.6.2" data-path="multiple-regression.html"><a href="multiple-regression.html#ranking-methods"><i class="fa fa-check"></i><b>2.6.2</b> Ranking methods</a></li>
<li class="chapter" data-level="2.6.3" data-path="multiple-regression.html"><a href="multiple-regression.html#cross-validation"><i class="fa fa-check"></i><b>2.6.3</b> Cross-validation</a></li>
<li class="chapter" data-level="2.6.4" data-path="multiple-regression.html"><a href="multiple-regression.html#sequential-methods"><i class="fa fa-check"></i><b>2.6.4</b> Sequential methods</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="multiple-regression.html"><a href="multiple-regression.html#leverage-influential-points-and-standardized-residuals"><i class="fa fa-check"></i><b>2.7</b> Leverage, influential points, and standardized residuals</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="multiple-regression.html"><a href="multiple-regression.html#leverage"><i class="fa fa-check"></i><b>2.7.1</b> Leverage</a></li>
<li class="chapter" data-level="2.7.2" data-path="multiple-regression.html"><a href="multiple-regression.html#standardized-residuals"><i class="fa fa-check"></i><b>2.7.2</b> Standardized residuals</a></li>
<li class="chapter" data-level="2.7.3" data-path="multiple-regression.html"><a href="multiple-regression.html#cooks-distance"><i class="fa fa-check"></i><b>2.7.3</b> Cook’s distance</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="multiple-regression.html"><a href="multiple-regression.html#appendix-regression-as-a-linear-algebra-problem"><i class="fa fa-check"></i>Appendix: Regression as a linear algebra problem</a>
<ul>
<li class="chapter" data-level="" data-path="multiple-regression.html"><a href="multiple-regression.html#singular-or-pathological-design-matrices"><i class="fa fa-check"></i>Singular, or pathological, design matrices</a></li>
<li class="chapter" data-level="" data-path="multiple-regression.html"><a href="multiple-regression.html#additional-results"><i class="fa fa-check"></i>Additional results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="non-linear-regression-models.html"><a href="non-linear-regression-models.html"><i class="fa fa-check"></i><b>3</b> Non-linear regression models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="non-linear-regression-models.html"><a href="non-linear-regression-models.html#polynomial-regression"><i class="fa fa-check"></i><b>3.1</b> Polynomial regression</a></li>
<li class="chapter" data-level="3.2" data-path="non-linear-regression-models.html"><a href="non-linear-regression-models.html#nls"><i class="fa fa-check"></i><b>3.2</b> Non-linear least squares</a></li>
<li class="chapter" data-level="3.3" data-path="non-linear-regression-models.html"><a href="non-linear-regression-models.html#starsmoothing-methods"><i class="fa fa-check"></i><b>3.3</b> <em><span class="math inline">\(^\star\)</span>Smoothing methods</em></a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="non-linear-regression-models.html"><a href="non-linear-regression-models.html#loess-smoothers"><i class="fa fa-check"></i><b>3.3.1</b> Loess smoothers</a></li>
<li class="chapter" data-level="3.3.2" data-path="non-linear-regression-models.html"><a href="non-linear-regression-models.html#splines"><i class="fa fa-check"></i><b>3.3.2</b> Splines</a></li>
<li class="chapter" data-level="3.3.3" data-path="non-linear-regression-models.html"><a href="non-linear-regression-models.html#generalized-additive-models-gams"><i class="fa fa-check"></i><b>3.3.3</b> Generalized additive models (GAMs)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part II: Designed experiments</b></span></li>
<li class="chapter" data-level="4" data-path="one-factor-anova.html"><a href="one-factor-anova.html"><i class="fa fa-check"></i><b>4</b> One-factor ANOVA</a>
<ul>
<li class="chapter" data-level="4.1" data-path="one-factor-anova.html"><a href="one-factor-anova.html#grouped-data-and-the-design-of-experiments-doe-an-overview"><i class="fa fa-check"></i><b>4.1</b> Grouped data and the design of experiments (DoE): an overview</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="one-factor-anova.html"><a href="one-factor-anova.html#a-vocabulary-for-describing-designed-experiments"><i class="fa fa-check"></i><b>4.1.1</b> A vocabulary for describing designed experiments</a></li>
<li class="chapter" data-level="4.1.2" data-path="one-factor-anova.html"><a href="one-factor-anova.html#roadmap"><i class="fa fa-check"></i><b>4.1.2</b> Roadmap</a></li>
<li class="chapter" data-level="4.1.3" data-path="one-factor-anova.html"><a href="one-factor-anova.html#the-simplest-experiment"><i class="fa fa-check"></i><b>4.1.3</b> The simplest experiment</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="one-factor-anova.html"><a href="one-factor-anova.html#one-factor-anova-the-basics"><i class="fa fa-check"></i><b>4.2</b> One-factor ANOVA: The basics</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="one-factor-anova.html"><a href="one-factor-anova.html#connections-between-one-factor-anova-and-other-statistical-procedures"><i class="fa fa-check"></i><b>4.2.1</b> Connections between one-factor ANOVA and other statistical procedures</a></li>
<li class="chapter" data-level="4.2.2" data-path="one-factor-anova.html"><a href="one-factor-anova.html#assumptions-in-anova"><i class="fa fa-check"></i><b>4.2.2</b> Assumptions in ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="one-factor-anova.html"><a href="one-factor-anova.html#linear-contrasts-of-group-means"><i class="fa fa-check"></i><b>4.3</b> Linear contrasts of group means</a></li>
<li class="chapter" data-level="4.4" data-path="one-factor-anova.html"><a href="one-factor-anova.html#using-sas-the-effects-parameterization-of-the-one-factor-anova"><i class="fa fa-check"></i><b>4.4</b> Using SAS: The effects parameterization of the one-factor ANOVA</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="one-factor-anova.html"><a href="one-factor-anova.html#effects-model-parameterization-of-the-one-factor-anova-model"><i class="fa fa-check"></i><b>4.4.1</b> Effects-model parameterization of the one-factor ANOVA model</a></li>
<li class="chapter" data-level="4.4.2" data-path="one-factor-anova.html"><a href="one-factor-anova.html#sas-implementation-of-the-one-factor-anova-model-in-proc-glm"><i class="fa fa-check"></i><b>4.4.2</b> SAS implementation of the one-factor ANOVA model in PROC GLM</a></li>
<li class="chapter" data-level="4.4.3" data-path="one-factor-anova.html"><a href="one-factor-anova.html#using-the-estimate-and-contrast-statements-for-linear-contrasts-in-proc-glm"><i class="fa fa-check"></i><b>4.4.3</b> Using the ESTIMATE and CONTRAST statements for linear contrasts in PROC GLM</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="one-factor-anova.html"><a href="one-factor-anova.html#linear-contrasts-revisited-testing-multiple-simultaneous-contrasts"><i class="fa fa-check"></i><b>4.5</b> Linear contrasts revisited: Testing multiple simultaneous contrasts</a></li>
<li class="chapter" data-level="4.6" data-path="one-factor-anova.html"><a href="one-factor-anova.html#multiple-comparisons"><i class="fa fa-check"></i><b>4.6</b> Multiple comparisons</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="one-factor-anova.html"><a href="one-factor-anova.html#multiple-testing-in-general"><i class="fa fa-check"></i><b>4.6.1</b> Multiple testing in general</a></li>
<li class="chapter" data-level="4.6.2" data-path="one-factor-anova.html"><a href="one-factor-anova.html#bonferroni-and-bonferroni-like-procedures"><i class="fa fa-check"></i><b>4.6.2</b> Bonferroni and Bonferroni-like procedures</a></li>
<li class="chapter" data-level="4.6.3" data-path="one-factor-anova.html"><a href="one-factor-anova.html#multiple-comparisons-in-anova"><i class="fa fa-check"></i><b>4.6.3</b> Multiple comparisons in ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="one-factor-anova.html"><a href="one-factor-anova.html#general-strategy-for-analyzing-data-from-a-crd-with-a-one-factor-treatment-structure"><i class="fa fa-check"></i><b>4.7</b> General strategy for analyzing data from a CRD with a one-factor treatment structure</a></li>
<li class="chapter" data-level="4.8" data-path="one-factor-anova.html"><a href="one-factor-anova.html#advanced-topic-power-and-sample-size-determination-in-anova"><i class="fa fa-check"></i><b>4.8</b> Advanced topic: Power and sample-size determination in ANOVA</a></li>
<li class="chapter" data-level="4.9" data-path="one-factor-anova.html"><a href="one-factor-anova.html#advanced-topic-orthogonal-contrasts"><i class="fa fa-check"></i><b>4.9</b> Advanced topic: Orthogonal contrasts</a></li>
<li class="chapter" data-level="4.10" data-path="one-factor-anova.html"><a href="one-factor-anova.html#advanced-topic-polynomial-trends"><i class="fa fa-check"></i><b>4.10</b> Advanced topic: Polynomial trends</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">ST 512 course notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multiple-regression" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Multiple regression<a href="multiple-regression.html#multiple-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this chapter, we examine regression models that contain several predictor variables. Thankfully, many of the ideas from the simple linear regression also apply to regression models with several predictors. After an overview of the basics, this chapter will focus on the new aspects of regression modeling that arise when considering several predictors.</p>
<div id="multiple-regression-basics" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Multiple regression basics<a href="multiple-regression.html#multiple-regression-basics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="the-multiple-regression-model" class="section level3 hasAnchor" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> The multiple regression model<a href="multiple-regression.html#the-multiple-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Just as SLR was used to characterize the relationship between a single predictor and a response, multiple regression can be used to characterize the relationship between several predictors and a response.</p>
<p><em>Example.</em> In the BAC data, we also know each individual’s weight and gender:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="multiple-regression.html#cb42-1" aria-hidden="true" tabindex="-1"></a>beer <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;data/beer2.csv&quot;</span>, <span class="at">head =</span> T, <span class="at">stringsAsFactors =</span> T)</span>
<span id="cb42-2"><a href="multiple-regression.html#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(beer)</span></code></pre></div>
<pre><code>##     BAC weight beers
## 1 0.100    132     5
## 2 0.030    128     2
## 3 0.190    110     9
## 4 0.120    192     8
## 5 0.040    172     3
## 6 0.095    250     7</code></pre>
<p>A plot of the residuals from the BAC vs. beers consumed model against weight strongly suggests that some of the variation in BAC is attributable to differences in weight:</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="multiple-regression.html#cb44-1" aria-hidden="true" tabindex="-1"></a>fm1 <span class="ot">&lt;-</span> <span class="fu">with</span>(beer, <span class="fu">lm</span>(BAC <span class="sc">~</span> beers))</span>
<span id="cb44-2"><a href="multiple-regression.html#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> beer<span class="sc">$</span>weight, <span class="at">y =</span> <span class="fu">resid</span>(fm1), <span class="at">xlab =</span> <span class="st">&quot;weight&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;residual&quot;</span>)</span>
<span id="cb44-3"><a href="multiple-regression.html#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">lty =</span> <span class="st">&quot;dotted&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-2"></span>
<img src="02-MultipleRegression_files/figure-html/unnamed-chunk-2-1.png" alt="SLR residuals vs. weight." width="480" />
<p class="caption">
Figure 1.1: SLR residuals vs. weight.
</p>
</div>
<p>To simultaneously characterize the effect that the variables “beers” and “weight” have on BAC, we might want to entertain a model with both predictors. In words, the model is
<span class="math display">\[
\mbox{BAC} = \mbox{intercept} + \mbox{(parameter associated with beers)} \times \mbox{beers}  + \mbox{(parameter associated with weight)} \times \mbox{weight} + \mbox{error}
\]</span>
where (for the moment) we are intentionally vague about what we mean by “parameter associated with beers”. As in SLR, the error term can be interpreted as a catch-all term that includes all the variation not accounted for by the linear associations betwen the response and the predictors “beers” and “weight”.</p>
<p>In mathematical notation, we can write the model as
<span class="math display" id="eq:beer-two-predictors">\[\begin{equation}
y = \beta_0 +\beta_1 x_1 +\beta_2 x_2 +\varepsilon
\tag{2.1}
\end{equation}\]</span>
We use subscripts to distinguish different predictors. In this case, <span class="math inline">\(x_1\)</span> is the number of beers consumed and <span class="math inline">\(x_2\)</span> is the individual’s weight. Of course, the order in which we designate the predictors is arbitrary.</p>
<p>There are a variety of ways to think about this model. As in SLR, we can separate this model into a mean or signal component <span class="math inline">\(\beta_0 +\beta_1 x_1 +\beta_2 x_2\)</span> and an error component <span class="math inline">\(\varepsilon\)</span>. Note that the mean component is now a function of two variables, and suggests that the relationship between the average response and either predictor is linear. If we wish to make statistical inferences about the parameters <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> (which we do), then we need to place the standard assumptions on the error component: independence, constant variance, and normality. In notation, <span class="math inline">\(\varepsilon \sim \mathcal{N}\left(0,\sigma_{\varepsilon}^2 \right)\)</span>.</p>
<p>We can also think about this model geometrically. Recall that in SLR, we could interpret the SLR model as a line passing through a cloud of data points. With 2 predictors, we are now fitting a plane to data points that “exist” in a three- dimensional data cloud.
<!-- ```{r fig.height = 3, echo = FALSE} -->
<!-- knitr::include_graphics("images/regression-schematic.png", dpi = 100) -->
<!-- ``` -->
As in SLR, we use the least squares criteria to find the best-fitting parameter estimates. That is to say, we will agree that the best estimates of the parameters <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are the values that minimize
<span class="math display">\[\begin{eqnarray*}
SSE &amp; = &amp; \sum_{i=1}^n e_i^2 \\
&amp; = &amp; \sum_{i=1}^n \left(y_i -\hat{y}_i \right)^2 \\
&amp; = &amp; \sum_{i=1}^n\left(y_i -\left[\hat{\beta}_0 +\hat{\beta}_1 x_{i1} +\hat{\beta}_{2} x_{i2} \right]\right)^2  
\end{eqnarray*}\]</span>
In R, we can fit this model by adding a new term to the right-hand side of the model formula in the call to the function ‘lm’:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="multiple-regression.html#cb45-1" aria-hidden="true" tabindex="-1"></a>fm2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(BAC <span class="sc">~</span> beers <span class="sc">+</span> weight, <span class="at">data =</span> beer)</span>
<span id="cb45-2"><a href="multiple-regression.html#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fm2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = BAC ~ beers + weight, data = beer)
## 
## Residuals:
##        Min         1Q     Median         3Q        Max 
## -0.0162968 -0.0067796  0.0003985  0.0085287  0.0155621 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  3.986e-02  1.043e-02   3.821  0.00212 ** 
## beers        1.998e-02  1.263e-03  15.817 7.16e-10 ***
## weight      -3.628e-04  5.668e-05  -6.401 2.34e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.01041 on 13 degrees of freedom
## Multiple R-squared:  0.9518, Adjusted R-squared:  0.9444 
## F-statistic: 128.3 on 2 and 13 DF,  p-value: 2.756e-09</code></pre>
<p>Thus, we see that the LSEs are <span class="math inline">\(\hat{\beta}_0 = 0.040\%\)</span>, <span class="math inline">\(\hat{\beta}_1 = 0.020\%\)</span> per beer consumed, and <span class="math inline">\(\hat{\beta}_{2} = -0.0003\%\)</span> per pound of body weight.</p>
<p>As in SLR, we can define the fitted value associated with the <span class="math inline">\(i\)</span>th data point as <span class="math inline">\(\hat{y}_i =\hat{\beta}_0 +\hat{\beta}_1 x_{i1} +\hat{\beta}_{2} x_{i2}\)</span>, and the residual associated with the <span class="math inline">\(i\)</span>th data point as <span class="math inline">\(e_i =y_i -\hat{y}_i\)</span>. Here, we require a double subscripting of the <span class="math inline">\(x\)</span>’s, with the first subscript is used to distinguish the individual observations and the second subscript is used to distinguish different predictors. For example, <span class="math inline">\(x_{i2}\)</span> is the value of the second predictor for the <span class="math inline">\(i\)</span>th data point.</p>
<p><em>Example.</em> Find the fitted value and residual for the first observation in the data set, a <span class="math inline">\(x_2=132\)</span> lb person who drank <span class="math inline">\(x_1=5\)</span> beers and had a BAC of <span class="math inline">\(y=0.1\)</span>. Answer: <span class="math inline">\(\hat{y}_1 =0.092\)</span> and <span class="math inline">\(e_1 =0.008\)</span>.</p>
<p>We can define the error sum of squares as <span class="math inline">\(SSE=\sum_{i=1}^n e_i^2 = \sum_{i=1}^n \left(y_i -\hat{y}_i \right)^2\)</span>. How many df are associated with the SSE? In this model, there are <span class="math inline">\(n-3\)</span> df associated with the SSE, because 3 parameters are needed to determine the mean component of the model. As in SLR, we can estimate the error variance <span class="math inline">\(\sigma_{\varepsilon}^2\)</span> with the MSE, although now we must be careful to divide by the appropriate df:
<span class="math display">\[
s_\varepsilon^2 = MSE = \frac{SSE}{n-3}.
\]</span>
For the model above, <span class="math inline">\(s_\varepsilon = 0.010\%\)</span>.</p>
<p>In general, the equation for an MLR model with any number of predictors can be written:<br />
<span class="math display">\[
y_i =\beta_0 +\beta_1 x_{i1} +\beta_2 x_{i2} +\ldots +\beta_k x_{ik} +\varepsilon_i
\]</span>
The error term is subject to the standard assumptions of independence, constant variance, and normality. We will use the notation that <span class="math inline">\(k\)</span> is the number of parameters that need to be estimated in the mean component of the model excluding the intercept. (When counting parameters, some texts include the intercept, while others do not. If you consult a text, check to make sure you know what definition is being used.) The SSE will be associated with <span class="math inline">\(n - (k + 1)\)</span> df. Thus, the estimate of <span class="math inline">\(\sigma_{\varepsilon}^2\)</span> will be
<span class="math display">\[
s_\varepsilon^2 = MSE = \frac{SSE}{n-(k+1)}.
\]</span></p>
<div id="sums-of-squares-decomposition-and-r2." class="section level4 hasAnchor" number="2.1.1.1">
<h4><span class="header-section-number">2.1.1.1</span> Sums of squares decomposition and <span class="math inline">\(R^2\)</span>.<a href="multiple-regression.html#sums-of-squares-decomposition-and-r2." class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The sums-of-squares decomposition also carries over from SLR. We still have <span class="math inline">\({\rm SS(Total)} = \sum_{i=1}^n \left(y_i -\bar{y}\right)^2\)</span>, <span class="math inline">\({\rm SS(Regression)} = \sum_{i=1}^n \left(\hat{y}_i -\bar{y}\right)^2\)</span>, and <span class="math inline">\({\rm SS(Total) = SS(Regression) + SSE}\)</span>. Thus, we can define <span class="math inline">\(R^2\)</span> using the same formula:
<span class="math display">\[
R^2 = \frac{{\rm SS(Regression)}}{{\rm SS(Total)}} = 1- \frac{{\rm SSE}}{{\rm SS(Total)}}
\]</span>
We still interpret <span class="math inline">\(R^2\)</span> as a measure of the proportion of variability in the response that is explained by the regression model. In the BAC example, <span class="math inline">\(R^2=0.952\)</span>.</p>
</div>
</div>
<div id="interpreting-partial-regression-coefficients." class="section level3 hasAnchor" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Interpreting partial regression coefficients.<a href="multiple-regression.html#interpreting-partial-regression-coefficients." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <span class="math inline">\(\hat{\beta}\)</span>’s in a MLR model are called partial regression coefficients (or partial regression slopes). Their interpretation is subtly different from SLR regression slopes. Misinterpretation of partial regression coefficients is one of the most common sources of statistical confusion in the scientific literature.</p>
<p>We can interpret the partial regression coefficients geometrically. In this interpretation, <span class="math inline">\(\beta_j\)</span> is the slope of the regression plane in the direction of the predictor <span class="math inline">\(x_j\)</span>. Imagine taking a “slice” of the regression plane. In the terminology of calculus, <span class="math inline">\(\beta_j\)</span> is also the partial derivative of the regression plane with respect to the predictor <span class="math inline">\(x_j\)</span> (hence the term “partial regression coefficient”).</p>
<p>Another ways to express this same idea in everyday language is that <span class="math inline">\(\beta_j\)</span> quantifies the linear association between predictor <span class="math inline">\(j\)</span> and the response when the other predictors are held constant, or while controlling for the effects of the other predictors. This is different from an SLR slope, which we can interpret as the slope of the linear association between <span class="math inline">\(y\)</span> and <span class="math inline">\(x_j\)</span> while ignoring all other predictors. Thus, the interpretation of a partial regression coefficient <em>depends on the model in which the parameter is found</em>.</p>
<p>Compare the estimated regression coefficients for the number of beers consumed in the SLR model and the MLR model that includes weight.</p>
<ul>
<li>Estimated SLR coefficient for no. of beers consumed: 0.018</li>
<li>Estimated MLR coefficient for no. of beers consumed: 0.020</li>
</ul>
<p>The coefficients differ because they estimate different parameters that mean different things. The SLR coefficient estimates a slope that does not account for the effect of weight, while the MLR coefficient estimates a slope that does account for the effect of weight.</p>
<p><span class="citation">Gelman, Hill, and Vehtari (<a href="#ref-gelman2020regression" role="doc-biblioref">2020</a>)</span>‘s interpretation of regression coefficients extends nicely here. Recall that we interpreted the SLR slope as saying that if we compare two individuals who consume different numbers of beers (call those values <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>), then the average difference in the individuals’ BAC equals <span class="math inline">\(0.018 \times (x_1 - x_2)\)</span>. In a multiple regression context, we would now say that if we compare two individuals <em>who weigh the same</em> but who consume different numbers of beers, then the average difference in the individuals’ BAC equals <span class="math inline">\(0.020 \times (x_1 - x_2)\)</span>. Thus, the partial regression coefficient tells us something different than the simple regression coefficient. Therefore, we are not surprised that the values differ.</p>
<p>As a final point, note that our interpretation of the partial regression coefficient 0.020 above does not depend on the particular weight of the two individuals that we are comparing; all that matters is that the two individuals compared weigh the same. Thus, if we are comparing two 100-pound individuals, one of whom has comsumed 1 beer and the other who has consumed 3 beers, then we estimate that the person who drank 3 beers would have a BAC 0.040 larger than the person who drank 1 beer. Under the current model, this would also be true if we compared two 250-point individuals, one of whom had comsumed 1 beer and the other who had consumed 3 beers. All that matters, so far, is that the individuals to be compared weigh the same. That said, knowing what we do about human physiology, we might that this value should differ depending on whether we are comparing two 100-pound individuals or two 250-pound individuals. This idea — that the association between one predictor and the response depends on the value of another predictor — is the idea of a <em>statistical interaction</em>. We will encounter interactions soon.</p>
<p>Here’s another example from the world of food science. As cheese ages, various chemical processes take place that determine the taste of the final product. These data are from the <span class="citation">Moore and McCabe (<a href="#ref-moore1989introduction" role="doc-biblioref">1989</a>)</span>. The response variable is the taste scores averaged from several tasters. There are three predictors that describe the chemical content of the cheese. They are:</p>
<ul>
<li>acetic: the natural log of acetic acid concentration</li>
<li>h2s: the natural log of hydrogen sulfide concentration</li>
<li>lactic: the concentration of lactic acid</li>
</ul>
<p>Here is a “pairs plot” of the data. In this plot, each panel is a scatterplot showing the relationship between two of the four variables in the model. Pairs plots are useful ways to gain a quick grasp of the structure in the data and how the constituent variables are related to one another.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="multiple-regression.html#cb47-1" aria-hidden="true" tabindex="-1"></a>cheese <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="st">&quot;data/cheese.txt&quot;</span>, <span class="at">head =</span> T, <span class="at">stringsAsFactors =</span> T)</span>
<span id="cb47-2"><a href="multiple-regression.html#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(cheese)</span></code></pre></div>
<p><img src="02-MultipleRegression_files/figure-html/unnamed-chunk-4-1.png" width="480" style="display: block; margin: auto;" />
Let’s entertain a model that uses all three predictors.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="multiple-regression.html#cb48-1" aria-hidden="true" tabindex="-1"></a>cheese_regression <span class="ot">&lt;-</span> <span class="fu">lm</span>(taste <span class="sc">~</span> Acetic <span class="sc">+</span> H2S <span class="sc">+</span> Lactic, <span class="at">data =</span> cheese)</span>
<span id="cb48-2"><a href="multiple-regression.html#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(cheese_regression)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = taste ~ Acetic + H2S + Lactic, data = cheese)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -17.390  -6.612  -1.009   4.908  25.449 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept) -28.8768    19.7354  -1.463  0.15540   
## Acetic        0.3277     4.4598   0.073  0.94198   
## H2S           3.9118     1.2484   3.133  0.00425 **
## Lactic       19.6705     8.6291   2.280  0.03108 * 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 10.13 on 26 degrees of freedom
## Multiple R-squared:  0.6518, Adjusted R-squared:  0.6116 
## F-statistic: 16.22 on 3 and 26 DF,  p-value: 3.81e-06</code></pre>
<p>Compare this MLR model with each of the three possible SLR models:</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="multiple-regression.html#cb50-1" aria-hidden="true" tabindex="-1"></a>slr1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(taste <span class="sc">~</span> Acetic, <span class="at">data =</span> cheese)</span>
<span id="cb50-2"><a href="multiple-regression.html#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(slr1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = taste ~ Acetic, data = cheese)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -29.642  -7.443   2.082   6.597  26.581 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)  -61.499     24.846  -2.475  0.01964 * 
## Acetic        15.648      4.496   3.481  0.00166 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 13.82 on 28 degrees of freedom
## Multiple R-squared:  0.302,  Adjusted R-squared:  0.2771 
## F-statistic: 12.11 on 1 and 28 DF,  p-value: 0.001658</code></pre>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="multiple-regression.html#cb52-1" aria-hidden="true" tabindex="-1"></a>slr2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(taste <span class="sc">~</span> H2S, <span class="at">data =</span> cheese)</span>
<span id="cb52-2"><a href="multiple-regression.html#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(slr2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = taste ~ H2S, data = cheese)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -15.426  -7.611  -3.491   6.420  25.687 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -9.7868     5.9579  -1.643    0.112    
## H2S           5.7761     0.9458   6.107 1.37e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 10.83 on 28 degrees of freedom
## Multiple R-squared:  0.5712, Adjusted R-squared:  0.5558 
## F-statistic: 37.29 on 1 and 28 DF,  p-value: 1.374e-06</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="multiple-regression.html#cb54-1" aria-hidden="true" tabindex="-1"></a>slr3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(taste <span class="sc">~</span> Lactic, <span class="at">data =</span> cheese)</span>
<span id="cb54-2"><a href="multiple-regression.html#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(slr3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = taste ~ Lactic, data = cheese)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -19.9439  -8.6839  -0.1095   8.9998  27.4245 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -29.859     10.582  -2.822  0.00869 ** 
## Lactic        37.720      7.186   5.249 1.41e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 11.75 on 28 degrees of freedom
## Multiple R-squared:  0.4959, Adjusted R-squared:  0.4779 
## F-statistic: 27.55 on 1 and 28 DF,  p-value: 1.405e-05</code></pre>
<p>What do you make of the fact that an SLR analysis suggests that there is a (statistically significant) positive relationship between acetic acid concentration and taste, yet the partial regression coefficient associated with acetic acid concentration is not statistically significant in the MLR model?</p>
<hr />
<p><span style="color: gray;"> The fact that the interpretation of regression parameters depends on the context of the model in which they are found cannot be overemphasized. I speculate that much of the confusion surrounding this point flows from the arguably deficient notation that we use to write regression models. Consider the beer data. When we compare the simple regression model <span class="math inline">\(y = \beta_0 + \beta_1 x_1 + \varepsilon\)</span> and the multiple regression model <span class="math inline">\(y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon\)</span>, it seems natural to conclude that the parameter <span class="math inline">\(\beta_1\)</span> means the same thing in both models. But the parameters differ, as we have seen. In a different world, we might imagine notation that makes this context-dependence explicit, by writing something like <span class="math inline">\(\beta_1(x_1)\)</span> for the regression coefficient associated with <span class="math inline">\(x_1\)</span> in a model that includes only <span class="math inline">\(x_1\)</span>, and <span class="math inline">\(\beta_1(x_1, x_2)\)</span> for the regression coefficient associated with <span class="math inline">\(x_1\)</span> in a model that includes both <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. But such notation would quickly become unwieldy. So we are stuck with the notation that we have, and must avoid the confusion that it can create.</span></p>
<hr />
</div>
<div id="visualizing-a-multiple-regression-model" class="section level3 hasAnchor" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> Visualizing a multiple regression model<a href="multiple-regression.html#visualizing-a-multiple-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>How do you visualize a multiple regression model? With two predictors (as in our current BAC model), the regression fit is a (rigid) plane in three-dimensional space. So, we have a fighting chance of making a picture of the model fit if we have good graphics software handy. However, once we enocunter models with more than two predictors, the regression fits become increasingly high-dimensional objects that we humans won’t be able to visualize in full. So the best we can do in general is to think about the bivariate relationship implied between each predictor and the response.</p>
<p>We’ll continue to use the BAC model as an example. For starters, we might consider plotting the implied relationship between the predicted response and a predictor when the other predictors are set at their average value. In the BAC model, that means plotting the relationship between the predicted BAC and beers consumed for an individual of average weight, where by average weight we mean equal to the average weight of the individuals in the data set. We can couple this with a plot of predicted BAC vs weight for an individual who has consumed an average number of beers, where again we determine this average based on the average value that appears in the data set. Here are those plots for the BAC model:</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="multiple-regression.html#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb56-2"><a href="multiple-regression.html#cb56-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-3"><a href="multiple-regression.html#cb56-3" aria-hidden="true" tabindex="-1"></a>b.hat <span class="ot">&lt;-</span> <span class="fu">coefficients</span>(fm2)  <span class="co"># extract LSEs of regression coefficients</span></span>
<span id="cb56-4"><a href="multiple-regression.html#cb56-4" aria-hidden="true" tabindex="-1"></a>avg.beers <span class="ot">&lt;-</span> <span class="fu">mean</span>(beer<span class="sc">$</span>beers)</span>
<span id="cb56-5"><a href="multiple-regression.html#cb56-5" aria-hidden="true" tabindex="-1"></a>avg.weight <span class="ot">&lt;-</span> <span class="fu">mean</span>(beer<span class="sc">$</span>weight)</span>
<span id="cb56-6"><a href="multiple-regression.html#cb56-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-7"><a href="multiple-regression.html#cb56-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(BAC <span class="sc">~</span> beers, <span class="at">type =</span> <span class="st">&quot;n&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;beers consumed&quot;</span>, <span class="at">data =</span> beer)  <span class="co"># set up axes</span></span>
<span id="cb56-8"><a href="multiple-regression.html#cb56-8" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> b.hat[<span class="dv">1</span>] <span class="sc">+</span> b.hat[<span class="dv">3</span>] <span class="sc">*</span> avg.weight, <span class="at">b =</span> b.hat[<span class="dv">2</span>])  <span class="co"># predicted BAC for average weight individual</span></span>
<span id="cb56-9"><a href="multiple-regression.html#cb56-9" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="at">leg =</span> <span class="fu">paste</span>(<span class="st">&quot;weight = &quot;</span>, <span class="fu">round</span>(avg.weight, <span class="dv">0</span>), <span class="st">&quot;lbs&quot;</span>), <span class="at">bty =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb56-10"><a href="multiple-regression.html#cb56-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-11"><a href="multiple-regression.html#cb56-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(BAC <span class="sc">~</span> weight, <span class="at">type =</span> <span class="st">&quot;n&quot;</span>, <span class="at">data =</span> beer)  <span class="co"># set up axes</span></span>
<span id="cb56-12"><a href="multiple-regression.html#cb56-12" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> b.hat[<span class="dv">1</span>] <span class="sc">+</span> b.hat[<span class="dv">2</span>] <span class="sc">*</span> avg.beers, <span class="at">b =</span> b.hat[<span class="dv">3</span>])  <span class="co"># predicted BAC for average weight individual</span></span>
<span id="cb56-13"><a href="multiple-regression.html#cb56-13" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="at">leg =</span> <span class="fu">paste</span>(<span class="st">&quot;beers consumed = &quot;</span>, <span class="fu">round</span>(avg.beers, <span class="dv">1</span>)), <span class="at">bty =</span> <span class="st">&quot;n&quot;</span>)</span></code></pre></div>
<p><img src="02-MultipleRegression_files/figure-html/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>It’s not clear whether we should overlay the data on these plots. Data are helpful, but showing the data points would suggest that the fitted line is a simple regression fit. We want to be clear that it isn’t.</p>
</div>
<div id="statistical-inference-for-partial-regression-coefficients" class="section level3 hasAnchor" number="2.1.4">
<h3><span class="header-section-number">2.1.4</span> Statistical inference for partial regression coefficients<a href="multiple-regression.html#statistical-inference-for-partial-regression-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Statistical inference for partial regression coefficients proceeds in the same way as statistical inference for SLR slopes. Standard errors for both partial regression coefficients are provided in the R output: <span class="math inline">\(s_{\hat{\beta}_1 }\)</span>= 0.0013, <span class="math inline">\(s_{\hat{\beta}_{2} }\)</span>= 0.000057. Under the standard regression assumptions, the quantity <span class="math inline">\(t=(\hat{\beta}_i -\beta _i )/s_{\hat{\beta}_i }\)</span> has a <span class="math inline">\(t\)</span>-distribution. The number of degrees of freedom is the number of df associated with the SSE. This fact can be used to construct confidence intervals and hypothesis tests.</p>
<p>Most software will do the needed math for us. For example, to find 99% CIs for the partial regression coefficients in the BAC model, we can use</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="multiple-regression.html#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(fm2, <span class="at">level =</span> <span class="fl">0.99</span>)</span></code></pre></div>
<pre><code>##                     0.5 %        99.5 %
## (Intercept)  0.0084354279  0.0712912780
## beers        0.0161715108  0.0237799132
## weight      -0.0005335564 -0.0001920855</code></pre>
<p>Thus a 99% CI for the partial regression coefficient associated with weight is given by the interval from -0.00053 to -0.00019.</p>
<p>Most statistical software will automatically provide tests of the null <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\beta_i =0\)</span> vs. the alternative <span class="math inline">\(H_a\)</span>: <span class="math inline">\(\beta_i \ne 0\)</span>. For example, the R output above tells us that the <span class="math inline">\(p\)</span>-value associated with this test for the partial regression coefficient associated with weight is <span class="math inline">\(p = 0.000023\)</span>.</p>
<p>If we were reporting this analysis in scientific writing, we might say that when comparing people who have consumed the same number of beers, every 1-lb increase in weight is associated with an average BAC decrease of 3.6% <span class="math inline">\(\times\)</span> 10<span class="math inline">\(^{-4}\)</span> (s.e. 5.7% <span class="math inline">\(\times\)</span> 10<span class="math inline">\(^{-5}\)</span>). This association is statistically significant (<span class="math inline">\(t_{13} =-6.40\)</span>, <span class="math inline">\(p &lt; .001\)</span>).</p>
</div>
<div id="prediction" class="section level3 hasAnchor" number="2.1.5">
<h3><span class="header-section-number">2.1.5</span> Prediction<a href="multiple-regression.html#prediction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As in SLR, we distinguish between predictions of the average response of the population at a new value of the predictors vs. the value of a single future observation. The point estimates of the two predictions are identical, but the value of a single future observation is more uncertain. Therefore, we use a prediction interval for a single observation and a confidence interval for a population average. The width of a PI or CI is affected by the same factors as in SLR. In MLR, the width of the PI or CI depends on the distance between the new observation and the “center of mass” of the predictors in the data set. For example, if we now use the BAC model to predict the BAC of a 170-lb individual who consumes 4 beers:</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="multiple-regression.html#cb59-1" aria-hidden="true" tabindex="-1"></a>new.data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">weight =</span> <span class="dv">170</span>, <span class="at">beers =</span> <span class="dv">4</span>)</span>
<span id="cb59-2"><a href="multiple-regression.html#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(fm2, <span class="at">newdata =</span> new.data, <span class="at">interval =</span> <span class="st">&quot;prediction&quot;</span>)</span></code></pre></div>
<pre><code>##          fit        lwr        upr
## 1 0.05808664 0.03480226 0.08137103</code></pre>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="multiple-regression.html#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(fm2, <span class="at">newdata =</span> new.data, <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>)</span></code></pre></div>
<pre><code>##          fit        lwr        upr
## 1 0.05808664 0.05205732 0.06411596</code></pre>
</div>
</div>
<div id="F-test" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> <span class="math inline">\(F\)</span>-tests for several regression coefficients<a href="multiple-regression.html#F-test" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider a general multiple regression model with <span class="math inline">\(k\)</span> predictors:
<span class="math display">\[
y=\beta_0 +\beta_1 x_1 +\beta_2 x_2 +...+\beta_k x_k +\varepsilon
\]</span>
So far, we’ve seen how to test whether any one individual partial regression coefficient is equal to zero, i.e., <span class="math inline">\(H_0 :\beta_j =0\)</span> vs. <span class="math inline">\(H_a :\beta_j \ne 0\)</span>. We will now discuss how to generalize this idea to test if multiple partial regression coefficients are simultaneously equal to zero. The statistical method that we will use is an <span class="math inline">\(F\)</span>-test.</p>
<p>Unfortunately, we don’t yet have a great context for motivating <span class="math inline">\(F\)</span>-tests. We will see more compelling motivations for <span class="math inline">\(F\)</span>-tests soon when we consider <a href="multiple-regression.html#categorical-predictors">categorical predictors</a>. For now, we’ll consider our multiple regression model for the BAC data with the two predictors “beers consumed” and “weight”. Recall that the equation for this model is given in eq. <a href="multiple-regression.html#eq:beer-two-predictors">(2.1)</a>. Let’s suppose that we were interested in testing the null hypothesis that neither of the predictors have a linear association with the response, versus the alternative hypothesis that at least one predictor (and perhaps both) has a linear association with the response. In symbols, we would write the null hypothesis as <span class="math inline">\(H_0 :\beta_1 =\beta_2 =0\)</span> and the alternative as <span class="math inline">\(H_a :\beta_1 \ne 0{\rm \; or\; }\beta_2 \ne 0\)</span>.</p>
<p><span class="math inline">\(F\)</span>-tests are essentially comparisons between models. The model that provides the context for the null hypothesis is the “full” model, in the sense that it will prove to be the more flexible of our two models to be compared. In the context of our present example, the full model is given by <a href="multiple-regression.html#eq:beer-two-predictors">(2.1)</a>. The full model is then compared to a “reduced” model, which is the full model constrained by the null hypothesis. In the present example, when we constrain the full model by the null hypothesis <span class="math inline">\(\beta_1 =\beta_2 =0\)</span>, we are left with a reduced model that includes only the intercept:
<span class="math display">\[\begin{equation}
y = \beta_0 + \varepsilon.
\end{equation}\]</span></p>
<p>Formally, it is important to note that the reduced model is a special case of the full model. The statistical jargon for this observation is that the reduced model is “nested” in the full model, or (to say it in the active voice) the full model “nests” the reduced model.</p>
<p>Because the full model nests the reduced model, we know that the full model is guaranteed to explain at least as much of the variation in the response as the reduced model. The operative question is whether the improvement obtained by the full model is statistically significant, that is, whether it is more of an improvement than we would expect by random chance.</p>
<p>An <span class="math inline">\(F\)</span>-test proceeds by fitting both the full and reduced model, and for each model recording the SSE and the df associated with the SSE. An <span class="math inline">\(F\)</span>-statistic is then calculated as<br />
<span class="math display">\[
F=\frac{\left[SSE_{reduced} -SSE_{full} \right]/\left[df_{reduced} -df_{full} \right]}{{SSE_{full} / df_{full} }}
\]</span>
where by <span class="math inline">\(df_{full}\)</span> and <span class="math inline">\(df_{reduced}\)</span> we mean the df associated with the SSE of the full and reduced models, respectively. Roughly, the numerator of the <span class="math inline">\(F\)</span>-statistic quantifies the improvement in fit that the full model provides relative to the reduced model. The denominator quantifies the variation in the response left unexplained by the full model. Both numerator and denominator are standardized by their associated df to create an apples-to-apples comparison. The larger the numerator is relative to the denominator, the less likely it is that the improvement in fit was strictly due to random chance.</p>
<p>For the BAC data, we could compute the <span class="math inline">\(F\)</span>-statistic “manually” by first fitting both models and extracting the SSE:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="multiple-regression.html#cb63-1" aria-hidden="true" tabindex="-1"></a>full.model <span class="ot">&lt;-</span> <span class="fu">lm</span>(BAC <span class="sc">~</span> beers <span class="sc">+</span> weight, <span class="at">data =</span> beer)</span>
<span id="cb63-2"><a href="multiple-regression.html#cb63-2" aria-hidden="true" tabindex="-1"></a>(sse.full <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">resid</span>(full.model)<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 0.001408883</code></pre>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="multiple-regression.html#cb65-1" aria-hidden="true" tabindex="-1"></a>reduced.model <span class="ot">&lt;-</span> <span class="fu">lm</span>(BAC <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> beer)</span>
<span id="cb65-2"><a href="multiple-regression.html#cb65-2" aria-hidden="true" tabindex="-1"></a>(sse.full <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">resid</span>(reduced.model)<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 0.029225</code></pre>
<p>Then our <span class="math inline">\(F\)</span>-statistic evaluates to:
<span class="math display">\[
F=\frac{\left[0.0292 - 0.0014 \right]/\left[15 - 13 \right]}{{0.0014 / 13 }} = 128.3
\]</span></p>
<p>If the null hypothesis is true, then the <span class="math inline">\(F\)</span>-statistic will be drawn from an <span class="math inline">\(F\)</span> distribution with numerator df equal to <span class="math inline">\(df_{reduced} -df_{full}\)</span>, and denominator df equal to <span class="math inline">\(df_{full}\)</span>. Evidence against the null and in favor of the alternative comes from large values of the <span class="math inline">\(F\)</span>-statistic. The <span class="math inline">\(p\)</span>-value associated with the test is the probability of observing an <span class="math inline">\(F\)</span>-statistic at least as large as the one observed if the null hypothesis is true.</p>
<p>In R, this <span class="math inline">\(p\)</span>-values can be found with the command <code>pf</code>.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="multiple-regression.html#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pf</span>(<span class="fl">128.3</span>, <span class="at">df1 =</span> <span class="dv">2</span>, <span class="at">df2 =</span> <span class="dv">13</span>, <span class="at">lower =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## [1] 2.760276e-09</code></pre>
<p>Thus, our <span class="math inline">\(p\)</span>-value is infinitesimal. In light of these data, it is almost completely implausible that there is no linear association between BAC and both the number of beers consumed and the individual’s weight.</p>
<p>We’ve taken the above calculations slowly so that we can understand their logic. In R, we can use the <code>anova</code> command to execute the <span class="math inline">\(F\)</span>-test in one fell swoop.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="multiple-regression.html#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(reduced.model, full.model)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: BAC ~ 1
## Model 2: BAC ~ beers + weight
##   Res.Df       RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1     15 0.0292250                                  
## 2     13 0.0014089  2  0.027816 128.33 2.756e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div id="model-utility-tests" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Model utility tests<a href="multiple-regression.html#model-utility-tests" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The test that we have just conducted for the BAC data turns out to be a type of <span class="math inline">\(F\)</span>-test called the <em>model utility test</em>. In general, with the general regression model
<span class="math display">\[
y=\beta_0 +\beta_1 x_1 +\beta_2 x_2 +...+\beta_k x_k +\varepsilon
\]</span>
the model utility test is a test of the null hypothesis that all the regression coefficients equal zero, that is, <span class="math inline">\(H_0 :\beta_1 =\beta_2 =...=\beta_k =0\)</span> vs. the alternative that at least one of the partial regression coefficients is not equal to zero. In other words, it is a test of whether the regression model provides a significant improvement in fit compared a simpler model that assumes the <span class="math inline">\(y\)</span>’s are a simple random sample from a Gaussian distirbution. The model utility test is easy for computer programmers to automate, so it is usually included as part of the standard regression output. In the R <code>summary</code> of a regression model, we can find it at the very end of the output.</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="multiple-regression.html#cb71-1" aria-hidden="true" tabindex="-1"></a>full.model <span class="ot">&lt;-</span> <span class="fu">lm</span>(BAC <span class="sc">~</span> beers <span class="sc">+</span> weight, <span class="at">data =</span> beer)</span>
<span id="cb71-2"><a href="multiple-regression.html#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(full.model)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = BAC ~ beers + weight, data = beer)
## 
## Residuals:
##        Min         1Q     Median         3Q        Max 
## -0.0162968 -0.0067796  0.0003985  0.0085287  0.0155621 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  3.986e-02  1.043e-02   3.821  0.00212 ** 
## beers        1.998e-02  1.263e-03  15.817 7.16e-10 ***
## weight      -3.628e-04  5.668e-05  -6.401 2.34e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.01041 on 13 degrees of freedom
## Multiple R-squared:  0.9518, Adjusted R-squared:  0.9444 
## F-statistic: 128.3 on 2 and 13 DF,  p-value: 2.756e-09</code></pre>
<p>Although the model utility test has a grandiose name, it is rarely interesting. Rejecting the null in the model utility test is usually not an impressive conclusion (<span class="citation">Quinn and Keough (<a href="#ref-quinn2002experimental" role="doc-biblioref">2002</a>)</span>). You may have also noticed that in SLR the model utility test always provided a <span class="math inline">\(p\)</span>-value that was exactly equal to the <span class="math inline">\(p\)</span>-value generated for the test of <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\beta_1 =0\)</span> vs. <span class="math inline">\(H_a\)</span>: <span class="math inline">\(\beta_1 \ne 0\)</span>. Can you figure out why this is so?</p>
<!-- ### More on the relationship between $t$-tests and $F$-tests. -->
<!-- This section started to seem unnecessary.  Maybe revisit later and condense to something much shorter? -->
<!-- In linear statistical models (regression and ANOVA), we encounter predominantly two types of tests, $t$-tests and $F$-tests.  We have already seen how $t$-tests work.  $t$-tests are used to test null hypotheses that have a single equality.  In general notation, if we have a generic parameter $\theta$, a $t$-test tests $H_0$: $\theta = \theta_0$ vs. either the two-sided alternative $H_a$: $\theta \ne \theta_0$ or either of the one-sided alternatives $H_a$: $\theta < \theta_0$ or $H_a$: $\theta > \theta_0$.  One calculates a one- or two-tailed $p$-value based on whether the alternative hypothesis is one- or two-sided. -->
<!-- $F$-tests are used to test null hypotheses that either multiple parameters or multiple combinations of parameters are simultaneously equal to 0.  For example, in the fish data, we have just tested hypotheses $H_0 :\beta_2 =\beta_3 =0$ vs. $H_a :\beta_2 \ne 0{\rm \; or\; }\beta_3 \ne 0$.  The mechanics of an $F$-test are such that the only viable alternative is a two-sided alternative.  (This is actually not entirely true, but we can take it as true for our purposes in these notes.)  $P$-values are found by comparing $F$-statistics to $F$-distributions.  $F$-distributions are specified by two separate degrees of freedom, which we call the numerator degrees of freedom (ndf) and the denominator degrees of freedom (ddf).  The ndf will typically be the number of equalities needed to specify the null hypothesis, and the denominator df will be the number of df associated with the SSE for the full model.  -->
<!-- Note that $F$-statistics take only positive values.  The larger the value of the $F$-statistic, the more evidence the data provide against the null.  Thus, with an $F$-test, the $p$-value is always the area to the right of the observed statistic, or the probability of observing a test statistic at least as large as the value observed.  Consequently, the terminology associated with an $F$-test can be mildly confusing: the $p$-value is always a one-tailed $p$-value, but it is used to test a two-sided alternative hypothesis. -->
<!-- Finally, $F$-tests can also be used to test null hypotheses with single equalities.  That is, we can use an $F$-test to test $H_0$: $\theta = \theta_0$, but with an $F$-test we can only consider the two-sided alternative $H_a$: $\theta \ne \theta_0$. (To put this more in the form of an $F$-test, we might re-write the hypotheses as $H_0$: $\theta - \theta_0 = 0$ and $H_a$: $\theta - \theta_0 \ne 0$.)  Thus, in some sense, an $F$-test is a generalization of a $t$-test.  However, only a $t$-test can be used to test $H_0$: $\theta =\theta_0$ vs. the one-sided alternatives $H_a$: $\theta < \theta_0$ or $H_a$: $\theta > \theta_0$.   -->
</div>
</div>
<div id="categorical-predictors" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Categorical predictors<a href="multiple-regression.html#categorical-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far, we have dealt exclusively with quantitative predictors. Although we haven’t given it much thought, a key feature of quantitative predictors is that their values can be ordered, and that the distance between ordered values is meaningful. For example, in the BAC data, a predictor value of <span class="math inline">\(x=3\)</span> beers consumed is greater than <span class="math inline">\(x=2\)</span> beers consumed. Moreover, the difference between <span class="math inline">\(x=2\)</span> and <span class="math inline">\(x=3\)</span> beers consumed is exactly one-half of the distance between <span class="math inline">\(x=2\)</span> and <span class="math inline">\(x=4\)</span> beers consumed. Another way to think about quantitative predictors is that we could sensibly place all of their values on a number line.</p>
<p>Categorical variables are variables whose values cannot be sensibly placed on a number line. Examples include ethnicity or brand of manufacture. We use indicator variables as devices to include categorical predictors in regression models.<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a></p>
<p><em>Example.</em> D. K. Sackett investigated mercury accumulation in the tissues of large-mouth bass (a species of fish) in several lakes in the Raleigh, NC area (<span class="citation">Sackett et al. (<a href="#ref-sackett2013influence" role="doc-biblioref">2013</a>)</span>). We will examine data from three lakes: Adger, Bennett’s Millpond, and Waterville. From each lake, several fish were sampled, and the mercury (Hg) content of their tissues was measured. Because fish are known to accumulate mercury in their tissues as they age, the age of each fish (in years) was also determined. The plot below shows the mercury content (in mg / kg) for each fish plotted vs. age, with different plotting symbols used for the three lakes. To stabilize the variance, we will use the log of mercury content as the response variable. There are <span class="math inline">\(n=23\)</span> data points in this data set.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="multiple-regression.html#cb73-1" aria-hidden="true" tabindex="-1"></a>fish <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="st">&quot;data/fish-mercury.txt&quot;</span>, <span class="at">head =</span> T, <span class="at">stringsAsFactors =</span> T)</span>
<span id="cb73-2"><a href="multiple-regression.html#cb73-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-3"><a href="multiple-regression.html#cb73-3" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(fish, <span class="fu">plot</span>(<span class="fu">log</span>(hg) <span class="sc">~</span> age, <span class="at">xlab =</span> <span class="st">&quot;age (years)&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;log(tissue Hg)&quot;</span>, <span class="at">type =</span> <span class="st">&quot;n&quot;</span>))</span>
<span id="cb73-4"><a href="multiple-regression.html#cb73-4" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(<span class="fu">subset</span>(fish, site <span class="sc">==</span> <span class="st">&quot;Adger&quot;</span>), <span class="fu">points</span>(<span class="fu">log</span>(hg) <span class="sc">~</span> age, <span class="at">pch =</span> <span class="st">&quot;A&quot;</span>))</span>
<span id="cb73-5"><a href="multiple-regression.html#cb73-5" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(<span class="fu">subset</span>(fish, site <span class="sc">==</span> <span class="st">&quot;Bennett&quot;</span>), <span class="fu">points</span>(<span class="fu">log</span>(hg) <span class="sc">~</span> age, <span class="at">pch =</span> <span class="st">&quot;B&quot;</span>))  </span>
<span id="cb73-6"><a href="multiple-regression.html#cb73-6" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(<span class="fu">subset</span>(fish, site <span class="sc">==</span> <span class="st">&quot;Waterville&quot;</span>), <span class="fu">points</span>(<span class="fu">log</span>(hg) <span class="sc">~</span> age, <span class="at">pch =</span> <span class="st">&quot;W&quot;</span>))</span></code></pre></div>
<p><img src="02-MultipleRegression_files/figure-html/unnamed-chunk-14-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>With these data, we would like to ask: when comparing fish of the same age, is there evidence that tissue mercury content in fish differs among the three lakes? To do so, we need to develop a set of <span class="math inline">\(indicator\)</span> variables to capture the differences among the lakes. As is often the case, one has options here. If you wish, you can construct the indicator variables manually, perhaps using a spreadsheet program. Alternatively, most computer programs, including R, will create indicator variables automatically. We’ll sketch out the ideas behind indicator variables first and then see how R creates them.</p>
<p>To create a set of indicator variables, we first need to choose one level of the variable as the reference level or baseline. While the scientific context of a problem will sometimes make it more natural to designate one level as a reference, the choice is often arbitrary. In all cases, the choice of a reference level will not affect the ensuing analysis. For every level other than the reference, we create a separate indicator variable that is equal to 1 for that level and is equal to 0 for all levels. Thus, to include a categorical variable with <span class="math inline">\(c\)</span> different levels, we need <span class="math inline">\(c−1\)</span> indicator variables.</p>
<p>To see how R constructs indicator variables, we can use the <code>contrast</code> command<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a></p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="multiple-regression.html#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(fish<span class="sc">$</span>site)</span></code></pre></div>
<pre><code>##            Bennett Waterville
## Adger            0          0
## Bennett          1          0
## Waterville       0          1</code></pre>
<p>In the R output above, each column represents an indicator variable, the rows give the levels of the categorical variable, and the numbers give the coding of each indicator variable. We see that R has created two indicator variables (as we’d expect), and labeled them “Bennett” and “Waterville”. The names of the indicator variables provide strong hints about their coding, but to be completely sure we can inspect the numerical coding given below. The indicator labeled “Bennett” takes the value of 1 when <code>site</code> equals <code>Bennett</code> and takes the value of zero otherwise. In other words, it is the indicator for Bennett. The indicator labeled “Waterville” takes the value of 1 when <code>site</code> equals <code>Waterville</code> and takes the value of zero for the other two sites. In other words, it is the indicator for Waterville. Adger, therefore, is the reference site.<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a></p>
<p>Now let’s fit a regression model with both <code>age</code> and <code>site</code> as predictors.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="multiple-regression.html#cb76-1" aria-hidden="true" tabindex="-1"></a>fish_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(hg) <span class="sc">~</span> age <span class="sc">+</span> site, <span class="at">data =</span> fish)</span>
<span id="cb76-2"><a href="multiple-regression.html#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fish_model)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log(hg) ~ age + site, data = fish)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.47117 -0.08896  0.03796  0.13910  0.31327 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    -1.80618    0.15398 -11.730 3.80e-10 ***
## age             0.14309    0.01967   7.276 6.66e-07 ***
## siteBennett     0.07107    0.12910   0.550   0.5884    
## siteWaterville -0.26105    0.10817  -2.413   0.0261 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.2084 on 19 degrees of freedom
## Multiple R-squared:  0.7971, Adjusted R-squared:  0.765 
## F-statistic: 24.88 on 3 and 19 DF,  p-value: 8.58e-07</code></pre>
<p>The R output gives us information about the partial regression coefficients associated with <code>age</code> and with each of the indicator variables for <code>site</code>. In an equation, we could write this model as
<span class="math display">\[
y=\beta_0 +\beta_1 x_1 +\beta_2 x_2 +\beta_3 x_3 +\varepsilon
\]</span>
where <span class="math inline">\(x_1\)</span> gives the age of the fish and <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span> are the indicator variables for Bennett and Waterville, respectively.</p>
<p>While the R output gives us tests of the significance of the individual regression coefficients, we want to determine whether there are significant differences among the three sites when comparing fish of the same age. To answer this question, we need to test the hypothesis <span class="math inline">\(H_0: \beta_2 = \beta_3 = 0\)</span>. We can do this with an <a href="multiple-regression.html#F-test"><span class="math inline">\(F\)</span>-test</a>.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="multiple-regression.html#cb78-1" aria-hidden="true" tabindex="-1"></a>fish_model_full <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(hg) <span class="sc">~</span> age <span class="sc">+</span> site, <span class="at">data =</span> fish)</span>
<span id="cb78-2"><a href="multiple-regression.html#cb78-2" aria-hidden="true" tabindex="-1"></a>fish_model_reduced <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(hg) <span class="sc">~</span> age, <span class="at">data =</span> fish)</span>
<span id="cb78-3"><a href="multiple-regression.html#cb78-3" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(fish_model_reduced, fish_model_full)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: log(hg) ~ age
## Model 2: log(hg) ~ age + site
##   Res.Df     RSS Df Sum of Sq      F  Pr(&gt;F)  
## 1     21 1.17251                              
## 2     19 0.82529  2   0.34722 3.9969 0.03558 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Thus, we see that there are significant differences among the sites, when comparing fish of the same age (<span class="math inline">\(F_{2, 19} = 4.00\)</span>, <span class="math inline">\(p = 0.036\)</span>).</p>
<p>For what it’s worth, we also could have obtained this test by running the <code>anova</code> command on the full model.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="multiple-regression.html#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(fish_model_full)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: log(hg)
##           Df  Sum Sq Mean Sq F value   Pr(&gt;F)    
## age        1 2.89448 2.89448 66.6374 1.24e-07 ***
## site       2 0.34722 0.17361  3.9969  0.03558 *  
## Residuals 19 0.82529 0.04344                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The <code>anova</code> command also gives an <span class="math inline">\(F\)</span>-test for the null hypothesis of no association between age and the response when comparing fish from the same lake, that is, that <span class="math inline">\(\beta_1 = 0\)</span>. This <span class="math inline">\(F\)</span>-test gives the same result as the <span class="math inline">\(t\)</span>-test provided in the earlier <code>summary</code>, because the two tests are identical. Arguably, for single regression coefficients, the output from <code>summary</code> is more useful, because it gives us the estimate of <span class="math inline">\(\beta_1\)</span>, while the <code>anova</code> output only gives us an <span class="math inline">\(F\)</span>-test.</p>
<p>Finally, now that we have established that there are significant differences among the sites when comparing fish of the same age, we can go back and make sense of the estimates of <span class="math inline">\(\beta_2\)</span> and <span class="math inline">\(\beta_3\)</span> in the full model. The partial regression coefficients associated with an indicator variable quantify the difference between the level that the variable is an indicator for and the reference. In other words, for the fish data, the value <span class="math inline">\(\hat{\beta}_2 = 0.071\)</span> tells us that if we compare two fish of the same age, then a fish from Bennett will have a response that is on average 0.071 larger than a fish from Adger (the reference site). The value <span class="math inline">\(\hat{\beta}_3 = -0.261\)</span> tells us that, on average, a fish from Waterville will have a response that is 0.261 less than a similarly aged-fish from Adger. Note that these values also give us enough information to compute the average difference in <span class="math inline">\(y\)</span> between two similarly aged fish from Bennett and Waterville, even though that value isn’t directly included in the output.</p>
<hr />
<p><span style="color: gray;"> Indicator variables may seem like a bit of an awkward device. Why can’t we just fit a model with a separate intercept for each lake? In fact, we can. In <code>R</code>, the program <code>lm</code> includes the intercept <span class="math inline">\(\beta_0\)</span> in any model by default, because most regression models include it. However, if we instruct <code>lm</code> to omit the baseline intercept <span class="math inline">\(\beta_0\)</span>, then the program will parameterize the model by the lake-specific intercepts. We instruct <code>lm</code> to omit the intercept by including a <code>-1</code> on the right-hand side of the model formula as follows:</span></p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="multiple-regression.html#cb82-1" aria-hidden="true" tabindex="-1"></a>fish_model_alt <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(hg) <span class="sc">~</span> age <span class="sc">+</span> site <span class="sc">-</span> <span class="dv">1</span>, <span class="at">data =</span> fish)</span>
<span id="cb82-2"><a href="multiple-regression.html#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fish_model_alt)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log(hg) ~ age + site - 1, data = fish)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.47117 -0.08896  0.03796  0.13910  0.31327 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## age             0.14309    0.01967   7.276 6.66e-07 ***
## siteAdger      -1.80618    0.15398 -11.730 3.80e-10 ***
## siteBennett    -1.73511    0.09440 -18.381 1.47e-13 ***
## siteWaterville -2.06723    0.16351 -12.643 1.07e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.2084 on 19 degrees of freedom
## Multiple R-squared:  0.9721, Adjusted R-squared:  0.9662 
## F-statistic: 165.2 on 4 and 19 DF,  p-value: 1.781e-14</code></pre>
<p><span style="color: gray;"> The model is now parameterized by the lake-specific intercepts and the common slope. While this parameterization is more straightforward, note that the <span class="math inline">\(R^2\)</span> value and the model utility test are now wrong. The software’s routine for calculating <span class="math inline">\(R^2\)</span> and the model utility test assumes that the intercept <span class="math inline">\(\beta_0\)</span> will be present in the model. Of course, it’s easy to use our original parameterization to get the correct <span class="math inline">\(R^2\)</span> value (and model utility test) and use the alternative parameterization to get the lake-specific slopes. We just have to be careful with regard to the rest of the output when the baseline intercept is omitted.</span></p>
<p><span style="color: gray;"> None of this behavior is unique to <code>R</code>. In SAS, PROC GLM has a similar option for reparameterizing the model without the common intercept, but it will cause the computation of <span class="math inline">\(R^2\)</span> and the model utility test to break there as well.</span></p>
<hr />
<p>Models that combine a single numerical predictor and a single categorical predictor can be visualized by plotting the trend lines for each level of the categorical predictor. In making this plot, it is easier to use the alternative parameterization of the model described in the gray text above.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="multiple-regression.html#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(fish, <span class="fu">plot</span>(<span class="fu">log</span>(hg) <span class="sc">~</span> age, <span class="at">xlab =</span> <span class="st">&quot;age (years)&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;log(tissue Hg)&quot;</span>, <span class="at">type =</span> <span class="st">&quot;n&quot;</span>))</span>
<span id="cb84-2"><a href="multiple-regression.html#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(<span class="fu">subset</span>(fish, site <span class="sc">==</span> <span class="st">&quot;Adger&quot;</span>), <span class="fu">points</span>(<span class="fu">log</span>(hg) <span class="sc">~</span> age, <span class="at">pch =</span> <span class="st">&quot;A&quot;</span>, <span class="at">col =</span> <span class="st">&quot;forestgreen&quot;</span>))</span>
<span id="cb84-3"><a href="multiple-regression.html#cb84-3" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(<span class="fu">subset</span>(fish, site <span class="sc">==</span> <span class="st">&quot;Bennett&quot;</span>), <span class="fu">points</span>(<span class="fu">log</span>(hg) <span class="sc">~</span> age, <span class="at">pch =</span> <span class="st">&quot;B&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>))  </span>
<span id="cb84-4"><a href="multiple-regression.html#cb84-4" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(<span class="fu">subset</span>(fish, site <span class="sc">==</span> <span class="st">&quot;Waterville&quot;</span>), <span class="fu">points</span>(<span class="fu">log</span>(hg) <span class="sc">~</span> age, <span class="at">pch =</span> <span class="st">&quot;W&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>))</span>
<span id="cb84-5"><a href="multiple-regression.html#cb84-5" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> fish_model_alt<span class="sc">$</span>coefficients[<span class="dv">2</span>], <span class="at">b =</span> fish_model_alt<span class="sc">$</span>coefficients[<span class="dv">1</span>], <span class="at">col =</span> <span class="st">&quot;forestgreen&quot;</span>)</span>
<span id="cb84-6"><a href="multiple-regression.html#cb84-6" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> fish_model_alt<span class="sc">$</span>coefficients[<span class="dv">3</span>], <span class="at">b =</span> fish_model_alt<span class="sc">$</span>coefficients[<span class="dv">1</span>], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb84-7"><a href="multiple-regression.html#cb84-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> fish_model_alt<span class="sc">$</span>coefficients[<span class="dv">4</span>], <span class="at">b =</span> fish_model_alt<span class="sc">$</span>coefficients[<span class="dv">1</span>], <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb84-8"><a href="multiple-regression.html#cb84-8" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Lake A&quot;</span>, <span class="st">&quot;Lake B&quot;</span>, <span class="st">&quot;Lake W&quot;</span>), <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;forestgreen&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>))</span></code></pre></div>
<p><img src="02-MultipleRegression_files/figure-html/unnamed-chunk-19-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
<div id="regression-interactions" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Interactions between predictors<a href="multiple-regression.html#regression-interactions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider (again) the BAC data, with our working model
<span class="math display">\[
y =\beta_0 +\beta_1 x_1 +\beta_2 x_2 +\varepsilon
\]</span>
where <span class="math inline">\(y\)</span> is the response (BAC), <span class="math inline">\(x_1\)</span> is beers consumed, <span class="math inline">\(x_2\)</span> is weight, and <span class="math inline">\(\varepsilon\)</span> is iid normal error. This is an <em>additive</em> model, in the sense that the joint association between beers consumed and weight (as a pair) and BAC can be found by adding together the individual associations between each of the two predictors and the response. However, we might instead want to allow for the possibility that the association between one predictor and the response itself depends on the value of a second predictor. This state of affairs is called a <em>statistical interaction</em>. More specifically, we call it a statistical interaction between the two predictors with respect to their association with the response. Note that whether or not two predictors interact has nothing to do with whether the predictors themselves are associated.<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a></p>
<p>An <em>interaction</em> between the two predictors allows the effect of beers consumed to depend on weight, and vice versa. A model with an interaction can be written as:
<span class="math display" id="eq:beer-interaction">\[
y =\beta_0 +\beta_1 x_1 +\beta_2 x_2 +\beta_3 x_1 x_2 +\varepsilon
\tag{2.2}
\]</span>
There are two equally good ways to code this model in R:</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="multiple-regression.html#cb85-1" aria-hidden="true" tabindex="-1"></a>fm1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(BAC <span class="sc">~</span> beers <span class="sc">+</span> weight <span class="sc">+</span> beers<span class="sc">:</span>weight, <span class="at">data =</span> beer)</span></code></pre></div>
<p>or</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="multiple-regression.html#cb86-1" aria-hidden="true" tabindex="-1"></a>fm2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(BAC <span class="sc">~</span> beers <span class="sc">*</span> weight, <span class="at">data =</span> beer)</span></code></pre></div>
<p>In the first notation, the colon (:) tells R to include the interaction between the predictors that appear on either side of the colon. In the second notation, the asterisk (*) is shorthand for both the individual predictors and their interaction.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="multiple-regression.html#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fm2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = BAC ~ beers * weight, data = beer)
## 
## Residuals:
##        Min         1Q     Median         3Q        Max 
## -0.0169998 -0.0070909  0.0008463  0.0084267  0.0164373 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)   3.010e-02  3.495e-02   0.861  0.40601   
## beers         2.162e-02  5.760e-03   3.754  0.00275 **
## weight       -2.993e-04  2.241e-04  -1.336  0.20646   
## beers:weight -1.066e-05  3.627e-05  -0.294  0.77393   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.0108 on 12 degrees of freedom
## Multiple R-squared:  0.9521, Adjusted R-squared:  0.9402 
## F-statistic: 79.57 on 3 and 12 DF,  p-value: 3.453e-08</code></pre>
<p><em>Interpreting the interaction</em>.<br />
The partial regression coefficient associated with an interaction between two predictors (call them “A” and “B”) quantifies the effect that predictor A has on the linear association between predictor B and the response. Or, equivalently, the same partial regression coefficient quantifies the effect that predictor B has on the linear association between predictor A and the response. (It may not be obvious right away that the same regression coefficient permits both interpretations, but you might be able to convince yourself that this is true by doing some algebra with the regression model.) Thus, if we reject the null hypothesis that a partial regression coefficient associated with an interaction equals zero, then we conclude that the effects of the two predictors depend on one another. With the BAC data, the interaction term is not statistically significant. Thus, these data provide no evidence that the association between beers consumed and BAC depends on weight, or vice versa.</p>
<p>For the sake of argument, let’s examine the estimate of the interaction between beers consumed and weight, despite the fact that it is not statistically significant. How can we interpret the value <span class="math inline">\(\hat{\beta}_3 = -1.07 \times 10^{-5}\)</span>? This value tells us how the association between beers consumed and BAC changes as weight changes. In other words, the model predicts that a heavier person’s BAC will increase less for each additional beer consumed, compared to a lighter person. How much less? If the heavier person weighs 1 lb more than the lighter person, then one additional beer will increase the heavier person’s BAC by <span class="math inline">\(1.07 \times 10^{-5}\)</span> less than it increases the lighter person’s BAC. This agrees with our expectations, but these data do not provide enough evidence to declare that this interaction is statistically significant. Alternatively, <span class="math inline">\(\beta_3\)</span> also tells us how the association between weight and BAC changes as the number of beers consumed changes. That is, as the number of beers consumed increases, then the association between weight and BAC becomes more steeply negative. Again, this coincides with our expectation, despite the lack of statistical significance.</p>
<p><em>Interpreting partial regression coefficients in the presence of an interaction.</em><br />
There is a major and unexpected complication that ensues from including an interaction term in a regression model. This complication concerns how we interpret the partial regression coefficients associated with the individual predictors engaged in the interaction, that is, the regression coefficients <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> in eq. <a href="multiple-regression.html#eq:beer-interaction">(2.2)</a>. It turns out that, in <a href="multiple-regression.html#eq:beer-interaction">(2.2)</a>, the partial regression coefficient associated with an individual predictor quantifies the relationship between that predictor and the response when the other predictor involved in the interaction equals 0. Sometimes this interpretation is scientifically meaningful, but usually it isn’t.</p>
<p>For example, in the BAC model that includes the interaction above, the parameter <span class="math inline">\(\beta_1\)</span> now quantifies the association between beers consumed and BAC for people who weigh 0 lbs. Obviously, this is a meaningless quantity. Alternatively, the parameter <span class="math inline">\(\beta_2\)</span> quantifies the association between weight and BAC for people who have consumed 0 beers. This is a bit less ridiculous — in fact, it makes a lot of sense — but still requires extrapolating the model fit outside the range of the observed data, because there are no data points here for people who have had 0 beers.<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a></p>
<p>To summarize, the subtlety here is that if we compare the additive model
<span class="math display">\[
y =\beta_0 +\beta_1 x_1 +\beta_2 x_2 +\varepsilon
\]</span>
with the model that includes an interaction
<span class="math display">\[
y =\beta_0 +\beta_1 x_1 +\beta_2 x_2 +\beta_3 x_1 x_2 +\varepsilon,
\]</span>
the parameters <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> have <strong>completely different meanings</strong> in these two models. In other words, adding an interaction between two predictors changes the meaning of the regression coefficient associated with the individual predictors involved in the interaction. This is a subtlety that routinely confuses even top investigators. It’s a hard point to grasp, but essential for interpreting models that include interactions correctly.</p>
<p>We can ease the interpretation of partial regression coefficients in a model that includes an interaction by <a href="#centering-the-predictors">centering the predictors</a>. Recall that centering the predictors means creating new versions of each predictor by subtracting off their respective averages. For the BAC data, we could create centered versions of the two predictors by:
<span class="math display">\[
\begin{array}{l} {x_1^{ctr} =x_1 -\bar{x}_1 } \\ {x_2^{ctr} =x_2 -\bar{x}_{2} } \end{array}
\]</span>
We then fit the model with the interaction, using the centered predictors instead:</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="multiple-regression.html#cb89-1" aria-hidden="true" tabindex="-1"></a>beer<span class="sc">$</span>beers.c <span class="ot">&lt;-</span> beer<span class="sc">$</span>beers <span class="sc">-</span> <span class="fu">mean</span>(beer<span class="sc">$</span>beers)</span>
<span id="cb89-2"><a href="multiple-regression.html#cb89-2" aria-hidden="true" tabindex="-1"></a>beer<span class="sc">$</span>weight.c <span class="ot">&lt;-</span> beer<span class="sc">$</span>weight <span class="sc">-</span> <span class="fu">mean</span>(beer<span class="sc">$</span>weight)</span>
<span id="cb89-3"><a href="multiple-regression.html#cb89-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(beer)</span></code></pre></div>
<pre><code>##     BAC weight beers beers.c weight.c
## 1 0.100    132     5  0.1875 -39.5625
## 2 0.030    128     2 -2.8125 -43.5625
## 3 0.190    110     9  4.1875 -61.5625
## 4 0.120    192     8  3.1875  20.4375
## 5 0.040    172     3 -1.8125   0.4375
## 6 0.095    250     7  2.1875  78.4375</code></pre>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="multiple-regression.html#cb91-1" aria-hidden="true" tabindex="-1"></a>fm3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(BAC <span class="sc">~</span> beers.c <span class="sc">*</span> weight.c, <span class="at">data =</span> beer)</span>
<span id="cb91-2"><a href="multiple-regression.html#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fm3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = BAC ~ beers.c * weight.c, data = beer)
## 
## Residuals:
##        Min         1Q     Median         3Q        Max 
## -0.0169998 -0.0070909  0.0008463  0.0084267  0.0164373 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       7.402e-02  2.849e-03  25.984 6.45e-12 ***
## beers.c           1.980e-02  1.447e-03  13.685 1.10e-08 ***
## weight.c         -3.506e-04  7.206e-05  -4.865 0.000388 ***
## beers.c:weight.c -1.066e-05  3.627e-05  -0.294 0.773927    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.0108 on 12 degrees of freedom
## Multiple R-squared:  0.9521, Adjusted R-squared:  0.9402 
## F-statistic: 79.57 on 3 and 12 DF,  p-value: 3.453e-08</code></pre>
<p>Note that centering the predictors does not change the estimated interaction or its statistical significance. The main advantage of centering the predictors is that the partial regression coefficients associated with the centered versions of the predictors have a nice interpretation. Now, the partial regression coefficients associated with the main effects quantify the relationship between the predictor and the response when the other predictor involved in the interaction equals its average value.</p>
<p>While we’re at it, we can also return to the fish data to ask if fish accumulate mercury in their tissues at different rates in the three lakes.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="multiple-regression.html#cb93-1" aria-hidden="true" tabindex="-1"></a>fish_model_interaction <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(hg) <span class="sc">~</span> age <span class="sc">*</span> site, <span class="at">data =</span> fish)</span>
<span id="cb93-2"><a href="multiple-regression.html#cb93-2" aria-hidden="true" tabindex="-1"></a>fish_model_additive <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(hg) <span class="sc">~</span> age <span class="sc">+</span> site, <span class="at">data =</span> fish)</span>
<span id="cb93-3"><a href="multiple-regression.html#cb93-3" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(fish_model_additive, fish_model_interaction)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: log(hg) ~ age + site
## Model 2: log(hg) ~ age * site
##   Res.Df     RSS Df Sum of Sq      F Pr(&gt;F)
## 1     19 0.82529                           
## 2     17 0.75322  2   0.07207 0.8133 0.4599</code></pre>
<p>The interaction between age and lake is not significant (<span class="math inline">\(F_{2, 17} = 0.81\)</span>, <span class="math inline">\(p=0.46\)</span>). There is no evidence that the rate at which fish accumulate mercury in their tissues differs among the three lakes.</p>
<p>Just for fun, we can plot the model with the interaction to see how the plot differs from the additive model that we considered earlier. As we did before, we’ll use the trick of fitting the model without the baseline intercept to make it easier to extract the slopes and intercepts of the lake-specific trend lines.</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="multiple-regression.html#cb95-1" aria-hidden="true" tabindex="-1"></a>fish_model_alt <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(hg) <span class="sc">~</span> site <span class="sc">+</span> age<span class="sc">:</span>site <span class="sc">-</span> <span class="dv">1</span>, <span class="at">data =</span> fish)</span>
<span id="cb95-2"><a href="multiple-regression.html#cb95-2" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(fish, <span class="fu">plot</span>(<span class="fu">log</span>(hg) <span class="sc">~</span> age, <span class="at">xlab =</span> <span class="st">&quot;age (years)&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;log(tissue Hg)&quot;</span>, <span class="at">type =</span> <span class="st">&quot;n&quot;</span>))</span>
<span id="cb95-3"><a href="multiple-regression.html#cb95-3" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(<span class="fu">subset</span>(fish, site <span class="sc">==</span> <span class="st">&quot;Adger&quot;</span>), <span class="fu">points</span>(<span class="fu">log</span>(hg) <span class="sc">~</span> age, <span class="at">pch =</span> <span class="st">&quot;A&quot;</span>, <span class="at">col =</span> <span class="st">&quot;forestgreen&quot;</span>))</span>
<span id="cb95-4"><a href="multiple-regression.html#cb95-4" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(<span class="fu">subset</span>(fish, site <span class="sc">==</span> <span class="st">&quot;Bennett&quot;</span>), <span class="fu">points</span>(<span class="fu">log</span>(hg) <span class="sc">~</span> age, <span class="at">pch =</span> <span class="st">&quot;B&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>))  </span>
<span id="cb95-5"><a href="multiple-regression.html#cb95-5" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(<span class="fu">subset</span>(fish, site <span class="sc">==</span> <span class="st">&quot;Waterville&quot;</span>), <span class="fu">points</span>(<span class="fu">log</span>(hg) <span class="sc">~</span> age, <span class="at">pch =</span> <span class="st">&quot;W&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>))</span>
<span id="cb95-6"><a href="multiple-regression.html#cb95-6" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> fish_model_alt<span class="sc">$</span>coefficients[<span class="dv">1</span>], <span class="at">b =</span> fish_model_alt<span class="sc">$</span>coefficients[<span class="dv">4</span>], <span class="at">col =</span> <span class="st">&quot;forestgreen&quot;</span>)</span>
<span id="cb95-7"><a href="multiple-regression.html#cb95-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> fish_model_alt<span class="sc">$</span>coefficients[<span class="dv">2</span>], <span class="at">b =</span> fish_model_alt<span class="sc">$</span>coefficients[<span class="dv">5</span>], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb95-8"><a href="multiple-regression.html#cb95-8" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> fish_model_alt<span class="sc">$</span>coefficients[<span class="dv">3</span>], <span class="at">b =</span> fish_model_alt<span class="sc">$</span>coefficients[<span class="dv">6</span>], <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb95-9"><a href="multiple-regression.html#cb95-9" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Lake A&quot;</span>, <span class="st">&quot;Lake B&quot;</span>, <span class="st">&quot;Lake W&quot;</span>), <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;forestgreen&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>))</span></code></pre></div>
<p><img src="02-MultipleRegression_files/figure-html/unnamed-chunk-25-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>A picture may be worth a thousand words, but it isn’t worth a test! The plot of the model suggests that fish in lake W accumulate mercury more slowly than fish in the other two lakes, but the differences among the slopes are no more greater we would have expected from random variation.</p>
</div>
<div id="collinearity" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> (Multi-)Collinearity<a href="multiple-regression.html#collinearity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Collinearity (or what is sometimes also called multi-collinearity) refers to correlations among predictors, or among weighted sums of predictors. In a designed experiment, collinearity should not be an issue: The experimenter should be able to assign predictors in such a way that predictors are not strongly correlated with one another, or even better, are perfectly uncorrelated.<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a> With observational data, however, collinearity is often the rule more than the exception. This is especially true when the number of predictors becomes large relative to the number of data points. For example, the problem is especially acute in genomic studies, in which one may seek to find genetic correlates of phenotypic differences with a sample of a few dozen genomes, each of which contains genotypes at several thousand or more loci. In this section, we will explain what collinearity is, how it affects regression modeling, how it can be measured, and what (if anything) can be done about it.</p>
<p>To illustrate, we’ll use a data set that details the tar content, nicotine content, weight, and carbon monoxide content of a couple dozen brands of cigarettes. I found these data in <span class="citation">McIntyre (<a href="#ref-mcintyre1994using" role="doc-biblioref">1994</a>)</span>, who offers the following context:</p>
<blockquote>
<p>The Federal Trade Commission annually rates varieties of domestic cigarettes according to their tar, nicotine, and carbon monoxide content. The United States Surgeon General considers each of these substances hazardous to a smoker’s health. Past studies have shown that increases in the tar and nicotine content of a cigarette are accompanied by an increase in the carbon monoxide emitted from the cigarette smoke.</p>
</blockquote>
<blockquote>
<p>The dataset presented here contains measurements of weight and tar, nicotine, and carbon monoxide (CO) content for 25 brands of cigarettes. The data were taken from <span class="citation">Mendenhall and Sincich (<a href="#ref-mendenhall1992statistics" role="doc-biblioref">2012</a>)</span>. The original source of the data is the Federal Trade Commission.”</p>
</blockquote>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="multiple-regression.html#cb96-1" aria-hidden="true" tabindex="-1"></a>cig <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="st">&quot;data/cigarettes.txt&quot;</span>, <span class="at">head =</span> T)</span>
<span id="cb96-2"><a href="multiple-regression.html#cb96-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(cig)</span></code></pre></div>
<pre><code>##           Brand  tar nicotine weight   co
## 1        Alpine 14.1     0.86 0.9853 13.6
## 2 Benson&amp;Hedges 16.0     1.06 1.0938 16.6
## 3    BullDurham 29.8     2.03 1.1650 23.5
## 4   CamelLights  8.0     0.67 0.9280 10.2
## 5       Carlton  4.1     0.40 0.9462  5.4
## 6  Chesterfield 15.0     1.04 0.8885 15.0</code></pre>
<p>(Note that the first variable in the data set is a character string that gives the brand name of each cigarette. In this case, we do not want to treat this as a categorical predictor, so we exclude the <code>stringsAsFactors = T</code> argument from the <code>read.table</code> command.)</p>
<p>Here is a pairs plot of the data:</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="multiple-regression.html#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(cig[, <span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>])</span></code></pre></div>
<p><img src="02-MultipleRegression_files/figure-html/unnamed-chunk-27-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>We wish to use tar content, nicotine content, and weight to build a predictive model of carbon monoxide content.<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a> Let’s first observe that, when considered on their own, both tar content and nicotine content have strongly significant associations with carbon monoxide content, as the pairs plot suggests.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="multiple-regression.html#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(co <span class="sc">~</span> tar, <span class="at">data =</span> cig))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = co ~ tar, data = cig)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.1124 -0.7167 -0.3754  1.0091  2.5450 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.74328    0.67521   4.063 0.000481 ***
## tar          0.80098    0.05032  15.918 6.55e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.397 on 23 degrees of freedom
## Multiple R-squared:  0.9168, Adjusted R-squared:  0.9132 
## F-statistic: 253.4 on 1 and 23 DF,  p-value: 6.552e-14</code></pre>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="multiple-regression.html#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(co <span class="sc">~</span> nicotine, <span class="at">data =</span> cig))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = co ~ nicotine, data = cig)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.3273 -1.2228  0.2304  1.2700  3.9357 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   1.6647     0.9936   1.675    0.107    
## nicotine     12.3954     1.0542  11.759 3.31e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.828 on 23 degrees of freedom
## Multiple R-squared:  0.8574, Adjusted R-squared:  0.8512 
## F-statistic: 138.3 on 1 and 23 DF,  p-value: 3.312e-11</code></pre>
<p>Yet in a model that includes all three predictors, only tar content seems to be statistically significant:</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="multiple-regression.html#cb103-1" aria-hidden="true" tabindex="-1"></a>cig.model <span class="ot">&lt;-</span> <span class="fu">lm</span>(co <span class="sc">~</span> tar <span class="sc">+</span> nicotine <span class="sc">+</span> weight, <span class="at">data =</span> cig)</span>
<span id="cb103-2"><a href="multiple-regression.html#cb103-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(cig.model)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = co ~ tar + nicotine + weight, data = cig)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.89261 -0.78269  0.00428  0.92891  2.45082 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   3.2022     3.4618   0.925 0.365464    
## tar           0.9626     0.2422   3.974 0.000692 ***
## nicotine     -2.6317     3.9006  -0.675 0.507234    
## weight       -0.1305     3.8853  -0.034 0.973527    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.446 on 21 degrees of freedom
## Multiple R-squared:  0.9186, Adjusted R-squared:  0.907 
## F-statistic: 78.98 on 3 and 21 DF,  p-value: 1.329e-11</code></pre>
<p>The multiple regression model suggests that if we compare cigarettes with the same nicotine content (and weight), then there will be a (strongly) significant statistical association between tar content and carbon monoxide content. On the other hand, if we compare cigarettes with the same tar content (and weight), then there will not be a significant association between the nicotine content and the carbon monoxide content. This seems like a strong distinction. Do we trust it?</p>
<p>The issue here is that tar and nicotine content are strongly correlated with one another, raising legitimate questions about whether we can separate the associations between one of the predictors and the response from the other. Indeed, in light of the strong correlation between tar content and nicotine content, does our comparative interpretation of the regression coefficients even make sense? If two cigarettes have the same nicotine content and weight, then by how much can their tar content differ?</p>
<p>This is the issue of collinearity. Perfect collinearity occurs when two predictors are perfectly correlated with one another. Perfect collinearity is rare (unless the number of predictors exceeds the number of data points, in which case it is inevitable). However, if predictors are strongly (but nor perfectly) correlated, trouble still lurks. Indeed, collinearity is not just caused by strong correlations between pairs of predictors: It can also be caused by strong correlations between weighted sums of predictors. For this reason, when there are many predictors relative to the number of data points, collinearity is nearly inevitable.</p>
<!-- Mathematically, calculating $\left(\X'\X\right)^{-1}$ with strong (but not perfect) collinearity is numerically unstable, and tends to magnify rounding errors (think of dividing by a number very close to, but not equal to zero).  Geometrically, the "plane" that we are trying to fit to the cloud of data points is not well anchored.  It is unstable, in the sense that small changes in the data can have large impacts on the estimated regression coefficients.   -->
<p>The usual guidance is that collinearity makes the estimated regression coefficients unreliable or unstable, in the sense that small changes in the data set can trigger large changes in the model fit (<span class="citation">Bowerman and O’Connell (<a href="#ref-bowerman1990linear" role="doc-biblioref">1990</a>)</span>). This sensitivity to small changes makes it difficult, if not impossible, to have confidence in our inferences about the estimated partial regression coefficients.</p>
<p>Collinearity is not a problem for prediction, however. As <span class="citation">Quinn and Keough (<a href="#ref-quinn2002experimental" role="doc-biblioref">2002</a>)</span> (p. 127) say:</p>
<blockquote>
<p>As long as we are not extrapolating beyond the range of our predictor variables and we are making predictions from data with a similar pattern of collinearity as the data to which we fitted our model, collinearity doesn’t necessarily prevent us from estimating a regression model that fits the data well and has good predictive power (<span class="citation">Rawlings, Pantula, and Dickey (<a href="#ref-rawlings1998applied" role="doc-biblioref">1998</a>)</span>). It does, however, mean that we are not confident in our estimates of the model parameters. A different sample from the same population of observations, even using the same values of the predictor variables, might produce very different parameter estimates.</p>
</blockquote>
<p>In other words, we can still use our regression model for prediction, as long as we supply values of tar content and nicotine content that are consistent with the strong correlation between those two variables in the original data set.</p>
<p>Collinearity is usually assessed by a variance inflation factor (VIF). The VIF is so named because it measures the amount by which the correlations among the predictors increase the standard error (and thus the variance) of the estimated regression coefficients. A separate VIF can be calculated for each predictor in the model. There is a relatively simple recipe for calculating VIFs that is somewhat edifying, although it’s easier to let the software compute the VIFs for you. Here’s the recipe if you are interested:</p>
<p>To calculate the VIF for predictor <span class="math inline">\(x_j\)</span>, do the following:</p>
<ol style="list-style-type: decimal">
<li><p>Regress <span class="math inline">\(x_j\)</span> against all other predictors. That is, fit a new regression model in which <span class="math inline">\(x_j\)</span> is the response. Note that the actual response <span class="math inline">\(y\)</span> is not included in this model.</p></li>
<li><p>Calculate <span class="math inline">\(R^2\)</span>.</p></li>
<li><p>The VIF associated with predictor <span class="math inline">\(x_j\)</span> is <span class="math inline">\(1/\left(1-R^2 \right)\)</span>.</p></li>
</ol>
<p>The interesting feature of the recipe is that it depends only on the values of the predictors — the response <span class="math inline">\(y\)</span> plays no part. The recipe also has a certain logic, in the sense that if a predictor is strongly collinear with the other predictors in the model, then we should be able to predict that predictor well using the other predictors in the model. The reason why the VIF is calculated as <span class="math inline">\(1/\left(1-R^2 \right)\)</span> in the last step is a bit of a mystery, and has to do with the fact that VIFs were originally developed to measure the increase in the variance of the regression coefficients caused by the collinearity.</p>
<p>Here, we’ll use the <code>vif</code> function found in the <code>car</code> package to compute the VIFs for the cigarette model:</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="multiple-regression.html#cb105-1" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">vif</span>(cig.model)</span></code></pre></div>
<pre><code>##       tar  nicotine    weight 
## 21.630706 21.899917  1.333859</code></pre>
<p>Larger values of VIF indicate stronger collinearity. For the cigarette data, the VIFs tell us that both tar and nicotine are strongly collinear with the other predictors in the model, but weight is not strongly collinear with tar and nicotine.</p>
<p>So that’s the good news: that collinearity is readily measured. The bad news is two-fold. First, VIFs are a continuous measure. The natural question is how large the VIF needs to be before one needs to worry about it. There’s no bright line to be found here. Most texts suggests that a VIF <span class="math inline">\(\geq\)</span> 10 indicates strong enough collinearity that additional measures should be taken. As always, don’t take the bright-line aspect of this rule too seriously; a VIF of 9.9 is not meaningfully different from a VIF of 10.1.</p>
<p>The second half of the bad news is that there’s no easy fix for collinearity. In some sense, this is just a statement of common sense: If two predictors and a response vary together, then it is difficult (if not impossible) to tease apart the effect of one predictor on the response from the effect of the other predictor with a statistical model. With the cigarette data, for example, if we really wanted to characterize the effects of tar and nicotine content separately, then we’d need to find some cigarettes with high tar content and low nicotine content, or vice versa.</p>
<p>The usual recommendations for coping with collinearity attempt to stabilize the estimated partial regression coefficients at the expense of accepting a (hopefully) small bias.<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a> Here are two:</p>
<ol style="list-style-type: decimal">
<li><p>Omit predictors that are highly correlated with other predictors in the model. The rationale here is that highly correlated predictors may just be redundant measures of the same thing. As an example, in the cigarette data, tar content and nicotine content may both be driven by the same underlying features of the cigarette’s ingredients. If this is true, there is little to be gained by including both variables as predictors in a regression model.</p></li>
<li><p>Use principal components analysis (PCA) to reduce the number of predictors, and use principal components as predictors. PCA is a multivariate statistical method that takes several variables and produces a smaller number of new variables (the “principal components”) that contain the majority of the information in the original variables. The advantage of using a principal component as a predictor is that different principal components are guaranteed to be independent of one another, by virtue of how they are calculated. The major disadvantage of using principal components is that the newly created predictors (the principal components) are amalgams of the original variables, and thus it is more difficult to assign a scientific interpretation to the partial regression coefficients associated with each. So, using PCA to find new predictors yields a more robust statistical model, albeit at the expense of reduced interpretability.</p></li>
</ol>
</div>
<div id="variable-selection-choosing-the-best-model" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Variable selection: Choosing the best model<a href="multiple-regression.html#variable-selection-choosing-the-best-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far, we’ve learned how to construct and fit regression models that can potentially include many different types of terms, including multiple predictors, transformations of the predictors, indicator variables for categorical predictors, interactions, and polynomial terms. Even a small set of possible predictors can produce a large array of possible models. How do we go about choosing the “best” model?</p>
<p>First, we have to define what we mean by a “best” model. What are the qualities of a good model?</p>
<ol style="list-style-type: decimal">
<li><p><em>Parsimony</em>. We seek a model that adequately captures the “signal” in the data, and no more. There are both philosophical and statistical reasons for seeking a parsimonious model. The statistical motivation is that a model with too many unnecessary predictors is prone to detect spurious patterns that would not appear in repeated samples from the same population. We call this phenomenon “overfitting” or “fitting the noise”. In technical terms, fitting a model with too many unnecessary predictors increases the standard errors of the parameter estimates.</p></li>
<li><p><em>Interpretability</em>. We often want to use regression models to understand associations between predictors and the response and to shed light on the data-generating process. This argues for keeping models simple. There are occasions where the sole goal of regression modeling is prediction and in this case interpretability is less important. This is often the case in so-called “big data” applications, where prediction is the primary goal and understanding is only secondary.</p></li>
<li><p><em>Statistical inference</em>. As scientists, we are not interested merely in describing patterns in our data set. Instead, we want to use the data to draw inferences about the population from which the sample was drawn. Therefore, we want a model that meets the assumptions of regression so that we can use regression theory to draw statistical inferences.</p></li>
</ol>
<p>In statistical jargon, the process of choosing which predictors to include in a regression model and which to leave out is called <em>variable selection</em>. More generally, beyond a regression context, the problem of identifying the best statistical model is called <em>model selection</em>.</p>
<p>We will look at several automated routines for choosing the best model. Helpful as these routines are, they are no substitute for intelligent analysis. Feel free to use an automated variable selection route to get started, but don’t throw your brain out the window in the process. Also, remember that there is a hierarchy among model terms in regression. Most automated variable selection routines do not incorporate this hierarchy, so we must impose it ourselves. In most cases, the following rules should be followed:</p>
<ol style="list-style-type: decimal">
<li>Models that include an interaction between predictors should also include the predictors individually.</li>
<li>Models that include polynomial powers of a predictor should include all lower-order terms of that predictor.</li>
</ol>
<p>Automated variable selection routines can be grouped into two types: ranking methods and sequential methods. Cross-validation is a type of ranking method that is popular in machine learning. It is important enough that we will study it in more depth. But first we need a word of caution.</p>
<div id="model-selection-and-inference" class="section level3 hasAnchor" number="2.6.1">
<h3><span class="header-section-number">2.6.1</span> Model selection and inference<a href="multiple-regression.html#model-selection-and-inference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Model selection of any sort destroys the opportunity for statistical inference, because any model that has been identified through model selection is bound to have predictors that are associated with the response. Inference procedures only have their nominal properties (coverage rates for confidence intervals, error rates for hypothesis tests) when the plan for statistical analysis is determined <em>before</em> looking at the data.</p>
<p>A thought experiment makes this clear. Consider a hypothetical data set with a single response and a large number (10? 100? 1000?) of candidate predictors that are perfectly uncorrelated with the response in the population from which the data are sampled. In any sample from that population, model selection will identify the predictors that are spuriously correlated with the response in that particular sample. If one then conducts the usual hypothesis tests on those predictors in the best-fitting model, the chances that the predictors will be spuriously statistically significant will be much larger than the nominal error rate of the test.</p>
<p>This seems to present a problem for practical analysis. To be sure, there are some particular instances in which analyses are specified before data are observed, namely studies in which either the government or the analyst’s scruples motivate them to preregister the study beforehand. In these cases, the inference procedures have their nominal properties, and accordingly we may place great faith in the results. In all other cases, though, most statistical analysis follows some sort of model selection, whether that selection is conducted formally using the methods such as those described below or informally via a casual glance at the data. Indeed, this is likely the norm in academic science. When inference follows model selection, any subsequent statistical inferences should be regarded as descriptive with unknown statistical properties (<span class="citation">Berry (<a href="#ref-berry2016p" role="doc-biblioref">2016</a>)</span>).</p>
</div>
<div id="ranking-methods" class="section level3 hasAnchor" number="2.6.2">
<h3><span class="header-section-number">2.6.2</span> Ranking methods<a href="multiple-regression.html#ranking-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Ranking methods work best when it is computationally feasible to fit every possible candidate model. In these situations, we calculate a metric that quantifies the model’s adequacy, and select the model that scores the best with respect to that metric. There are several possible metrics to choose from, and they don’t always point to the same best model. We will look at three different metric that enjoy wide use.</p>
<p>Throughout this section, we will refer to the number of predictors in a model, and denote this quantity by <span class="math inline">\(k\)</span>. To remove ambiguity, what we mean in this case is the number of partial regression coefficients to be estimated. So, for example, we would say that the model <span class="math inline">\(y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1^2 + \beta_4 x_1 x_2 + \varepsilon\)</span> has <span class="math inline">\(k = 4\)</span> predictors.</p>
<p>Before beginning, we should note that <span class="math inline">\(R^2\)</span> is <em>not</em> a good choice for a ranking metric. This is because adding a predictor will never decrease <span class="math inline">\(R^2\)</span>. Therefore, <span class="math inline">\(R^2\)</span> can only be used to compare models that have the same number of predictors.</p>
<ol style="list-style-type: decimal">
<li><p>Adjusted <span class="math inline">\(R^2\)</span>. Adjusted <span class="math inline">\(R^2\)</span> is a penalized version of <span class="math inline">\(R^2\)</span> that imposes a penalty for each additional parameter added to the model. The (rather opaque) formula for adjusted <span class="math inline">\(R^2\)</span> is
<span class="math display">\[
{\rm Adj-}R^2 =1-\frac{n-1}{n-\left(k+1\right)} \left(1-R^2 \right)
\]</span>
The model with the largest Adj-<span class="math inline">\(R^2\)</span> is considered best.</p></li>
<li><p>AIC (Akaike’s Information Criterion)</p>
<p>AIC is also a penalized goodness-of-fit measure, like adjusted <span class="math inline">\(R^2\)</span>. AIC enjoys a bit more theoretical support than adjusted <span class="math inline">\(R^2\)</span> and is more versatile, although its derivation is a bit more opaque. (As the name suggests, AIC has its roots in information theory.) The general form of AIC involves math that is beyond the scope of ST 512, but we can write down the specific formula for regression models, which is
<span class="math display">\[
AIC=n\ln \left[\frac{SSE}{n} \right]+2\left(k+1\right)
\]</span>
The smallest value of AIC is best. (Smaller here means algebraically smaller — that is, further to the left on the number line — not closer to zero.) Despite its theoretical support, AIC tends to favor models with too many predictors.</p></li>
</ol>
<!-- Example of various ranking criteria with the cheese data: -->
<!-- \begin{center} -->
<!-- \begin{tabular}{|p{1.2in}|p{0.7in}|p{0.7in}|p{0.7in}|p{0.6in}|} \hline  -->
<!--    predictors & $R^2$ & Adj-$R^2$ & PRESS & AIC \\ \hline  -->
<!--    acetic & .302 & .277 & 5140 & 159.5 \\ \hline  -->
<!--    H2S & .571 & .556 & 3135 & 144.9 \\ \hline  -->
<!--    lactic & .496 & .478 & 3695 & 149.8 \\ \hline  -->
<!--    acetic, H2S & .582 & .551 & 3011 & 146.1 \\ \hline  -->
<!--    acetic, lactic & .520 & .485 & 3461 & 150.3 \\ \hline  -->
<!--    H2S, lactic & .6517 & \textbf{\underbar{.626}} & 2510 & \textbf{\underbar{140.6}} \\ \hline  -->
<!--    acetic, H2S, lactic & .6518 & .612 & \textbf{\underbar{2471}} & 142.6 \\ \hline  -->
<!-- \end{tabular}\end{center} -->
<hr />
<p><span style="color: gray;"> Here’s a bit more about the idea behind AIC. To understand AIC, we first have to understand the notion of Kullback-Leibler (KL) divergence from the field of information theory. Suppose we have a data set in hand. In a very abstract way, we can think about the collection of all possible processes that could have generated those data. Somewhere in that collection is the true data-generating process. While we have no hope of ever finding the true data-generating process, we can propose models for that process, and those models are also in the abstract collection of all possible processes that could have generated our data. The KL divergence is a measure of how much each of the models that we might propose diverges from the true data-generating process. Smaller divergence values mean that the model is closer to the true process.</span></p>
<p><span style="color: gray;"> If we could measure the KL divergence from the truth to each of our candidate models, then we would favor the model with the smallest divergence. But we can’t measure the KL divergence, because we can never find the true data-generating process. Akaike’s genius was to show that we could develop an asymptotically unbiased estimate of the KL divergence plus an unknown constant that depends only on the data set in hand. (“Unbiased” in this sense means that the estimate is neither systematically too big or systematically too small. “Asymptotically unbiased” means that the bias of the estimate only vanishes as the data set becomes large. We’ll say more about this below.) Akaike named this value AIC, for “an information criterion”, although today the “A” is often understood to stand for Akaike, in his honor.</span></p>
<p><span style="color: gray;"> Because AIC only estimates the KL divergence plus some unknown constant, any one value of AIC is meaningless, because we never know the value of that constant that is baked in. But if our goal is to compare several models and determine which one has the smallest KL divergence, then we can compare AIC values for models fit to the same data set, because the unknown constant — whatever it is — will be the same for all the models that we compare. Thus, if we compute the AIC values for several models fit to the same data, we should favor the model with the smallest AIC.</span></p>
<p><span style="color: gray;"> There are a number of important caveats to using AIC, all of which are foreshadowed by the explanation above. First, AIC values cannot be compared between models fit to different data sets. Those comparisons have no meaning. Second, AIC is only an estimate of the KL divergence. Like any estimate, it is contaminated with error, yet (to the best of my knowledge) no one has ever come up with a good method for quantifying that error. (In other words, there’s a standard error associated with AIC, but no one knows how to compute it.) Third, AIC is only “asymptotically unbiased”. Asymptotic unbiasedness is a quality that we hope all good statistical estimators possess, because it assures us that our estimates will get better as we collect more data. But it is a weak desideratum nevertheless, because it only guarantees that AIC will accurately estimate the KL divergence for very large data sets. With small data sets, all bets are basically off, but this doesn’t usually faze anyone from using AIC for small data sets.<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a> There is a small-sample correction available for AIC, known as AICc (the second “c” stands for corrected), but the properties of AICc are only known for normally distributed data. How small is small in this case? That’s a good question, and it lacks a satisfying answer.</span></p>
<hr />
</div>
<div id="cross-validation" class="section level3 hasAnchor" number="2.6.3">
<h3><span class="header-section-number">2.6.3</span> Cross-validation<a href="multiple-regression.html#cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Cross validation is a type of ranking method that is popular in machine learning. The appeal of cross validation is that it doesn’t rely on any abstract statistical ideas; instead, the ideas flow directly from common sense. The difficulty lies in the fact that implementing a cross-validation procedure requires a small bit of programming chops. As of this writing, the <code>R</code> package <code>caret</code> (an acronym for classification and regression training) includes routines that do most of the hard programming work for you. Cross-validation is most compelling when we wish to use our regression model primarily for prediction.</p>
<p>The basic idea of cross-validation is that a good regression model (especially a good predictive mode) should make accurate predictions. At first, this may not seem like a useful observation: it’s usually hard to collect new data that we can use to evaluate how well our candidate models make predictions. The genius of cross-validation is that if we have enough data to begin with, we don’t have to collect new data to evaluate predictive performance. Instead, we can mimic the collection of new data by splitting our data into two subsets, fitting the model to the first subset, and then using that fit to predict the responses in the second subset. Hence the name “cross-validation”: we are validating our model by seeing how well it predicts data that we already have in hand.</p>
<p>The important point to note here is that we do <em>not</em> use the same data both for fitting and for prediction. Doing so would be circular and would exaggerate the predictive ability of the model. Of course a model should make good predictions for the data that were used to fit the model in the first place!</p>
<p>When we split the data, the subset to which the data are fit is called the training subset, and the subset which are held out for prediction is called the testing subset. Usually the training subset is larger than the testing subset. Of course, if we can split the data once, then we can split it many times, and we obtain a better idea of how well our model makes predictions by averaging its predictive performance over many splits. To quantify predictive performance at each split, we can use any sensible measure of predictive accuracy such as the mean-squared error of the predictions (or more commonly the square root of the mean squared error) or mean absolute error of the predictions. The averaged predictive performance over many splits provides a measure of prediction accuracy that we can use to rank candidate models.</p>
<p>There are many variations on this theme. <span class="math inline">\(K\)</span>-fold cross-validation splits the data into <span class="math inline">\(k\)</span> different subsets (or folds) of roughly equal size, and then uses each fold once as the testing subset, for a total of <span class="math inline">\(k\)</span> different splits. Leave-one-out cross validation is <span class="math inline">\(k\)</span>-fold cross validation with <span class="math inline">\(k=n\)</span> folds. In other words, the testing subset in each split consists of a single data point. As it happens, leave-one-out cross validation combined with the use the sum of squared prediction errors as the measure of predictive performance produces a special statistic called the PRESS statistic (an acronym for [pre]dicted [s]um of [s]quares). The PRESS statistic is handy because it can be calculated with some clever math and a single fit to the entire data set; it doesn’t actually require going to the trouble of splitting the data and re-fitting the model <span class="math inline">\(n\)</span> times. The PRESS statistic was popular before the advent of machine learning, and you’ll find software routines that are able to compute it.</p>
</div>
<div id="sequential-methods" class="section level3 hasAnchor" number="2.6.4">
<h3><span class="header-section-number">2.6.4</span> Sequential methods<a href="multiple-regression.html#sequential-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Sequential methods work best for problems where the set of candidate models is so vast that fitting all the candidate models is not feasible. Because computers are faster today than they were years ago, it is now feasible to fit a large number of candidate models quickly, and thus sequential methods are less necessary today than they were years ago. Nevertheless, the ideas are straightforward.</p>
<p>There are three different types of sequential methods, based on the direction in which the model is built. In <em>forward selection</em>, we begin with the simplest possible model (namely, <span class="math inline">\(y = \beta_0 + \varepsilon\)</span>), and grow the model by adding predictors to it. In <em>backwards elimination</em>, we start with the most expansive possible model and shrink it by removing unnecessary predictors. In <em>stepwise selection</em>, we start with the simplest possible model (namely, <span class="math inline">\(y = \beta_0 + \varepsilon\)</span>) and then merge forwards and backwards steps, either growing and shrinking the model until converging on one that cannot be improved. Stepwise selection is used more often than the other two variations. Each of the three procedures could possibly lead to a different best model.</p>
<p>Here is the algorithm for forward selection:</p>
<ol style="list-style-type: decimal">
<li><p>Initiation step: Start with the model <span class="math inline">\(y=\beta_0 +\varepsilon\)</span>. This is the initial “working” model.</p></li>
<li><p>Iteration step: Fit a set of candidate models, each of which differs from the working model by the inclusion of a single additional model term. Be aware that the set of candidate models must abide our rules of thumb about hierarchy. (That is, we wouldn’t consider a model like <span class="math inline">\(y = \beta_0 + \beta_1 x_1 x_2 + \varepsilon\)</span>.)</p></li>
<li><p>Ask: Do any of the candidate models improve upon the working model?</p>
<ul>
<li>If the answer to this question is “yes”, then choose the model that improves upon the working model the most. Making this model the working model, and begin a new iteration (return to step 2).</li>
<li>(Termination step): If the answer to this question is “no”, then stop. The current working model is the final model.</li>
</ul></li>
</ol>
<p>The algorithm for backwards elimination is similar:</p>
<ol style="list-style-type: decimal">
<li><p>Initiation step: Start with the most expansive possible model. Usually, this will be the model with all possible predictors, and potentially with all possible first- (and conceivably second-) order interactions. Note that we can consider a quadratic term to be an interaction of a predictor with itself in this context. This is the initial “working” model.</p></li>
<li><p>Iteration step: Fit a set of candidate models, each of which differs from the working model by the elimination of a single model term. Again, be aware that the set of candidate models must abide our rules of thumb about hierarchy, so (for example) we wouldn’t consider a model that removes <span class="math inline">\(x_1\)</span> if the interaction <span class="math inline">\(x_1 x_2\)</span> is still in the model.</p></li>
<li><p>(Same as forward selection.) Ask: Do any of the candidate models improve upon the working model?</p>
<ul>
<li>If the answer to this question is “yes”, then choose the model that improves upon the working model the most. Making this model the working model, and begin a new iteration (return to step 2).</li>
<li>(Termination step): If the answer to this question is “no”, then stop. The current working model is the final model.</li>
</ul></li>
</ol>
<p>The algorithm for stepwise selection initiates the algorithm with <span class="math inline">\(y=\beta_0 +\varepsilon\)</span>, and then forms a set of candidate models that differ from the working model by either the addition or elimination of a single model term. The algorithm proceeds until the working model cannot be improved either by a single addition or elimination.</p>
<p>So far, we have been silent about how we determine whether a candidate model improves on the working model, and if so, how to find the candidate model that offers the most improvement. We can use any of our ranking methods at this step. Historically, <span class="math inline">\(p\)</span>-values have often been used to determine whether a candidate model improves on the working model, though this practice has largely been discontinued. The argument against it is that using <span class="math inline">\(p\)</span>-values for variable selection destroys their interpretation in the context of hypothesis testing. This being said, <em>any</em> statistical tests can only be regarded as descriptive if the tests occur in the context of a model that has been identified by model selection. Statistical tests only have their advertised properties if decisions about which predictors to include are made <em>before</em> looking at the data.</p>
<p>The `step’ routine in R uses AIC as its default criterion for adding or dropping terms in stepwise selection. Here is an example of stepwise selection with the cheese data, considering only models without interactions or polynomial terms.</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="multiple-regression.html#cb107-1" aria-hidden="true" tabindex="-1"></a>fm0 <span class="ot">&lt;-</span> <span class="fu">lm</span>(taste <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> cheese)  <span class="co"># the initial model y = b0 + eps</span></span>
<span id="cb107-2"><a href="multiple-regression.html#cb107-2" aria-hidden="true" tabindex="-1"></a><span class="fu">step</span>(fm0, taste <span class="sc">~</span> Acetic <span class="sc">+</span> H2S <span class="sc">+</span> Lactic, <span class="at">data =</span> cheese)  </span></code></pre></div>
<pre><code>## Start:  AIC=168.29
## taste ~ 1
## 
##          Df Sum of Sq    RSS    AIC
## + H2S     1    4376.7 3286.1 144.89
## + Lactic  1    3800.4 3862.5 149.74
## + Acetic  1    2314.1 5348.7 159.50
## &lt;none&gt;                7662.9 168.29
## 
## Step:  AIC=144.89
## taste ~ H2S
## 
##          Df Sum of Sq    RSS    AIC
## + Lactic  1     617.2 2669.0 140.65
## &lt;none&gt;                3286.1 144.89
## + Acetic  1      84.4 3201.7 146.11
## - H2S     1    4376.7 7662.9 168.29
## 
## Step:  AIC=140.65
## taste ~ H2S + Lactic
## 
##          Df Sum of Sq    RSS    AIC
## &lt;none&gt;                2669.0 140.65
## + Acetic  1      0.55 2668.4 142.64
## - Lactic  1    617.18 3286.1 144.89
## - H2S     1   1193.52 3862.5 149.74</code></pre>
<pre><code>## 
## Call:
## lm(formula = taste ~ H2S + Lactic, data = cheese)
## 
## Coefficients:
## (Intercept)          H2S       Lactic  
##     -27.592        3.946       19.887</code></pre>
<p>There is no prescription for building models automatically. (If there were, someone would have written a computer package implementing the procedure and would be filthy rich.) Here is one cycle of steps for building a regression model, courtesy of Dr. Roger Woodard, formerly of NCSU:</p>
<ol style="list-style-type: decimal">
<li><p>Examine univariate (ST 511) summaries of the data (summary statistics, boxplots, etc.). Identify unusual values or possible problems. (Don’t take it on faith that your data are all correct!)</p></li>
<li><p>Examine scatterplots with all variables. Find variables that are closely correlated with the response and with each other.</p></li>
<li><p>Candidate model selection: Identify a model that includes relevant variables. Use automated selection procedures if you wish.</p></li>
<li><p>Assumption checking: Check (standardized) residuals. Determine if polynomial terms or transformations may be needed.</p></li>
<li><p>Examine collinearity diagnostics. Inspect VIFs and pairwise correlations between variables. Decide if some variables may be removed or added.</p></li>
<li><p>Revision. Add or remove terms based on steps 4-5. Return to step 3.</p></li>
<li><p>Prediction and testing: Consider validating the model with a sample that was not included in building the model.</p></li>
</ol>
</div>
</div>
<div id="leverage-influential-points-and-standardized-residuals" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Leverage, influential points, and standardized residuals<a href="multiple-regression.html#leverage-influential-points-and-standardized-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="leverage" class="section level3 hasAnchor" number="2.7.1">
<h3><span class="header-section-number">2.7.1</span> Leverage<a href="multiple-regression.html#leverage" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall that in SLR, a data point can have undue influence on the regression model if the value of the predictor, <span class="math inline">\(x\)</span>, is for away from <span class="math inline">\(\bar{x}\)</span>. The same notion applies in MLR: data points can be unduly influential if their combination of predictors lies far away from the “center of mass” of the other predictors. However, with multiple predictors, it is harder to detect influential points visually. The <em>leverage</em> of a data point is a measure of its distance from the center of mass of the predictors, and hence its influence. The formula for calculating leverages is complicated, so we’ll use a computer to calculate them. Leverages are usually denoted with the letter <span class="math inline">\(h\)</span>, so the leverage for the <span class="math inline">\(i\)</span>th data point is <span class="math inline">\(h_i\)</span>. If we looked at the equation for a leverage, however, we would discover that leverages are strictly functions of the predictors, and do not depend on th response.</p>
<p>To calculate leverages in R, first pass the regression model object to the function <code>influence.lm</code>. This function returns several diagnostic measures; the leverages are contained in the component called <code>hat</code>. To extract the leverages, use code like the following:</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="multiple-regression.html#cb110-1" aria-hidden="true" tabindex="-1"></a>fm1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(BAC <span class="sc">~</span> weight <span class="sc">+</span> beers, <span class="at">data =</span> beer)</span>
<span id="cb110-2"><a href="multiple-regression.html#cb110-2" aria-hidden="true" tabindex="-1"></a>fm1.diagnostics <span class="ot">&lt;-</span> <span class="fu">lm.influence</span>(fm1)</span>
<span id="cb110-3"><a href="multiple-regression.html#cb110-3" aria-hidden="true" tabindex="-1"></a>lev <span class="ot">&lt;-</span> fm1.diagnostics<span class="sc">$</span>hat</span></code></pre></div>
<p>Here is a look at some of the leverage values for the BAC data:</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="multiple-regression.html#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">cbind</span>(beer, lev))</span></code></pre></div>
<pre><code>##     BAC weight beers beers.c weight.c       lev
## 1 0.100    132     5  0.1875 -39.5625 0.1118535
## 2 0.030    128     2 -2.8125 -43.5625 0.1948842
## 3 0.190    110     9  4.1875 -61.5625 0.5176556
## 4 0.120    192     8  3.1875  20.4375 0.2029871
## 5 0.040    172     3 -1.8125   0.4375 0.1111125
## 6 0.095    250     7  2.1875  78.4375 0.2588899</code></pre>
<p>Not surprisingly, the point with the greatest leverage is the 110-lb person who drank 9 beers.</p>
<p>What qualifies as a large leverage? It can be shown that, in a regression model with <span class="math inline">\(k\)</span> predictors, the total of the leverages for all the data points must equal <span class="math inline">\(k+1\)</span>. Therefore, a typical leverage value will be <span class="math inline">\((k+1)/n\)</span>.</p>
<!-- For what it's worth, @hoaglin1978hat suggest that data points with leverages greater than twice this value (that is, greater than $2\left(k+1\right)/n$) should be checked, although it is not abundantly clear what it means to check a data point in this context. -->
</div>
<div id="standardized-residuals" class="section level3 hasAnchor" number="2.7.2">
<h3><span class="header-section-number">2.7.2</span> Standardized residuals<a href="multiple-regression.html#standardized-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Data points associated with large leverages tend to have smaller (raw) residuals. A better choice than analyzing the raw residuals is to analyze the standardized residuals. The formula for a standardized residual is:
<span class="math display">\[
e_i^{(s)} =\frac{e_i}{s_\varepsilon \sqrt{1-h_i} }
\]</span></p>
<p>The benefit of standardized residuals is that if our model assumptions are appropriate, then they should behave like an iid sample from a normal distribution with mean 0 and variance 1. That is to say, most standardized residuals should be between -2 and +2, and only a few should be <span class="math inline">\(&lt; -3\)</span> or <span class="math inline">\(&gt; +3\)</span>. Some texts call these “studentized residuals” instead of standardized residuals.</p>
<p>R does not have a built-in function for calculating standardized residuals. However, there is a library of functions called the <code>MASS</code> library that contains the function <code>stdres</code>. (MASS is an acronym for , which is one of the original advanced texts for using R. It is written by W.N. Venables and B.D. Ripley.)</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="multiple-regression.html#cb113-1" aria-hidden="true" tabindex="-1"></a>MASS<span class="sc">::</span><span class="fu">stdres</span>(fm1)</span></code></pre></div>
<pre><code>##          1          2          3          4          5          6          7 
##  0.8307561 -0.3611695  1.4198337 -1.0767727  0.2664003  0.6708175  1.6189097 
##          8          9         10         11         12         13         14 
## -1.6125272 -1.6623992  1.2179975 -0.2650284  0.1241508 -0.8509379 -0.0485307 
##         15         16 
##  1.0854287 -0.6259662</code></pre>
</div>
<div id="cooks-distance" class="section level3 hasAnchor" number="2.7.3">
<h3><span class="header-section-number">2.7.3</span> Cook’s distance<a href="multiple-regression.html#cooks-distance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Leverages and standardized residuals can be combined into various quantities that measure the influence each observation has on the fitted regression line. One of the most popular of these are Cook’s distance, <span class="math inline">\(D_i\)</span>. In R, if you pass a regression model to the command <code>plot</code>, it will produce four (somewhat sophisticated) diagnostic plots. The last of these shows Cook’s distance.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="multiple-regression.html#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fm1)</span></code></pre></div>
<p><img src="02-MultipleRegression_files/figure-html/unnamed-chunk-35-1.png" width="480" style="display: block; margin: auto;" /><img src="02-MultipleRegression_files/figure-html/unnamed-chunk-35-2.png" width="480" style="display: block; margin: auto;" /><img src="02-MultipleRegression_files/figure-html/unnamed-chunk-35-3.png" width="480" style="display: block; margin: auto;" /><img src="02-MultipleRegression_files/figure-html/unnamed-chunk-35-4.png" width="480" style="display: block; margin: auto;" />
Observations with large values of Cook’s distance merit greater scrutiny.</p>
</div>
</div>
<div id="appendix-regression-as-a-linear-algebra-problem" class="section level2 unnumbered hasAnchor">
<h2>Appendix: Regression as a linear algebra problem<a href="multiple-regression.html#appendix-regression-as-a-linear-algebra-problem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This section assumes familiarity with linear algebra.</p>
<p>Ultimately, the linear statistical model (that encompasses regression and ANOVA) is a linear-algebra problem. In short, the least-squares estimates are found by projecting the data vector (an element of <span class="math inline">\(\mathcal{R}^n\)</span>) onto the linear subspace of <span class="math inline">\(\mathcal{R}^n\)</span> spanned by the predictors.</p>
<p>In matrix notation, the regression equations for a model with <span class="math inline">\(k\)</span> predictors can be written compactly as
<span class="math display">\[
\mathbf{Y}= \mathbf{X}\mathbf{\beta}+ \mathbf{\epsilon}
\]</span>
where
<span class="math display">\[
\mathbf{Y}= \left[\begin{array}{c} y_1 \\ y_2 \\ \vdots  \\ y_n \end{array}\right],
\]</span>
<span class="math display">\[
\mathbf{X}= \left[\begin{array}{cccc} 1 &amp; x_{11} &amp; \cdots &amp; x_{1k} \\ 1 &amp; x_{21} &amp; \cdots&amp; x_{2k} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots  \\ 1 &amp; x_{n1} &amp; \cdots &amp;  x_{nk} \end{array}\right],
\]</span>
<span class="math display">\[
\mathbf{\beta}=\left[\begin{array}{c} \beta_0  \\ \beta_1 \\ \vdots \\ \beta_k  \end{array}\right],
\]</span>
<span class="math display">\[
\mathbf{\epsilon}=\left[\begin{array}{c} \varepsilon_1 \\ \varepsilon_2  \\ \vdots \\ \varepsilon_n  \end{array}\right].
\]</span>
The most important component of the above equation is the <span class="math inline">\(\mathbf{X}\)</span> matrix, also called the design matrix. Here are the first few rows of the design matrix for the BAC regression that includes beers consumed and weight as the two predictors:
<span class="math display">\[
\mathbf{X}=\left[\begin{array}{ccc} {1} &amp; {5} &amp; {132} \\ {1} &amp; {2} &amp; {128} \\ {1} &amp; {9} &amp; {110} \\ {1} &amp; {8} &amp; {192} \\ {\vdots } &amp; {\vdots } &amp; {\vdots } \end{array}\right]
\]</span></p>
<p>The short of the long is that the vector of least-squares estimates can be found by the matrix equation
<span class="math display">\[
\hat{\mathbf{\beta}}= \left(\mathbf{X}&#39;\mathbf{X}\right)^{-1} \mathbf{X}&#39;\mathbf{Y}
\]</span></p>
<div id="singular-or-pathological-design-matrices" class="section level3 unnumbered hasAnchor">
<h3>Singular, or pathological, design matrices<a href="multiple-regression.html#singular-or-pathological-design-matrices" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The power and beauty of the equation <span class="math inline">\(\hat{\mathbf{\beta}}= \left(\mathbf{X}&#39;\mathbf{X}\right)^{-1} \mathbf{X}&#39;\mathbf{Y}\)</span> is that it works for any regression or ANOVA model, not just SLR. However, the matrix inverse <span class="math inline">\(\left(\mathbf{X}&#39;\mathbf{X}\right)^{-1}\)</span> does not exist for every possible choice of <span class="math inline">\(\mathbf{X}\)</span> matrices. Roughly, there are some pathological <span class="math inline">\(\mathbf{X}\)</span> matrices for which trying to find the matrix inverse <span class="math inline">\(\left(\mathbf{X}&#39;\mathbf{X}\right)^{-1}\)</span> is equivalent to dividing by zero. For example, suppose you are studying the effect of temperature on weight gain in fish, and measure temperature in both degrees Fahrenheit and Centigrade. (Recall that one can convert between Fahrenheit and Centigrade by the equation F = (9/5) C + 32.) The design matrix might be
<span class="math display">\[
\mathbf{X}=\left[\begin{array}{ccc} {1} &amp; {5} &amp; {41} \\ {1} &amp; {10} &amp; {50} \\ {1} &amp; {15} &amp; {59} \\ {1} &amp; {20} &amp; {68} \end{array}\right]
\]</span>
where the predictor in the 2<span class="math inline">\(^{nd}\)</span> column is degrees Centigrade and the predictor in the 3<span class="math inline">\(^{rd}\)</span> column is degrees Fahrenheit. Let’s try to fit a regression model in R for some made up values of <span class="math inline">\(y\)</span>:</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="multiple-regression.html#cb116-1" aria-hidden="true" tabindex="-1"></a>bad.data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">y    =</span> <span class="fu">c</span>(<span class="fl">2.4</span>, <span class="fl">6.1</span>, <span class="fl">4.4</span>, <span class="fl">7.0</span>),</span>
<span id="cb116-2"><a href="multiple-regression.html#cb116-2" aria-hidden="true" tabindex="-1"></a>                       <span class="at">degC =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">20</span>),</span>
<span id="cb116-3"><a href="multiple-regression.html#cb116-3" aria-hidden="true" tabindex="-1"></a>                       <span class="at">degF =</span> <span class="fu">c</span>(<span class="dv">41</span>, <span class="dv">50</span>, <span class="dv">59</span>, <span class="dv">68</span>))</span>
<span id="cb116-4"><a href="multiple-regression.html#cb116-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(y <span class="sc">~</span> degC <span class="sc">+</span> degF, <span class="at">data =</span> bad.data))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ degC + degF, data = bad.data)
## 
## Residuals:
##     1     2     3     4 
## -0.76  1.73 -1.18  0.21 
## 
## Coefficients: (1 not defined because of singularities)
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)   1.9500     1.9378   1.006    0.420
## degC          0.2420     0.1415   1.710    0.229
## degF              NA         NA      NA       NA
## 
## Residual standard error: 1.582 on 2 degrees of freedom
## Multiple R-squared:  0.5938, Adjusted R-squared:  0.3908 
## F-statistic: 2.924 on 1 and 2 DF,  p-value: 0.2294</code></pre>
<p>The <code>NA</code> values for the partial regression coefficient associated with degrees Fahrenheit tells us that something has gone awry.</p>
<p>While this may seem to be cause for concern, all that is happening here is that the model contains two predictors that are perfectly correlated with one another. (Actually, <span class="math inline">\(\left(\mathbf{X}&#39;\mathbf{X}\right)^{-1}\)</span> will fail to exist if the regression contains two different weighted sums of predictors that are perfectly correlated with one another.) When regression models contain perfectly correlated predictors (or perfectly correlated weighted sums of predictors), then there is no way to separate the linear associations between each of the (combinations of) predictors and the response. Thus, the inability to find least-squares regression estimates in some cases is just the logical consequence of the fact that it is impossible to separate the effects of two perfectly correlated predictors on a single response.</p>
<p>Unfortunately, the computer output doesn’t provide a plain-language explanation of the problem. Each software package behaves a bit differently when faced with a case where the matrix inverse <span class="math inline">\(\left(\mathbf{X}&#39;\mathbf{X}\right)^{-1}\)</span> does not exist. In R, the program <code>lm</code> lops off columns of the design matrix (starting from the right-most column) until it obtains a design matrix where the inverse <span class="math inline">\(\left(\mathbf{X}&#39;\mathbf{X}\right)^{-1}\)</span> does exist, and the computations can proceed. This is why the output above does not provide values for the regression coefficient associated with degrees Fahrenheit. There is nothing about degrees Fahrenheit as opposed to degrees Celsius that makes degrees Fahrenheit a problematic predictor. It is simply that the two predictors are perfectly correlated, so <code>lm</code> responds by deleting the last predictor given — in this case degrees Fahrenheit — and proceeding.</p>
<!-- What is the common feature of design matrices that allows us to determine whether or not $\left(\X'\X\right)^{-1}$ exists?  In mathematical terms, the matrix inverse $\left(\X'\X \right)^{-1}$ will not exist if the columns of the design matrix contain a *linear dependency*.  Let's write each column of the design matrix as the vector $\vecx$, i.e., for the temperature example above,  -->
<!-- \[ -->
<!-- x_0 =\left[\begin{array}{c} {1} \\ {1} \\ {1} \\ {1} \end{array}\right],x_1 =\left[\begin{array}{c} {5} \\ {10} \\ {15} \\ {20} \end{array}\right],x_2 =\left[\begin{array}{c} {41} \\ {50} \\ {59} \\ {68} \end{array}\right] -->
<!-- \]  -->
<!-- A design matrix with $k+1$ columns has a linear dependency if and only if there exist a set of constants $\lambda_0 ,\lambda_1 ,\lambda_2 ,\ldots ,\lambda_k$ such that  -->
<!-- \[ -->
<!-- \lambda_0 x_0 +\lambda_1 x_1 +...+\lambda_k x_k =\left[\begin{array}{c} {0} \\ {0} \\ {\vdots } \\ {0} \end{array}\right] -->
<!-- \]  -->
<!-- and at least one of the $\lambda$'s is $\ne 0$.  Thus, the temperature example has a linear dependency because -->
<!-- \[ -->
<!-- -32\left[\begin{array}{c} {1} \\ {1} \\ {1} \\ {1} \end{array}\right]-\frac{9}{5} \left[\begin{array}{c} {5} \\ {10} \\ {15} \\ {20} \end{array}\right]+\left[\begin{array}{c} {41} \\ {50} \\ {59} \\ {68} \end{array}\right]=\left[\begin{array}{c} {0} \\ {0} \\ {0} \\ {0} \end{array}\right]. -->
<!-- \]  -->
</div>
<div id="additional-results" class="section level3 unnumbered hasAnchor">
<h3>Additional results<a href="multiple-regression.html#additional-results" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can derive additional results by using using the theory of multivariate normal random variables. The distributional assumptions of the regression model are encapsulated in the assumption that <span class="math inline">\(\mathbf{\epsilon}\)</span> has a multivariate normal distribution with mean <span class="math inline">\(\mathbf{0}\)</span> (a vector of 0’s) and variance matrix <span class="math inline">\(\sigma^2_\varepsilon \mathbf{I}\)</span>, where <span class="math inline">\(\mathbf{I}\)</span> is the (<span class="math inline">\(n\)</span>-by-<span class="math inline">\(n\)</span>) identity matrix. (Recall that a variance matrix includes variances on the diagonal and covariances in the off-diagonal elements. Thus, the statement <span class="math inline">\(\bf{\Sigma} = \sigma^2_\varepsilon \mathbf{I}\)</span> says that every component of <span class="math inline">\(\mathbf{\epsilon}\)</span> has variance <span class="math inline">\(\sigma^2_\varepsilon\)</span>, and that the covariance between any two components is 0.) Or, using <span class="math inline">\(\mathcal{N}\)</span> to denote a (univariate or multivariate) normal distribution, we can write
<span class="math display">\[
\mathbf{\epsilon}\sim \mathcal{N}(\mathbf{0}, \sigma^2_\varepsilon \mathbf{I}).
\]</span>
It then follows that
<span class="math display">\[
\mathbf{Y}\sim \mathcal{N}(\mathbf{X}\mathbf{\beta}, \sigma^2_\varepsilon \mathbf{I}).
\]</span>
Let <span class="math inline">\(\mathrm{E}\left[\cdot\right]\)</span> and <span class="math inline">\(\mbox{Var}\left(\cdot\right)\)</span> denote the expectation (mean) and variance of a random variable, respectively. First, we can show that <span class="math inline">\(\hat{\mathbf{\beta}}\)</span> is an ``unbiased’’ estimate of <span class="math inline">\(\mathbf{\beta}\)</span>, using the linearity of expectations:
<span class="math display">\[\begin{eqnarray*}
\mathrm{E}\left[\hat{\mathbf{\beta}}\right] &amp; = &amp; \mathrm{E}\left[\left(\mathbf{X}&#39;\mathbf{X}\right)^{-1} \mathbf{X}&#39;\mathbf{Y}\right] \\
&amp; = &amp; \left(\mathbf{X}&#39;\mathbf{X}\right)^{-1} \mathbf{X}&#39;\mathrm{E}\left[\mathbf{Y}\right] \hspace{0.5in} \mbox{(linearity)}\\
&amp; = &amp; \left(\mathbf{X}&#39;\mathbf{X}\right)^{-1} \mathbf{X}&#39; \mathbf{X}\mathbf{\beta}\\
&amp; = &amp; \mathbf{\beta}.
\end{eqnarray*}\]</span>
Next, we can find the variance of <span class="math inline">\(\hat{\mathbf{\beta}}\)</span> using a result for the variance of linear combinations of random variables:
<span class="math display">\[\begin{eqnarray*}
    \mbox{Var}\left(\hat{\mathbf{\beta}}\right) &amp; = &amp; \mbox{Var}\left(\left(\mathbf{X}&#39;\mathbf{X}\right)^{-1} \mathbf{X}&#39;\mathbf{Y}\right) \\
    &amp; = &amp; \left(\mathbf{X}&#39;\mathbf{X}\right)^{-1} \mathbf{X}&#39; \mbox{Var}\left(\mathbf{Y}\right) \mathbf{X}\left(\mathbf{X}&#39;\mathbf{X}\right)^{-1} \\
    &amp; = &amp; \sigma^2_{\varepsilon} \left(\mathbf{X}&#39;\mathbf{X}\right)^{-1} \mathbf{X}&#39; \mathbf{X}\left(\mathbf{X}&#39;\mathbf{X}\right)^{-1} \\
    &amp; = &amp; \sigma^2_{\varepsilon} \left(\mathbf{X}&#39;\mathbf{X}\right)^{-1}.
\end{eqnarray*}\]</span>
The second equality above is a quadratic form, and uses the fact that <span class="math inline">\(\left(\mathbf{X}&#39;\mathbf{X}\right)^{-1}\)</span> is symmetric (and thus equal to its transpose). The final result, <span class="math inline">\(\mbox{Var}\left(\hat{\mathbf{\beta}}\right) = \sigma^2_{\varepsilon} \left(\mathbf{X}&#39;\mathbf{X}\right)^{-1}\)</span>, shows that the variances of the least squares estimates (and thus their standard errors) are proportional to the diagonal elements of <span class="math inline">\(\left(\mathbf{X}&#39;\mathbf{X}\right)^{-1}\)</span>. This result will become important in multiple regression when we discuss multicollinearity.</p>
<p>Finally, let <span class="math inline">\(\mathbf{\hat{Y}}\)</span> be a vector of fitted values, i.e., <span class="math inline">\(\mathbf{\hat{Y}}= \left[ \hat{y}_1, \hat{y_2}, \ldots, \hat{y_n} \right]&#39;\)</span>. We can find an experssion for <span class="math inline">\(\mathbf{\hat{Y}}\)</span> simply as:
<span class="math display">\[\begin{eqnarray*}
\mathbf{\hat{Y}}&amp; = &amp; \mathbf{X}\hat{\mathbf{\beta}}\\
&amp; = &amp; \mathbf{X}\left(\mathbf{X}&#39;\mathbf{X}\right)^{-1} \mathbf{X}&#39; \mathbf{Y}
\end{eqnarray*}\]</span>
The matrix <span class="math inline">\(\mathbf{X}\left(\mathbf{X}&#39;\mathbf{X}\right)^{-1} \mathbf{X}&#39;\)</span> (sometimes called the hat matrix, because it maps <span class="math inline">\(\mathbf{Y}\)</span> to <span class="math inline">\(\mathbf{\hat{Y}}\)</span>) is a projection matrix, and thus the fitted values are the orthogonal projection of <span class="math inline">\(\mathbf{Y}\)</span> onto the columnspace of <span class="math inline">\(\mathbf{X}\)</span>. The right-triangle that results from the vectors <span class="math inline">\(\mathbf{Y}\)</span> (the hypotenuse), <span class="math inline">\(\mathbf{\hat{Y}}\)</span>, and <span class="math inline">\(\mathbf{e}= \mathbf{Y}- \mathbf{\hat{Y}}\)</span> gives rise to the sum-of-squares decomposition behind <span class="math inline">\(R^2\)</span>.</p>
<p>There are many good texts that present the linear-algebra formulation of the linear statistical model, including <span class="citation">Sen and Srivastava (<a href="#ref-sen1997regression" role="doc-biblioref">1997</a>)</span> and <span class="citation">Monahan (<a href="#ref-monahan2008primer" role="doc-biblioref">2008</a>)</span>.</p>

</div>
</div>
</div>
<h3>Bibliography<a href="bibliography.html#bibliography" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-berry2016p" class="csl-entry">
Berry, Donald A. 2016. <span>“P-Values Are Not What They’re Cracked up to Be.”</span> <em>Online Commentary to ASA Statement on Statistical Significance and <span class="math inline">\(p\)</span>-Values.(doi: 10.1080/00031305.2016. 1154108)</em>.
</div>
<div id="ref-bowerman1990linear" class="csl-entry">
Bowerman, Bruce L, and Richard T O’Connell. 1990. <em>Linear Statistical Models: An Applied Approach</em>. Brooks/Cole.
</div>
<div id="ref-gelman2020regression" class="csl-entry">
Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. <em>Regression and Other Stories</em>. Cambridge University Press.
</div>
<div id="ref-mcintyre1994using" class="csl-entry">
McIntyre, Lauren. 1994. <span>“Using Cigarette Data for an Introduction to Multiple Regression.”</span> <em>Journal of Statistics Education</em> 2 (1).
</div>
<div id="ref-mendenhall1992statistics" class="csl-entry">
Mendenhall, William M, and Terry L Sincich. 2012. <em>Statistics for Engineering and the Sciences</em>. New York: Dellen Publishing Co.
</div>
<div id="ref-monahan2008primer" class="csl-entry">
Monahan, John F. 2008. <em>A Primer on Linear Models</em>. CRC Press.
</div>
<div id="ref-moore1989introduction" class="csl-entry">
Moore, David S, and George P McCabe. 1989. <em>Introduction to the Practice of Statistics.</em> WH Freeman.
</div>
<div id="ref-quinn2002experimental" class="csl-entry">
Quinn, Gerry P, and Michael J Keough. 2002. <em>Experimental Design and Data Analysis for Biologists</em>. Cambridge university press.
</div>
<div id="ref-rawlings1998applied" class="csl-entry">
Rawlings, John O, Sastry G Pantula, and David A Dickey. 1998. <em>Applied Regression Analysis: A Research Tool</em>. Springer.
</div>
<div id="ref-sackett2013influence" class="csl-entry">
Sackett, Dana K, W Gregory Cope, James A Rice, and D Derek Aday. 2013. <span>“The Influence of Fish Length on Tissue Mercury Dynamics: Implications for Natural Resource Management and Human Health Risk.”</span> <em>International Journal of Environmental Research and Public Health</em> 10 (2): 638–59.
</div>
<div id="ref-sen1997regression" class="csl-entry">
Sen, Ashish, and Muni Srivastava. 1997. <em>Regression Analysis: Theory, Methods, and Applications</em>. Springer Science &amp; Business Media.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="13">
<li id="fn13"><p>Whether a variable should be treated as quantitative or categorical is not always clear cut. For example, consider an individual’s political leaning, and suppose that for the sake of argument we could classify each individual’s political leaning as either conservative, moderate, or liberal. Most everyone would agree that these three values have a natural ordering, with moderates occupying a middle ground between conservatives and liberals. However, the operative question for the analyst is whether to assume that moderates are exactly halfway between conservatives and liberals. In other words, is the difference between conservatives and moderates exactly the same size as the difference between moderates and liberals? If we are willing to make this assumption, then we could code political leaning with a quantitative variable, taking values (say) 0, 1, and 2 for conservatives, moderates, and liberals, respectively. If we weren’t willing to make this assumption, then we should treat political leaning as a categorical variable with three levels. There is scope here for thoughtful analysts to disagree.<a href="multiple-regression.html#fnref13" class="footnote-back">↩︎</a></p></li>
<li id="fn14"><p>The <code>contrast</code> command does much more than report how indicator variables are coded for a categorical variable, as we will see later.<a href="multiple-regression.html#fnref14" class="footnote-back">↩︎</a></p></li>
<li id="fn15"><p>If we prefer, it is easy enough to select a different reference level in R. Suppose we wanted Bennett to serve as the reference site. We could do so with the code <code>contrasts(fish$site) &lt;- contr.treatment(n = 3, base = 2)</code> See the R documentation for <code>contr.treatment</code> for more details.<a href="multiple-regression.html#fnref15" class="footnote-back">↩︎</a></p></li>
<li id="fn16"><p>While the possibility of correlations among predictors is important, that is not what we are discussing here. Instead, correlations among predictors are the subject of <em>collinearity</em>.<a href="multiple-regression.html#fnref16" class="footnote-back">↩︎</a></p></li>
<li id="fn17"><p>To take this argument a little further, we don’t expect an association between weight and BAC for people who have not drunk any beers. Thus, it is perhaps not surprising that we cannot reject <span class="math inline">\(H_0: \beta_2 = 0\)</span> in favor of <span class="math inline">\(H_0: \beta_2 \ne 0\)</span> in the model that includes the interaction between weight and beers consumed.<a href="multiple-regression.html#fnref17" class="footnote-back">↩︎</a></p></li>
<li id="fn18"><p>Indeed, if the predictors are perfectly uncorrelated with one another, then the multiple regression coefficients will be identical to the slopes from individual simple regression models.<a href="multiple-regression.html#fnref18" class="footnote-back">↩︎</a></p></li>
<li id="fn19"><p>Another feature of these data that immediately catches the eye is that there is one cigarette — Bull Durham brand — that has noticeably larger values of all variables. There is also a brand — Now brand — that has noticeably lower values of all variables. These two data points will have large leverage, and we might wonder to what extent the fit will be driven by these two data points alone. That’s a fair question, and one that we should ask in a complete analysis of these data. But it’s beside the point for the present purposes, so we won’t engage with it here.<a href="multiple-regression.html#fnref19" class="footnote-back">↩︎</a></p></li>
<li id="fn20"><p>Here, we use “bias” in its technical sense, meaning the difference between the average of a sampling distribution of an estimate, and the parameter we are trying to estimate. All else being equal, we prefer estimators that are unbiased. However, sometimes a small amount of bias may be acceptable if it leads to big improvements in other properties of the estimator.<a href="multiple-regression.html#fnref20" class="footnote-back">↩︎</a></p></li>
<li id="fn21"><p>Indeed, this isn’t just true for AIC. There are many estimators in statistics that are only asymptotically unbiased but which are routinely used for small data sets anyway, without any guarantees that those estimators behave well or predictably for small data sets.<a href="multiple-regression.html#fnref21" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="simple-linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="non-linear-regression-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
