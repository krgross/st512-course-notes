<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Factorial experiments | ST 512 course notes</title>
  <meta name="description" content="Course notes for ST 512, Statistical Methods for Researchers II." />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Factorial experiments | ST 512 course notes" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Course notes for ST 512, Statistical Methods for Researchers II." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Factorial experiments | ST 512 course notes" />
  
  <meta name="twitter:description" content="Course notes for ST 512, Statistical Methods for Researchers II." />
  

<meta name="author" content="Kevin Gross" />


<meta name="date" content="2023-05-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="one-factor-anova.html"/>
<link rel="next" href="ancova.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#philosophy"><i class="fa fa-check"></i>Philosophy</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#scope-and-coverage"><i class="fa fa-check"></i>Scope and coverage</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#mathematical-level"><i class="fa fa-check"></i>Mathematical level</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#computing"><i class="fa fa-check"></i>Computing</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#format-of-the-notes"><i class="fa fa-check"></i>Format of the notes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments-and-license"><i class="fa fa-check"></i>Acknowledgments and license</a></li>
</ul></li>
<li class="part"><span><b>Part I: Regression modeling</b></span></li>
<li class="chapter" data-level="1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>1</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-basics-of-slr"><i class="fa fa-check"></i><b>1.1</b> The basics of SLR</a></li>
<li class="chapter" data-level="1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#least-squares-estimation"><i class="fa fa-check"></i><b>1.2</b> Least-squares estimation</a></li>
<li class="chapter" data-level="1.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#inference-for-the-slope"><i class="fa fa-check"></i><b>1.3</b> Inference for the slope</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#standard-errors"><i class="fa fa-check"></i><b>1.3.1</b> Standard errors</a></li>
<li class="chapter" data-level="1.3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#confidence-intervals"><i class="fa fa-check"></i><b>1.3.2</b> Confidence intervals</a></li>
<li class="chapter" data-level="1.3.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#statistical-hypothesis-tests"><i class="fa fa-check"></i><b>1.3.3</b> Statistical hypothesis tests</a></li>
<li class="chapter" data-level="1.3.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#inference-for-the-intercept"><i class="fa fa-check"></i><b>1.3.4</b> Inference for the intercept</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#sums-of-squares-decomposition-and-r2"><i class="fa fa-check"></i><b>1.4</b> Sums of squares decomposition and <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="1.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#fitting-the-slr-model-in-r"><i class="fa fa-check"></i><b>1.5</b> Fitting the SLR model in R</a></li>
<li class="chapter" data-level="1.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#diagnostic-plots"><i class="fa fa-check"></i><b>1.6</b> Diagnostic plots</a></li>
<li class="chapter" data-level="1.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#consequences-of-violating-model-assumptions-and-possible-fixes"><i class="fa fa-check"></i><b>1.7</b> Consequences of violating model assumptions, and possible fixes</a></li>
<li class="chapter" data-level="1.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#prediction-with-regression-models"><i class="fa fa-check"></i><b>1.8</b> Prediction with regression models</a></li>
<li class="chapter" data-level="1.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#regression-design"><i class="fa fa-check"></i><b>1.9</b> Regression design</a></li>
<li class="chapter" data-level="1.10" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#centering-the-predictor"><i class="fa fa-check"></i><b>1.10</b> <span class="math inline">\(^\star\)</span>Centering the predictor</a></li>
<li class="chapter" data-level="" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#appendix-regression-models-in-sas-proc-reg"><i class="fa fa-check"></i>Appendix: Regression models in SAS PROC REG</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#multiple-regression-basics"><i class="fa fa-check"></i><b>2.1</b> Multiple regression basics</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="multiple-regression.html"><a href="multiple-regression.html#the-multiple-regression-model"><i class="fa fa-check"></i><b>2.1.1</b> The multiple regression model</a></li>
<li class="chapter" data-level="2.1.2" data-path="multiple-regression.html"><a href="multiple-regression.html#interpreting-partial-regression-coefficients."><i class="fa fa-check"></i><b>2.1.2</b> Interpreting partial regression coefficients.</a></li>
<li class="chapter" data-level="2.1.3" data-path="multiple-regression.html"><a href="multiple-regression.html#visualizing-a-multiple-regression-model"><i class="fa fa-check"></i><b>2.1.3</b> Visualizing a multiple regression model</a></li>
<li class="chapter" data-level="2.1.4" data-path="multiple-regression.html"><a href="multiple-regression.html#statistical-inference-for-partial-regression-coefficients"><i class="fa fa-check"></i><b>2.1.4</b> Statistical inference for partial regression coefficients</a></li>
<li class="chapter" data-level="2.1.5" data-path="multiple-regression.html"><a href="multiple-regression.html#prediction"><i class="fa fa-check"></i><b>2.1.5</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#F-test"><i class="fa fa-check"></i><b>2.2</b> <span class="math inline">\(F\)</span>-tests for several regression coefficients</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#model-utility-tests"><i class="fa fa-check"></i><b>2.2.1</b> Model utility tests</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="multiple-regression.html"><a href="multiple-regression.html#categorical-predictors"><i class="fa fa-check"></i><b>2.3</b> Categorical predictors</a></li>
<li class="chapter" data-level="2.4" data-path="multiple-regression.html"><a href="multiple-regression.html#regression-interactions"><i class="fa fa-check"></i><b>2.4</b> Interactions between predictors</a></li>
<li class="chapter" data-level="2.5" data-path="multiple-regression.html"><a href="multiple-regression.html#collinearity"><i class="fa fa-check"></i><b>2.5</b> (Multi-)Collinearity</a></li>
<li class="chapter" data-level="2.6" data-path="multiple-regression.html"><a href="multiple-regression.html#variable-selection-choosing-the-best-model"><i class="fa fa-check"></i><b>2.6</b> Variable selection: Choosing the best model</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="multiple-regression.html"><a href="multiple-regression.html#model-selection-and-inference"><i class="fa fa-check"></i><b>2.6.1</b> Model selection and inference</a></li>
<li class="chapter" data-level="2.6.2" data-path="multiple-regression.html"><a href="multiple-regression.html#ranking-methods"><i class="fa fa-check"></i><b>2.6.2</b> Ranking methods</a></li>
<li class="chapter" data-level="2.6.3" data-path="multiple-regression.html"><a href="multiple-regression.html#cross-validation"><i class="fa fa-check"></i><b>2.6.3</b> Cross-validation</a></li>
<li class="chapter" data-level="2.6.4" data-path="multiple-regression.html"><a href="multiple-regression.html#sequential-methods"><i class="fa fa-check"></i><b>2.6.4</b> Sequential methods</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="multiple-regression.html"><a href="multiple-regression.html#leverage-influential-points-and-standardized-residuals"><i class="fa fa-check"></i><b>2.7</b> Leverage, influential points, and standardized residuals</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="multiple-regression.html"><a href="multiple-regression.html#leverage"><i class="fa fa-check"></i><b>2.7.1</b> Leverage</a></li>
<li class="chapter" data-level="2.7.2" data-path="multiple-regression.html"><a href="multiple-regression.html#standardized-residuals"><i class="fa fa-check"></i><b>2.7.2</b> Standardized residuals</a></li>
<li class="chapter" data-level="2.7.3" data-path="multiple-regression.html"><a href="multiple-regression.html#cooks-distance"><i class="fa fa-check"></i><b>2.7.3</b> Cook’s distance</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="multiple-regression.html"><a href="multiple-regression.html#appendix-regression-as-a-linear-algebra-problem"><i class="fa fa-check"></i>Appendix: Regression as a linear algebra problem</a>
<ul>
<li class="chapter" data-level="" data-path="multiple-regression.html"><a href="multiple-regression.html#singular-or-pathological-design-matrices"><i class="fa fa-check"></i>Singular, or pathological, design matrices</a></li>
<li class="chapter" data-level="" data-path="multiple-regression.html"><a href="multiple-regression.html#additional-results"><i class="fa fa-check"></i>Additional results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="non-linear-regression-models.html"><a href="non-linear-regression-models.html"><i class="fa fa-check"></i><b>3</b> Non-linear regression models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="non-linear-regression-models.html"><a href="non-linear-regression-models.html#polynomial-regression"><i class="fa fa-check"></i><b>3.1</b> Polynomial regression</a></li>
<li class="chapter" data-level="3.2" data-path="non-linear-regression-models.html"><a href="non-linear-regression-models.html#nls"><i class="fa fa-check"></i><b>3.2</b> Non-linear least squares</a></li>
<li class="chapter" data-level="3.3" data-path="non-linear-regression-models.html"><a href="non-linear-regression-models.html#starsmoothing-methods"><i class="fa fa-check"></i><b>3.3</b> <em><span class="math inline">\(^\star\)</span>Smoothing methods</em></a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="non-linear-regression-models.html"><a href="non-linear-regression-models.html#loess-smoothers"><i class="fa fa-check"></i><b>3.3.1</b> Loess smoothers</a></li>
<li class="chapter" data-level="3.3.2" data-path="non-linear-regression-models.html"><a href="non-linear-regression-models.html#splines"><i class="fa fa-check"></i><b>3.3.2</b> Splines</a></li>
<li class="chapter" data-level="3.3.3" data-path="non-linear-regression-models.html"><a href="non-linear-regression-models.html#generalized-additive-models-gams"><i class="fa fa-check"></i><b>3.3.3</b> Generalized additive models (GAMs)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>4</b> Generalized linear models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson-regression"><i class="fa fa-check"></i><b>4.1</b> Poisson regression</a></li>
<li class="chapter" data-level="4.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#binary-responses"><i class="fa fa-check"></i><b>4.2</b> Binary responses</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#individual-binary-responses-tb-in-boar"><i class="fa fa-check"></i><b>4.2.1</b> Individual binary responses: TB in boar</a></li>
<li class="chapter" data-level="4.2.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#grouped-binary-data-industrial-melanism"><i class="fa fa-check"></i><b>4.2.2</b> Grouped binary data: Industrial melanism</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#implementation-in-sas"><i class="fa fa-check"></i><b>4.3</b> Implementation in SAS</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#complete-separation"><i class="fa fa-check"></i><b>4.3.1</b> Complete separation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part II: Designed experiments</b></span></li>
<li class="chapter" data-level="5" data-path="one-factor-anova.html"><a href="one-factor-anova.html"><i class="fa fa-check"></i><b>5</b> One-factor ANOVA</a>
<ul>
<li class="chapter" data-level="5.1" data-path="one-factor-anova.html"><a href="one-factor-anova.html#grouped-data-and-the-design-of-experiments-doe-an-overview"><i class="fa fa-check"></i><b>5.1</b> Grouped data and the design of experiments (DoE): an overview</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="one-factor-anova.html"><a href="one-factor-anova.html#a-vocabulary-for-describing-designed-experiments"><i class="fa fa-check"></i><b>5.1.1</b> A vocabulary for describing designed experiments</a></li>
<li class="chapter" data-level="5.1.2" data-path="one-factor-anova.html"><a href="one-factor-anova.html#roadmap"><i class="fa fa-check"></i><b>5.1.2</b> Roadmap</a></li>
<li class="chapter" data-level="5.1.3" data-path="one-factor-anova.html"><a href="one-factor-anova.html#the-simplest-experiment"><i class="fa fa-check"></i><b>5.1.3</b> The simplest experiment</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="one-factor-anova.html"><a href="one-factor-anova.html#one-factor-anova-the-basics"><i class="fa fa-check"></i><b>5.2</b> One-factor ANOVA: The basics</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="one-factor-anova.html"><a href="one-factor-anova.html#connections-between-one-factor-anova-and-other-statistical-procedures"><i class="fa fa-check"></i><b>5.2.1</b> Connections between one-factor ANOVA and other statistical procedures</a></li>
<li class="chapter" data-level="5.2.2" data-path="one-factor-anova.html"><a href="one-factor-anova.html#assumptions-in-anova"><i class="fa fa-check"></i><b>5.2.2</b> Assumptions in ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="one-factor-anova.html"><a href="one-factor-anova.html#linear-contrasts-of-group-means"><i class="fa fa-check"></i><b>5.3</b> Linear contrasts of group means</a></li>
<li class="chapter" data-level="5.4" data-path="one-factor-anova.html"><a href="one-factor-anova.html#using-sas-the-effects-parameterization-of-the-one-factor-anova"><i class="fa fa-check"></i><b>5.4</b> Using SAS: The effects parameterization of the one-factor ANOVA</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="one-factor-anova.html"><a href="one-factor-anova.html#effects-model-parameterization-of-the-one-factor-anova-model"><i class="fa fa-check"></i><b>5.4.1</b> Effects-model parameterization of the one-factor ANOVA model</a></li>
<li class="chapter" data-level="5.4.2" data-path="one-factor-anova.html"><a href="one-factor-anova.html#sas-implementation-of-the-one-factor-anova-model-in-proc-glm"><i class="fa fa-check"></i><b>5.4.2</b> SAS implementation of the one-factor ANOVA model in PROC GLM</a></li>
<li class="chapter" data-level="5.4.3" data-path="one-factor-anova.html"><a href="one-factor-anova.html#using-the-estimate-and-contrast-statements-for-linear-contrasts-in-proc-glm"><i class="fa fa-check"></i><b>5.4.3</b> Using the ESTIMATE and CONTRAST statements for linear contrasts in PROC GLM</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="one-factor-anova.html"><a href="one-factor-anova.html#linear-contrasts-revisited-testing-multiple-simultaneous-contrasts"><i class="fa fa-check"></i><b>5.5</b> Linear contrasts revisited: Testing multiple simultaneous contrasts</a></li>
<li class="chapter" data-level="5.6" data-path="one-factor-anova.html"><a href="one-factor-anova.html#multiple-comparisons"><i class="fa fa-check"></i><b>5.6</b> Multiple comparisons</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="one-factor-anova.html"><a href="one-factor-anova.html#multiple-testing-in-general"><i class="fa fa-check"></i><b>5.6.1</b> Multiple testing in general</a></li>
<li class="chapter" data-level="5.6.2" data-path="one-factor-anova.html"><a href="one-factor-anova.html#bonferroni-and-bonferroni-like-procedures"><i class="fa fa-check"></i><b>5.6.2</b> Bonferroni and Bonferroni-like procedures</a></li>
<li class="chapter" data-level="5.6.3" data-path="one-factor-anova.html"><a href="one-factor-anova.html#multiple-comparisons-in-anova"><i class="fa fa-check"></i><b>5.6.3</b> Multiple comparisons in ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="one-factor-anova.html"><a href="one-factor-anova.html#general-strategy-for-analyzing-data-from-a-crd-with-a-one-factor-treatment-structure"><i class="fa fa-check"></i><b>5.7</b> General strategy for analyzing data from a CRD with a one-factor treatment structure</a></li>
<li class="chapter" data-level="5.8" data-path="one-factor-anova.html"><a href="one-factor-anova.html#starpower-and-sample-size-determination-in-anova"><i class="fa fa-check"></i><b>5.8</b> <span class="math inline">\(^\star\)</span>Power and sample-size determination in ANOVA</a></li>
<li class="chapter" data-level="5.9" data-path="one-factor-anova.html"><a href="one-factor-anova.html#starorthogonal-contrasts"><i class="fa fa-check"></i><b>5.9</b> <span class="math inline">\(^\star\)</span>Orthogonal contrasts</a></li>
<li class="chapter" data-level="5.10" data-path="one-factor-anova.html"><a href="one-factor-anova.html#starpolynomial-trends"><i class="fa fa-check"></i><b>5.10</b> <span class="math inline">\(^\star\)</span>Polynomial trends</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="factorial-experiments.html"><a href="factorial-experiments.html"><i class="fa fa-check"></i><b>6</b> Factorial experiments</a>
<ul>
<li class="chapter" data-level="6.1" data-path="factorial-experiments.html"><a href="factorial-experiments.html#crossed-vs.-nested-designs"><i class="fa fa-check"></i><b>6.1</b> Crossed vs. nested designs</a></li>
<li class="chapter" data-level="6.2" data-path="factorial-experiments.html"><a href="factorial-experiments.html#simple-effects-main-effects-and-interaction-effects"><i class="fa fa-check"></i><b>6.2</b> Simple effects, main effects, and interaction effects</a></li>
<li class="chapter" data-level="6.3" data-path="factorial-experiments.html"><a href="factorial-experiments.html#analysis-of-a-balanced-2-times-2-factorial-experiment"><i class="fa fa-check"></i><b>6.3</b> Analysis of a balanced 2 <span class="math inline">\(\times\)</span> 2 factorial experiment</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="factorial-experiments.html"><a href="factorial-experiments.html#example-weight-gain-in-rats"><i class="fa fa-check"></i><b>6.3.1</b> Example: Weight gain in rats</a></li>
<li class="chapter" data-level="6.3.2" data-path="factorial-experiments.html"><a href="factorial-experiments.html#analysis-using-proc-glm-in-sas"><i class="fa fa-check"></i><b>6.3.2</b> Analysis using PROC GLM in SAS</a></li>
<li class="chapter" data-level="6.3.3" data-path="factorial-experiments.html"><a href="factorial-experiments.html#effects-notation-for-the-two-factor-anova"><i class="fa fa-check"></i><b>6.3.3</b> Effects notation for the two-factor ANOVA</a></li>
<li class="chapter" data-level="6.3.4" data-path="factorial-experiments.html"><a href="factorial-experiments.html#a-second-example"><i class="fa fa-check"></i><b>6.3.4</b> A second example</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="factorial-experiments.html"><a href="factorial-experiments.html#a-times-b-factorial-designs"><i class="fa fa-check"></i><b>6.4</b> <span class="math inline">\(a \times b\)</span> factorial designs</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="factorial-experiments.html"><a href="factorial-experiments.html#example-without-a-significant-interaction"><i class="fa fa-check"></i><b>6.4.1</b> Example without a significant interaction</a></li>
<li class="chapter" data-level="6.4.2" data-path="factorial-experiments.html"><a href="factorial-experiments.html#example-with-a-significant-interaction"><i class="fa fa-check"></i><b>6.4.2</b> Example with a significant interaction</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="factorial-experiments.html"><a href="factorial-experiments.html#unreplicated-factorial-designs"><i class="fa fa-check"></i><b>6.5</b> Unreplicated factorial designs</a></li>
<li class="chapter" data-level="6.6" data-path="factorial-experiments.html"><a href="factorial-experiments.html#missing-cells"><i class="fa fa-check"></i><b>6.6</b> Missing cells</a></li>
<li class="chapter" data-level="6.7" data-path="factorial-experiments.html"><a href="factorial-experiments.html#more-than-two-factors"><i class="fa fa-check"></i><b>6.7</b> More than two factors</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ancova.html"><a href="ancova.html"><i class="fa fa-check"></i><b>7</b> ANCOVA</a></li>
<li class="chapter" data-level="8" data-path="random-effects.html"><a href="random-effects.html"><i class="fa fa-check"></i><b>8</b> Random effects</a>
<ul>
<li class="chapter" data-level="8.1" data-path="random-effects.html"><a href="random-effects.html#fixed-vs.-random-effects-the-big-picture"><i class="fa fa-check"></i><b>8.1</b> Fixed vs. random effects: the big picture</a></li>
<li class="chapter" data-level="8.2" data-path="random-effects.html"><a href="random-effects.html#random-effects-models"><i class="fa fa-check"></i><b>8.2</b> Random-effects models</a></li>
<li class="chapter" data-level="8.3" data-path="random-effects.html"><a href="random-effects.html#subsampling"><i class="fa fa-check"></i><b>8.3</b> Subsampling</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="random-effects.html"><a href="random-effects.html#equal-subsamples-per-eu"><i class="fa fa-check"></i><b>8.3.1</b> Equal subsamples per EU</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="random-effects.html"><a href="random-effects.html#starmathematical-foundations"><i class="fa fa-check"></i><b>8.4</b> <span class="math inline">\(^\star\)</span>Mathematical foundations</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="random-effects.html"><a href="random-effects.html#probability-refresher"><i class="fa fa-check"></i><b>8.4.1</b> Probability refresher</a></li>
<li class="chapter" data-level="8.4.2" data-path="random-effects.html"><a href="random-effects.html#application-to-models-with-random-effects"><i class="fa fa-check"></i><b>8.4.2</b> Application to models with random effects</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="blocked-designs.html"><a href="blocked-designs.html"><i class="fa fa-check"></i><b>9</b> Blocked designs</a>
<ul>
<li class="chapter" data-level="9.1" data-path="blocked-designs.html"><a href="blocked-designs.html#randomized-complete-block-designs"><i class="fa fa-check"></i><b>9.1</b> Randomized complete block designs</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="blocked-designs.html"><a href="blocked-designs.html#should-a-blocking-factor-be-a-fixed-or-random-effect"><i class="fa fa-check"></i><b>9.1.1</b> *Should a blocking factor be a fixed or random effect?</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="blocked-designs.html"><a href="blocked-designs.html#latin-squares-designs"><i class="fa fa-check"></i><b>9.2</b> Latin-squares designs</a></li>
<li class="chapter" data-level="9.3" data-path="blocked-designs.html"><a href="blocked-designs.html#split-plot-designs"><i class="fa fa-check"></i><b>9.3</b> Split-plot designs</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="blocked-designs.html"><a href="blocked-designs.html#denominator-degrees-of-freedom-in-split-plot-designs-and-the-satterthwaite-approximation"><i class="fa fa-check"></i><b>9.3.1</b> Denominator degrees of freedom in split-plot designs and the Satterthwaite approximation</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="blocked-designs.html"><a href="blocked-designs.html#repeated-measures"><i class="fa fa-check"></i><b>9.4</b> Repeated measures</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">ST 512 course notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="factorial-experiments" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Factorial experiments<a href="factorial-experiments.html#factorial-experiments" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this section, we consider experiments where the treatment structure involves multiple experimental factors.</p>
<div id="crossed-vs.-nested-designs" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Crossed vs. nested designs<a href="factorial-experiments.html#crossed-vs.-nested-designs" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider the following two experiments</p>
<p><em>Experiment 1.</em> (A hypothetical experiment based on example 15.8 in Ott &amp; Longnecker):</p>
<p>A citrus orchard contains 3 different varieties of citrus trees. Eight trees of each variety are randomly selected from the orchard. Four different pesticides are randomly assigned to two trees of each variety and applied according to recommended levels. The same four pesticides are used for each variety. Yields of fruit (in bushels per tree) are recorded at the end of the growing season.</p>
<!-- \begin{center}\begin{tabular}{|p{0.9in}|p{0.9in}|p{0.9in}|p{0.9in}|p{0.9in}|} \hline  -->
<!--  & Pesticide 1 & Pesticide 2 & Pesticide 3 & Pesticide 4 \\ \hline  -->
<!-- Variety 1 & 49, 39 & 50, 55 & 43, 38 & 53, 48 \\ \hline  -->
<!-- Variety 2 & 55, 41 & 67, 58 & 53, 42 & 85, 73 \\ \hline  -->
<!-- Variety 3 & 66, 68 & 85, 92 & 69, 62 & 85, 99 \\ \hline  -->
<!-- \end{tabular}\end{center} -->
<p><em>Experiment 2.</em> A study is conducted to investigate the effect of pest management practices on cotton in the central valley of California. 14 ranches are available for study. Each of the 14 ranches is managed by one consultant.</p>
<!-- The arrangement of ranches and consultants is: -->
<!-- \begin{center} -->
<!-- \begin{tabular}{|p{0.9in}|p{0.9in}|p{0.9in}|p{0.9in}|p{0.9in}|} \hline  -->
<!-- Consultant & \multicolumn{4}{|p{3.6in}|}{Ranches:} \\ \hline  -->
<!-- CD & BLACCO & Bonanza & Cauzza  & Vandborg  \\ \hline  -->
<!-- MM & Fortune & Houlding  & Vista Verde &  \\ \hline  -->
<!-- JS & Azcal  & Golden & Newton & Stoneland  \\ \hline  -->
<!-- JV & Cotta & Hamilton & Talley &  \\ \hline  -->
<!-- \end{tabular} -->
<!-- \end{center} -->
<p>In both experiments, there are two experimental factors — variety and pesticide in experiment 1, and consultant and ranch in experiment 2. Each unique combination of factors forms a separate <em>treatment combination</em>. In experiment 1, the treatment combinations are formed by crossing the two experimental factors. That is to say, every level of the first factor (variety) is combined with every level of the second factor (pesticide). This is an example of a <em>factorial</em> or <em>crossed design</em>. In experiment 2, each level of one factor (ranch) is only combined with one single level of the other factor (consultant). This is an example of a <em>hierarchical design</em>, and we would say that ranch is nested within the consultant.</p>
<p>The treatment structure in experiment 1 above could be alternatively called a <em>two-factor classification</em>, or a <em>two-way factorial</em> treatment structure, or a <em>4 x 3 factorial</em> treatment structure. Generically, we will call the two factors in a two-way factorial design factors “A” and “B”.</p>
<p>The scientific questions of most interest in a two-factor design are:</p>
<ol style="list-style-type: decimal">
<li><p>Is there evidence of an effect of factor A on the response?</p></li>
<li><p>Is there evidence of an effect of factor B on the response?</p></li>
<li><p>Is there evidence that the effect of factor A depends on the level of factor B, and vice versa? That is to say, is there evidence of an interaction between the two factors?</p></li>
</ol>
</div>
<div id="simple-effects-main-effects-and-interaction-effects" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Simple effects, main effects, and interaction effects<a href="factorial-experiments.html#simple-effects-main-effects-and-interaction-effects" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In a factorial experiment, we typically analyze three types of effects: simple effects, main effects and interaction effects. We can illustrate these effects with the following hypothetical experiment: A food scientist wants to know how the shelf-life of a product is affected by the temperature at which the product is packed (low vs. high temperature) and the wrapping material (foil vs. plastic). They conduct a balanced 2 x 2 factorial experiment to investigate the effect of temperature and wrapping material on shelf life.</p>
<p>In notation, let temperature be factor “A”, and let <span class="math inline">\(i\)</span> be an index that distinguishes the two temperatures. That is, let <span class="math inline">\(i = 1\)</span> indicate low temperature and <span class="math inline">\(i = 2\)</span> indicate high temperature. Let material be factor “B”, and let <span class="math inline">\(j\)</span> be an index that distinguishes the two materials. That is, let <span class="math inline">\(j = 1\)</span> indicate foil and <span class="math inline">\(j = 2\)</span> indicate plastic.</p>
<p>Suppose for a moment that we actually know what the actual means (in units of days) are for each treatment combination. (Remember, in real life the means for each treatment combination are parameters that have to be estimated.) Let <span class="math inline">\(\mu_{ij}\)</span> denote the true mean response for the treatment combination formed by temperature <span class="math inline">\(i\)</span> and material <span class="math inline">\(j\)</span>.
<!-- \begin{center} -->
<!--    \begin{tabular}{|p{1.1in}|p{1.1in}|p{1.1in}|p{1.1in}|} \hline  -->
<!--        \multicolumn{2}{|p{1in}|}{} & \multicolumn{2}{|p{2.2in}|}{Temperature} \\ \hline  -->
<!--        \multicolumn{2}{|p{1in}|}{} & Low & High \\ \hline  -->
<!--        Material & Foil & $\mu_{11} =2$ & $\mu_{21} =3$ \\ \hline  -->
<!--        & Plastic & $\mu_{12} =4$ & $\mu_{22} =5$ \\ \hline  -->
<!--    \end{tabular} -->
<!-- \end{center} -->
Each of the four means above are called <em>cell means</em>, because they provide the average response for each ‘cell’ in the factorial table. More generally, cell means are the average response for each unique treatment combination.</p>
<p>In addition to cell means, we can define <em>marginal means</em> as the averages of cell means. That is, the marginal mean for level <span class="math inline">\(i\)</span> of factor A is the average of each of the cell means associated with level <span class="math inline">\(i\)</span>, where the average is taken over all the levels of factor B. In a 2 <span class="math inline">\(\times\)</span> 2 design, the two marginal means for factor A are
<span class="math display">\[
\bar{\mu}_{1+} =\frac{\mu_{11} +\mu_{12} }{2}
\]</span>
and
<span class="math display">\[
\bar{\mu}_{2+} =\frac{\mu_{21} +\mu_{22} }{2}.
\]</span></p>
<p>Similarly, we can define the marginal means for each level <span class="math inline">\(j\)</span> of factor B as the average of each of the cell means associated with level <span class="math inline">\(j\)</span>, averaging over the levels of factor A. In the 2 <span class="math inline">\(\times\)</span> 2 design, the marginal means associated with factor B are
<span class="math display">\[
\bar{\mu}_{+1} =\frac{\mu_{11} +\mu_{21} }{2}
\]</span>
and
<span class="math display">\[
\bar{\mu}_{+2} =\frac{\mu_{12} +\mu_{22} }{2}.
\]</span></p>
<p>Marginal means take their name because they are found by marginalizing over the level(s) of the other factor(s). In a two-factor design, we might think about locating the marginal means in the margins of the table of cell means:
<!-- \begin{center} -->
<!-- \begin{tabular}{|p{0.9in}|p{0.9in}|p{0.9in}|p{0.9in}|p{0.8in}|} \hline  -->
<!-- \multicolumn{2}{|p{1in}|}{} & \multicolumn{2}{|p{1.8in}|}{Temperature} &  \\ \hline  -->
<!-- \multicolumn{2}{|p{1in}|}{} & Low & High &  \\ \hline  -->
<!-- Material & Foil & $\mu_{11} =2$ & $\mu_{21} =3$ & $\bar{\mu}_{+1} =2.5$ \\ \hline  -->
<!--  & Plastic & $\mu_{12} =4$ & $\mu_{22} =5$ & $\bar{\mu}_{+2} =4.5$ \\ \hline  -->
<!-- \multicolumn{2}{|p{1in}|}{} & $\bar{\mu}_{1+} =3$ & $\bar{\mu}_{2+} =4$ &  \\ \hline  -->
<!-- \end{tabular} -->
<!-- \end{center} --></p>
<p>Finally, we can define the grand mean as the average of marginal means. In a 2 <span class="math inline">\(\times\)</span> 2 design,
<span class="math display">\[
\bar{\mu}_{++} =\frac{\bar{\mu}_{1+} +\bar{\mu}_{2+} }{2} =\frac{\bar{\mu}_{+1} +\bar{\mu}_{+2} }{2}
\]</span>
Sometimes the grand mean is placed in the lower right-hand corner of the table of cell means:
<!-- \begin{center} -->
<!-- \begin{tabular}{|p{0.9in}|p{0.9in}|p{0.9in}|p{0.9in}|p{0.8in}|} \hline  -->
<!-- \multicolumn{2}{|p{1in}|}{} & \multicolumn{2}{|p{1.8in}|}{Temperature} &  \\ \hline  -->
<!-- \multicolumn{2}{|p{1in}|}{} & Low & High &  \\ \hline  -->
<!-- Material & Foil & $\mu_{11} =2$ & $\mu_{21} =3$ & $\bar{\mu}_{+1} =2.5$ \\ \hline  -->
<!--  & Plastic & $\mu_{12} =4$ & $\mu_{22} =5$ & $\bar{\mu}_{+2} =4.5$ \\ \hline  -->
<!-- \multicolumn{2}{|p{1in}|}{} & $\bar{\mu}_{1+} =3$ & $\bar{\mu}_{2+} =4$ & $\bar{\mu}_{++} =3.5$ \\ \hline  -->
<!-- \end{tabular} -->
<!-- \end{center} --></p>
<p>Let’s visualize the structure of these cell means with an interaction plot. (Other authors call these profile plots.) To construct an interaction plot, write the levels of one factor along the horizontal axis, and use the vertical axis to denote the response. Draw lines on the plot for each level of the other experimental factor. Two possible interaction plots for the above means are:
<img src="06-FactorialExperiments_files/figure-html/unnamed-chunk-2-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>Note that regardless of which factor we choose to place on the horizontal axis, the lines above are parallel. This indicates that the effects of the two factors do not depend on one another. When the effects of two (or more) factors do not depend on one another, we say that the effects of those factors are <em>additive</em>.</p>
<p>We can also express this idea mathematically. The <em>simple-effects comparisons</em> (or just simple effects) for a factor describe the differences among cell means associated with that factor at a single level of the other factor(s). In a 2 <span class="math inline">\(\times\)</span> 2 factorial design, simple effects can be expressed as simple differences. For example, to calculate the simple effect of temperature on food products wrapped in foil, simply calculate the difference <span class="math inline">\(\mu_{21} -\mu_{11} =3-2=1\)</span>. This difference means that for food products wrapped in foil, products packed at high temperature last 1 day longer than products wrapped at low temperature.</p>
<p>Similarly, we could calculate:</p>
<ul>
<li><p>The simple effect of temperature on food products wrapped in plastic:
<span class="math display">\[\mu_{22} -\mu_{12} =5-4=1\]</span></p></li>
<li><p>The simple effect of material on food products packed at low temperatures:
<span class="math display">\[\mu_{12} -\mu_{11} =4-2=2\]</span></p></li>
<li><p>The simple effect of material at food products packed at high temperatures:
<span class="math display">\[\mu_{22} -\mu_{21} =5-3=2\]</span></p></li>
</ul>
<p>In this case, the effect of temperature on shelf life does not depend on the type of material in which the food was wrapped. Similarly, the effect of material does not depend on the temperature at which the food was packed. This is a more precise expression of what we mean when we say that the effects of each factor do not depend on one another.</p>
<p>We can also define <em>main-effects comparisons</em> (or just main effects) for each experimental factor. Main effects describe differences among the marginal means associated with each level of an experimental factor. In a 2 <span class="math inline">\(\times\)</span> 2 design, main effects can be expressed as simple differences. In the example above, we could quantify the main effect of temperature as the difference
<span class="math display">\[\bar{\mu}_{2+} -\bar{\mu}_{1+} =4-3=1.\]</span>
Similarly, we can quantify the main effect of material as the difference
<span class="math display">\[\bar{\mu}_{+2} -\bar{\mu}_{+1} =4.5-2.5=2.\]</span>
With a little algebra, it can also be shown that main effects are equal to averages of simple effects. That is, in the example above, the main effect of temperature is equal to the average of the simple effect of temperature for foods wrapped in foil, and the simple effect of temperature for foods wrapped in plastic. Whether you prefer to think about main effects as differences among marginal means or as averages of simple effect is up to you — both interpretations work equally well.</p>
<p>Now, suppose that instead the means for each treatment combination are
<!-- \begin{center} -->
<!--    \begin{tabular}{|p{0.9in}|p{0.9in}|p{0.9in}|p{0.9in}|p{0.8in}|} \hline  -->
<!--        \multicolumn{2}{|p{1in}|}{} & \multicolumn{2}{|p{1.8in}|}{Temperature} &  \\ \hline  -->
<!--        \multicolumn{2}{|p{1in}|}{} & Low & High &  \\ \hline  -->
<!--        Material & Foil & $\mu_{11} =2$ & $\mu_{21} =3$ & $\bar{\mu}_{+1} =2.5$ \\ \hline  -->
<!--        & Plastic & $\mu_{12} =4$ & $\mu_{22} =6$ & $\bar{\mu}_{+2} =5$ \\ \hline  -->
<!--        \multicolumn{2}{|p{1in}|}{} & $\bar{\mu}_{1+} =3$ & $\bar{\mu}_{2+} =4.5$ & $\bar{\mu}_{++} =3.75$ \\ \hline  -->
<!--    \end{tabular} -->
<!-- \end{center} --></p>
<p>Again, let’s visualize the structure of these means with an interaction plot:
<img src="06-FactorialExperiments_files/figure-html/unnamed-chunk-3-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>Note that the lines above are no longer parallel. This is a visual indication that the effects of the two factors depend on one another. Or, to put it another way, there is an interaction between the two factors.</p>
<p>Again, we can express this idea mathematically. Let’s repeat the process of measuring the simple effect of each treatment factor:
+ Simple effect of temperature on food products wrapped in foil:
<span class="math display">\[\mu_{21} -\mu_{11} =3-2=1\]</span></p>
<ul>
<li><p>Simple effect of temperature on food products wrapped in plastic:
<span class="math display">\[\mu_{22} -\mu_{12} =6-4=2\]</span></p></li>
<li><p>Simple effect of material on food products packed at low temperatures:
<span class="math display">\[\mu_{12} -\mu_{11} =4-2=2\]</span></p></li>
<li><p>Simple effect of material at food products packed at high temperatures:
<span class="math display">\[\mu_{22} -\mu_{21} =6-3=3\]</span></p></li>
</ul>
<p>In this case, the effect of temperature on shelf life depends on the type of material in which the food was wrapped. Similarly, the effect of material depends on the temperature at which the food was packed. We describe this dependence by saying that the treatments <em>interact</em>.</p>
<p>Now, suppose that instead the means for each treatment combination are
<!-- \begin{center} -->
<!--    \begin{tabular}{|p{0.9in}|p{0.9in}|p{0.9in}|p{0.9in}|p{0.8in}|} \hline  -->
<!--        \multicolumn{2}{|p{1in}|}{} & \multicolumn{2}{|p{1.8in}|}{Temperature} &  \\ \hline  -->
<!--        \multicolumn{2}{|p{1in}|}{} & Low & High &  \\ \hline  -->
<!--        Material & Foil & $\mu_{11} =2$ & $\mu_{21} =3$ & $\bar{\mu}_{+1} =2.5$ \\ \hline  -->
<!--        & Plastic & $\mu_{12} =4$ & $\mu_{22} =6$ & $\bar{\mu}_{+2} =5$ \\ \hline  -->
<!--        \multicolumn{2}{|p{1in}|}{} & $\bar{\mu}_{1+} =3$ & $\bar{\mu}_{2+} =4.5$ & $\bar{\mu}_{++} =3.75$ \\ \hline  -->
<!--    \end{tabular} -->
<!-- \end{center} --></p>
<p>Suppose we wanted to quantify the interaction between the treatments in the second example above. We could do so by taking the difference of the simple effects of temperature:
<span class="math display">\[
\left(\mu_{22} -\mu_{12} \right)-\left(\mu_{21} -\mu_{11} \right)=\left(6-4\right)-\left(3-2\right)=2-1=1
\]</span>
Alternatively, we could calculate the difference between the simple effects of material:
<span class="math display">\[
\left(\mu_{22} -\mu_{21} \right)-\left(\mu_{12} -\mu_{11} \right)=\left(6-3\right)-\left(4-2\right)=3-2=1
\]</span>
Each of the above calculations are measures of an <em>interaction effect</em>. In a 2 <span class="math inline">\(\times\)</span> 2 factorial design, an interaction effect is a ``difference of differences’’, or a measure of how one factor effects the simple effects of the other factor. It is not just coincidence that the two measures of the interaction above produce the same number (=1). In fact, as we will show in a moment, in a 2 <span class="math inline">\(\times\)</span> 2 factorial design, there is only a single free difference (or degree of freedom) for the interaction effect. Because there is only a single degree of freedom, the two differences of differences above must be equal. (You might also be able to verify this with a little algebra.)</p>
<p><strong>Perhaps the single greatest conceptual pitfall in the analysis of factorial designs is appreciating the difficulty of interpreting main effects when the experimental factors interact.</strong> When the two factors in an experiment interact, we can still define main effects mathematically. In the example above, the main effect of temperature is now
<span class="math display">\[\bar{\mu}_{2+} -\bar{\mu}_{1+} =4.5-3=1.5\]</span>
and the main effect of material is now
<span class="math display">\[\bar{\mu}_{+2} -\bar{\mu}_{+1} =5-2.5=2.5.\]</span>
Are these differences meaningful? The answer usually depends on the scientific context of the data. In the presence of an interaction, main effects are only meaningful if it makes sense to average over the levels of an experimental factor. While averaging over the levels of one experimental factor may seem harmless for the (relatively mild) interaction above, consider what might happen with a more severe interaction, such as the one here:
<!-- \begin{center} -->
<!-- \begin{tabular}{|p{0.9in}|p{0.9in}|p{0.9in}|p{0.9in}|p{0.8in}|} \hline  -->
<!-- \multicolumn{2}{|p{1in}|}{} & \multicolumn{2}{|p{1.8in}|}{Temperature} &  \\ \hline  -->
<!-- \multicolumn{2}{|p{1in}|}{} & Low & High &  \\ \hline  -->
<!-- Material & Foil & $\mu_{11} =2$ & $\mu_{21} =5$ & $\bar{\mu}_{+1} =3.5$ \\ \hline  -->
<!--  & Plastic & $\mu_{12} =4$ & $\mu_{22} =1$ & $\bar{\mu}_{+2} =2.5$ \\ \hline  -->
<!-- \multicolumn{2}{|p{1in}|}{} & $\bar{\mu}_{1+} =3$ & $\bar{\mu}_{2+} =3$ & $\bar{\mu}_{++} =3$ \\ \hline  -->
<!-- \end{tabular} -->
<!-- \end{center} -->
<img src="06-FactorialExperiments_files/figure-html/unnamed-chunk-4-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>In this case, the simple effects of temperature are equal and opposite. (High temperature increases shelf life by 3 days for foods packed in foil, but decreases shelf life by 3 days for foods packed in plastic.) Thus, the main effect of temperature is zero! Certainly, it would be erroneous to conclude that temperature has no effect on shelf life based on the fact that the marginal means for the two temperature treatments are equal. In this case, the only way to appropriately characterize the effects of temperature is to consider the simple effects. The main effects are downright misleading.</p>
<p>Thus, the conventional approach to analyzing two-way factorial classifications is to <strong>inspect the interaction first</strong>. If the interaction is not significant, then analyze main effects. If the interaction is significant, then analyze simple effects. Main effects and simple effects can be analyzed using the same approaches as a one-factor ANOVA (i.e., <span class="math inline">\(F\)</span>-test for overall effects, followed by linear combinations and/or multiple comparisons procedures if the overall <span class="math inline">\(F\)</span>-test is significant.)</p>
</div>
<div id="analysis-of-a-balanced-2-times-2-factorial-experiment" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Analysis of a balanced 2 <span class="math inline">\(\times\)</span> 2 factorial experiment<a href="factorial-experiments.html#analysis-of-a-balanced-2-times-2-factorial-experiment" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="example-weight-gain-in-rats" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Example: Weight gain in rats<a href="factorial-experiments.html#example-weight-gain-in-rats" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In real life, cell means need to be estimated with data. Consider the following example taken from <span class="citation">Sokal and Rohlf (<a href="#ref-sokal1995biometry" role="doc-biblioref">1995</a>)</span>. This experiment was designed to examine differences in food consumption among rats. 6 male rats and 6 female rats were used in the experiment. Half the rats were fed fresh lard, and half the rats were fed rancid lard. The response is total food consumption (in grams) over 73 days. This is a 2 <span class="math inline">\(\times\)</span> 2 factorial design with a CRD. The experiment is balanced.</p>
<!-- \begin{center} -->
<!-- \begin{tabular}{|p{1.1in}|p{1.1in}|p{1.1in}|p{1.1in}|} \hline  -->
<!-- \multicolumn{2}{|p{1in}|}{} & \multicolumn{2}{|p{2.2in}|}{Fat} \\ \hline  -->
<!-- \multicolumn{2}{|p{1in}|}{} & Fresh & Rancid \\ \hline  -->
<!-- Sex & Male & 709, 679, 699 & 592, 538, 476 \\ \hline  -->
<!--  & Female & 657, 594, 677 & 508, 505, 539 \\ \hline  -->
<!-- \end{tabular} -->
<!-- \end{center} -->
<p>Here is some notation that we will use for two-factor experiments:
+ <span class="math inline">\(a\)</span>: number of levels of factor ``A’’ (here, we’ll set ‘sex’ as this factor, so <span class="math inline">\(a = 2\)</span>.)</p>
<ul>
<li><p><span class="math inline">\(b\)</span>: number of levels of factor ``B’’ (here, we’ll set ‘fat’ as this factor, so <span class="math inline">\(b = 2\)</span>.)</p></li>
<li><p><span class="math inline">\(i = 1, 2, \ldots, a\)</span>: an index to distinguish the different levels of factor A (<span class="math inline">\(i = 1\)</span> for males, <span class="math inline">\(i = 2\)</span> for females)</p></li>
<li><p><span class="math inline">\(j = 1, 2, \ldots, b\)</span>: an index to distinguish the different levels of factor B (<span class="math inline">\(j = 1\)</span> for fresh fat, <span class="math inline">\(j = 2\)</span> for rancid fat)</p></li>
<li><p><span class="math inline">\(n_{ij}\)</span>: sample size for the combination of level <span class="math inline">\(i\)</span> of factor A and level <span class="math inline">\(j\)</span> of factor B (in a balanced design, sometimes this gets replaced by <span class="math inline">\(n\)</span>).</p></li>
<li><p><span class="math inline">\(k = 1, 2, \ldots, n_{ij}\)</span>: an index to distinguish the different observations within each treatment combination</p></li>
<li><p><span class="math inline">\(y_{ijk}\)</span>: <span class="math inline">\(k\)</span>th observation from the combination of level <span class="math inline">\(i\)</span> of factor A and level <span class="math inline">\(j\)</span> of factor B.</p></li>
<li><p><span class="math inline">\(n_T =\sum _{i=1}^{a}\sum _{j=1}^{b}n_{ij}\)</span> : total sample size</p></li>
<li><p><span class="math inline">\(\bar{y}_{ij+} =\dfrac{\sum _{k=1}^{n_{ij}} y_{ijk}}{n_{ij}}\)</span>: sample mean for the combination of level <span class="math inline">\(i\)</span> of factor A and level <span class="math inline">\(j\)</span> of factor B</p></li>
<li><p><span class="math inline">\(\mu_{ij}\)</span> : unknown population mean for the combination of level <span class="math inline">\(i\)</span> of factor A and level <span class="math inline">\(j\)</span> of factor B</p></li>
</ul>
<p>First, let’s examine an interaction plot for the sample means in the rat diet data:
<img src="06-FactorialExperiments_files/figure-html/unnamed-chunk-5-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>We might think that the above plot suggests an interaction, because the lines are not parallel. However, even if there weren’t an interaction, experimental error would probably cause the lines to be non-parallel. Thus, while interaction plots are useful for visualizing the data from a factorial design, they are not suitable for determining statistical significance. Instead, we need a statistical test to determine whether the apparent interaction is statistically significant.</p>
<p>Ina two-factor ANOVA, we can use a sum-of-squares decomposition to test for the statistical significance of the main effects of each factor, and the interaction effects between the factors. (Tests for the statistical significance of the simple effects will require a little more work.) To be explicit, a test for the main effects of factor A is a test of whether the marginal means associated with each level of factor A are equal. That is, we test <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\bar{\mu}_{1+} =\bar{\mu}_{2+} =...=\bar{\mu}_{a+}\)</span> vs. <span class="math inline">\(H_a\)</span>: at least two marginal means for factor A differ.</p>
<p>Similarly, tests for the main effects of factor B are tests of whether the marginal means associated with each level of factor B are equal: <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\bar{\mu}_{+1} =\bar{\mu}_{+2} =...=\bar{\mu}_{+b}\)</span> vs. <span class="math inline">\(H_a\)</span>: at least two marginal means for factor B differ.</p>
<p>It is a bit more difficult to write down the specific hypotheses associated with the test of the interaction. Remember that in the absence of an interaction, the effects of the two experimental factors are additive. In mathematical terms, this means that the difference <span class="math inline">\(\mu_{ij} -\bar{\mu}_{++}\)</span> is equal to sum of the differences <span class="math inline">\(\bar{\mu}_{i+} -\bar{\mu}_{++}\)</span> and <span class="math inline">\(\bar{\mu}_{+j} -\bar{\mu}_{++}\)</span>. In equations, then, the test for the interaction effects is a test of
<span class="math display">\[
H_0: \ \ \mu_{ij} -\bar{\mu}_{++} =\bar{\mu}_{+i} -\bar{\mu}_{++} +\bar{\mu}_{+j} -\bar{\mu}_{++}.
\]</span></p>
<p>Just as with a one-factor ANOVA, we can conduct each of these hypothesis tests using a sum-of-squares decomposition. We’ll begin by partitioning the sum-of-squares into sum-of-squares attributable to variation among the treatment groups and variation within the treatment groups:
<span class="math display">\[\begin{eqnarray*}
\mbox{Total variation: } SS_{Total} &amp; = &amp; \sum_{i=1}^{a}\sum_{j=1}^{b}\sum_{k=1}^{n_{ij} }\left(y_{ijk} -\bar{y}_{+++} \right)^2   \\
\mbox{Variation among groups: } SS_{Groups} &amp; = &amp; \sum_{i=1}^{a}\sum_{j=1}^{b}\sum_{k=1}^{n_{ij} }\left(\bar{y}_{ij+} -\bar{y}_{+++} \right)^2 \\
\mbox{Variation within groups: } SS_{Error} &amp; = &amp; \sum_{i=1}^{a}\sum_{j=1}^{b}\sum_{k=1}^{n_{ij} }\left(y_{ijk} -\bar{y}_{ij+} \right)^2    
\end{eqnarray*}\]</span></p>
<p>Now, we will decompose the <span class="math inline">\(SS_{Groups}\)</span> into three separate SS: one for each of the two main effects, and one for the interaction. Formulas for these sum-of-squares are:
<span class="math display">\[\begin{eqnarray*}
    SS[A] &amp; = &amp; \sum_{i=1}^{a}\sum_{j=1}^{b}\sum_{k=1}^{n_{ij} }\left(\bar{y}_{i++} -\bar{y}_{+++} \right)^2 \\  
    SS[B] &amp; = &amp; \sum_{i=1}^{a}\sum_{j=1}^{b}\sum_{k=1}^{n_{ij} }\left(\bar{y}_{+j+} -\bar{y}_{+++} \right)^2  \\
    SS[AB] &amp; = &amp; \sum_{i=1}^{a}\sum_{j=1}^{b}\sum_{k=1}^{n_{ij} }\left(\bar{y}_{ij+} -\bar{y}_{i++} -\bar{y}_{+j+} +\bar{y}_{+++} \right)^2   
\end{eqnarray*}\]</span>
where SS[AB] denotes the sum-of-squares for the interaction. The formulas for SS[A] and SS[B] should make some sense: they consist of squared differences between the marginal means for one level of an experimental factor and the grand mean. The formula for SS[AB] is a bit more mysterious. Heuristically, the idea is this: with some algebra, the null hypothesis for no interaction <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\mu_{ij} -\bar{\mu}_{++} =\bar{\mu}_{+i} -\bar{\mu}_{++} +\bar{\mu}_{+j} -\bar{\mu}_{++}\)</span> can be re-written as <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\mu_{ij} -\bar{\mu}_{+i} -\bar{\mu}_{+j} +\bar{\mu}_{++} =0\)</span>. Thus, the term <span class="math inline">\(\bar{y}_{ij+} -\bar{y}_{i++} -\bar{y}_{+j+} +\bar{y}_{+++}\)</span> measures the extent to which the mean of group <span class="math inline">\(ij\)</span> departs this null hypothesis.</p>
<p>In a two-way factorial design, when the data are balanced, SS[A], SS[B], and SS[AB] form a perfect decomposition of <span class="math inline">\(SS_{Groups}\)</span>. That is,
<span class="math display">\[
SS_{Groups} = SS[A] + SS[B] + SS[AB]
\]</span>
Again, each sum-of-squares is calculated on the basis of a certain number of free differences. To place the sum-of-squares on equal footing, we need to know how these df are partitioned among the sum-of-squares. For a <span class="math inline">\(a \times b\)</span> factorial design, the number of df used to calculate each sum-of-squares breaks down in the following way:</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
source
</th>
<th style="text-align:left;">
df
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Factor A
</td>
<td style="text-align:left;">
<span class="math inline">\(a-1\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Factor B
</td>
<td style="text-align:left;">
<span class="math inline">\(b-1\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
A*B interaction
</td>
<td style="text-align:left;">
<span class="math inline">\((a-1)(b-1)\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Error
</td>
<td style="text-align:left;">
<span class="math inline">\(n_T-ab\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
<span class="math inline">\(n_T-1\)</span>
</td>
</tr>
</tbody>
</table>
For the rat weight-gain example, the df accounting is
<table>
<thead>
<tr>
<th style="text-align:left;">
source
</th>
<th style="text-align:left;">
df
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Sex
</td>
<td style="text-align:left;">
<span class="math inline">\(1\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Fat
</td>
<td style="text-align:left;">
<span class="math inline">\(1\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Sex*Fat
</td>
<td style="text-align:left;">
<span class="math inline">\(1\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Error
</td>
<td style="text-align:left;">
<span class="math inline">\(8\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
<span class="math inline">\(11\)</span>
</td>
</tr>
</tbody>
</table>
<p>To compare the sum-of-squares on equal footing, we need to calculate mean squares (MS), which are just equal to each sum-of-squares divided by its corresponding df. Here’s a sum-of-squares decomposition for the rat data:</p>
<pre><code>        Source    df     SS     MS 
        Sex        1   3781   3781
        Fat        1  61204  61204
        Sex*Fat    1    919    919 
        Error      8  11667   1458
        Total     11  77570</code></pre>
<p>We can test for the statistical significance of the main effects of Sex and Fat, and for the interaction between Sex and Fat, by computing F-ratios. F-ratios are simply the ratio of the MS for the factor being tested divided by the MS(Error). For example, the F-ratio for the test of the interaction is
<span class="math display">\[
F = \dfrac{MS[AB]}{MSE} = \dfrac{919}{1458}  = 0.63
\]</span>
We can calculate a p-value for the test of the null hypothesis that there is no interaction between the factors by comparing this statistic to an <span class="math inline">\(F\)</span>-distribution with 1 numerator df and 8 denominator df. (The numerator and denominator df are just the number of free differences used to calculate the MS in the numerator and denominator of the <span class="math inline">\(F\)</span> statistic, respectively.) Because larger values of the <span class="math inline">\(F\)</span> statistic provide more evidence against the null hypothesis, <span class="math inline">\(p\)</span>-values associated with the F-test are always one-tailed, and are the probability of observing a test statistic at least as large as the value observed. Here, the probability of observing an <span class="math inline">\(F\)</span>-statistic with 1 ndf and 8 ddf greater than or equal to 0.63 is <span class="math inline">\(p=0.45\)</span>. The <span class="math inline">\(p\)</span> value is large, so we have no evidence that the two factors in this experiment interact, with respect to their effect on weight gain.</p>
<p>We can conduct similar calculations to test for the main effects of ‘sex’ and ‘fat’, using MS[A] and MS[B], respectively. We’ll use these values to complete our ANOVA table for the rat example:</p>
<pre><code>        Source    df     SS     MS      F        p
        Sex        1   3781   3781   2.59   0.1460
        Fat        1  61204  61204  42.0    0.0002
        Sex*Fat    1    919    919   0.63   0.4503
        Error      8  11667   1458
        Total     11  77570</code></pre>
<p>Thus, there is no evidence of an interaction between the factors. Proceeding with an analysis of the main effects, there is no evidence that the marginal means for male vs. female rates differ (<span class="math inline">\(p=0.146\)</span>), but there is strong evidence that the marginal means between fresh vs. rancid fat differ (<span class="math inline">\(p = 0.0002\)</span>).</p>
<p>Because the main effect of ‘fat’ is significant, we want to go further and say more about the sign and magnitude of this effect. To do so, we’ll form a contrast that quantifies the main effect of fat.<br />
<span class="math display">\[
\theta _{fat} =\bar{\mu}_{+2} -\bar{\mu}_{+1} =\frac{\mu_{12} +\mu_{22} }{2} -\frac{\mu_{11} +\mu_{21} }{2}
\]</span>
We’ve written this contrast as the difference between the marginal mean for rancid fat and the marginal mean for fresh fat. Thus, a positive value corresponds to rats gaining more weight when fed rancid fat, and a negative value corresponds to rats gaining more weight when fed fresh fat. We first estimate the linear combination simply by plugging in sample means:
<span class="math display">\[
{\hat{\theta }_{fat} =\dfrac{\bar{y}_{12+} +\bar{y}_{22+} }{2} -\dfrac{\bar{y}_{11+} +\bar{y}_{21+} }{2} }  {=\dfrac{535+517}{2} -\dfrac{696+643}{2} }  {=-143}
\]</span>
Thus, we estimate that rats fed rancid fat gain 143g less weight than rats fed fresh fat. Next, we estimate the standard error of this linear combination using a modification of the formula from linear combinations for one-factor ANOVA:
<span class="math display">\[\begin{eqnarray*}
s_{\hat{\theta}} &amp; = &amp; \sqrt{\left\{\frac{c_{11}^2}{n_{11}} +\frac{c_{12}^2}{n_{12}} +\frac{c_{21}^2}{n_{21}} +\frac{c_{22}^2}{n_{22}} \right\} MS_{Error} } \\
&amp; = &amp; \sqrt{\left\{\frac{\left(-1/2\right)^2 }{3} +\frac{\left(1/2\right)^2 }{3} +\frac{\left(-1/2\right)^2 }{3} +\frac{\left(1/2\right)^2 }{3} \right\}1458}  \\
&amp; = &amp; 22.0
\end{eqnarray*}\]</span>
Finally, if we wanted a 95% CI for this linear combination, we could form one by taking the appropriate critical values from a t-distribution with 8 df. (Here, 8 df because the MSE is calculated based on 8 df). Here, the appropriate critical value is 2.306, so a 95% CI for the main effect of fat is -143g <span class="math inline">\(\pm\)</span> 2.306 <span class="math inline">\(\times\)</span> 22.0g = (-92g, -194g).</p>
</div>
<div id="analysis-using-proc-glm-in-sas" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Analysis using PROC GLM in SAS<a href="factorial-experiments.html#analysis-using-proc-glm-in-sas" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can have SAS do these calculations for us. In PROC GLM, we can obtain these calculations with the code:</p>
<pre><code>proc glm data=rat;
  class trt sex fat;
  model food = sex fat sex*fat;
run;</code></pre>
<p>Edited output:</p>
<pre><code>                                        Sum of
Source                      DF         Squares     Mean Square    F Value    Pr &gt; F
Model                        3     65903.58333     21967.86111      15.06    0.0012
Error                        8     11666.66667      1458.33333
Corrected Total             11     77570.25000

Source                      DF     Type III SS     Mean Square    F Value    Pr &gt; F
sex                          1      3780.75000      3780.75000       2.59    0.1460
fat                          1     61204.08333     61204.08333      41.97    0.0002
sex*fat                      1       918.75000       918.75000       0.63    0.4503</code></pre>
<p>Let’s note the following:</p>
<ol style="list-style-type: decimal">
<li>In the model statement, we can use a vertical bar as shorthand for including both main effects and interactions. The code below would produce identical output to the code above:</li>
</ol>
<pre><code>proc glm data=rat;
  class trt sex fat;
  model food = sex|fat;
run;</code></pre>
<ol start="2" style="list-style-type: decimal">
<li><p>PROC GLM provides two tables. The first is the sum-of-squares breakdown and associated ANOVA F-test if we were just treating the data as a one-factor ANOVA. That is, the <span class="math inline">\(F\)</span>-test in the first table is a test of <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\mu_{11} =\mu_{12} =\mu_{21} =...=\mu_{ab}\)</span>. (This is equivalent to a model-utility test in multiple regression.) The second table of output provides <span class="math inline">\(F\)</span>-tests for the main and interaction effects.</p></li>
<li><p>PROC GLM provides two sum-of-squares decompositions, one which it calls Type I and another which it calls Type III. Type I and Type III sum-of-squares are identical for balanced factorial designs. They are not identical for unbalanced designs. We will discuss the differences for unbalanced designs later.</p></li>
</ol>
<p>We can also have PROC GLM calculate linear combinations of cell means for us. To do so, though, we need to understand the <em>effects-model</em> coding that PROC GLM uses to represent the two-factor ANOVA model.</p>
</div>
<div id="effects-notation-for-the-two-factor-anova" class="section level3 hasAnchor" number="6.3.3">
<h3><span class="header-section-number">6.3.3</span> Effects notation for the two-factor ANOVA<a href="factorial-experiments.html#effects-notation-for-the-two-factor-anova" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For a two-factor ANOVA, we extend our effects-model notation in the following way:
<span class="math display">\[
\mu_{ij} =\mu +\alpha_i +\beta_j +\left(\alpha \beta \right)_{ij}
\]</span>
Here, <span class="math inline">\(\mu\)</span> is the reference level, <span class="math inline">\(\alpha_i\)</span> is the “effect” of level <span class="math inline">\(i\)</span> of factor A, <span class="math inline">\(\beta_j\)</span> is the “effect” of level <span class="math inline">\(j\)</span> of factor B, and <span class="math inline">\(\left(\alpha \beta \right)_{ij}\)</span> is the interaction between level <span class="math inline">\(i\)</span> of factor A and level <span class="math inline">\(j\)</span> of factor B. In one-factor ANOVA, we saw that it was not possible to estimate all the <span class="math inline">\(\alpha_i\)</span>’s uniquely, so we had to impose a constraint. A similar phenomenon prevails in the two-factor model. How many constraints do we need? The key equivalence is that the number of effects parameters that we can estimate is equal to the number of df for each effect in the df accounting.</p>
<!-- Here are the rules for dfs and constraints in the 2 $\times$ 2 factorial design.  For the main effects of factor A, there are 2 parameters and 1 df, so we must constrain one of the $\alpha_i$'s to be equal to 0.  For the main effects of factor B, there are also 2 parameters and 1 df, so we must constrain one of the $\beta_j$'s to be equal to 0.  For the interaction effects, there are 4 interaction parameters $\left(\alpha \beta \right)_{11} ,\left(\alpha \beta \right)_{12} ,\left(\alpha \beta \right)_{21} ,\left(\alpha \beta \right)_{22}$, but only 1 df for the interaction, so we must constrain 3 of the interaction effects to be equal to zero. -->
<!-- Another way to think about the set-to-zero constraints is to relate the two-factor ANOVA model to regression with indicators.  More information about this equivalence can be found in a later section in this chapter of the notes. -->
<p>Again, PROC GLM uses set-to-zero constraints. We can see the constraints by calling for SAS’s parameter estimates with the SOLUTION option to the MODEL statement in PROC GLM:</p>
<pre><code>proc glm data=rat;
  class trt sex fat;
  model food = sex|fat / solution;
run;

The GLM Procedure
                                                  Standard
Parameter                       Estimate             Error    t Value    Pr &gt; |t|
Intercept                    535.3333333 B     22.04792759      24.28      &lt;.0001
sex       female             -18.0000000 B     31.18047822      -0.58      0.5796
sex       male                 0.0000000 B       .                .         .
fat       fresh              160.3333333 B     31.18047822       5.14      0.0009
fat       rancid               0.0000000 B       .                .         .
sex*fat   female fresh       -35.0000000 B     44.09585518      -0.79      0.4503
sex*fat   female rancid        0.0000000 B       .                .         .
sex*fat   male fresh           0.0000000 B       .                .         .
sex*fat   male rancid          0.0000000 B       .                .         .

NOTE: The X&#39;X matrix has been found to be singular, and a generalized inverse was used to
solve the normal equations.  Terms whose estimates are followed by the letter &#39;B&#39;
are not uniquely estimable.</code></pre>
<p>Now, to calculate the main effect of fat using SAS, we have to recode our linear combination in terms of the parameters in the effects model. Here goes:
<span class="math display">\[\begin{eqnarray*}
\theta _{fat} &amp; = &amp; \frac{1}{2} \left(\mu_{12} +\mu_{22} -\mu_{11} -\mu_{21} \right) \\
&amp; = &amp; \frac{1}{2} \left(\mu +\alpha _{1} +\beta _{2} +\left(\alpha \beta \right)_{12} +\mu +\alpha _{2} +\beta _{2} +\left(\alpha \beta \right)_{22} -\mu -\alpha _{1} -\beta _{1} -\left(\alpha \beta \right)_{11} -\mu -\alpha _{2} -\beta _{1} -\left(\alpha \beta \right)_{21} \right) \\
&amp; = &amp; \frac{1}{2} \left(2\beta _{2} +\left(\alpha \beta \right)_{12} +\left(\alpha \beta \right)_{22} -2\beta _{1} -\left(\alpha \beta \right)_{11} -\left(\alpha \beta \right)_{21} \right) \\
&amp; = &amp; -\beta _{1} +\beta _{2} -\frac{1}{2} \left(\alpha \beta \right)_{11} +\frac{1}{2} \left(\alpha \beta \right)_{12} -\frac{1}{2} \left(\alpha \beta \right)_{21} +\frac{1}{2} \left(\alpha \beta \right)_{22}
\end{eqnarray*}\]</span>
Now we can read off the coefficients from the last line of the expression above and feed them into an ESTIMATE statement. Note that this combination only involves parameters for the ‘fat’ effect and the interaction:</p>
<pre><code>proc glm data=rat;
  class trt sex fat;
  model food = sex|fat;
  estimate &#39;Fresh v. rancid&#39; fat -1 1 sex*fat -.5 .5 -.5 .5;   
run;                                            

Parameter                   Estimate  Standard Error    t Value    Pr &gt; |t|
Fresh v. rancid          -142.833333      22.0479276      -6.48      0.0002</code></pre>
<p>As always, SAS provides (for free) a p-value for the test of <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\theta =0\)</span> vs. <span class="math inline">\(H_a\)</span>: <span class="math inline">\(\theta \ne 0\)</span>. For illustration’s sake, we can also write linear combinations for the main effect of sex, and for the interaction effect. One way to write these linear combinations is:
<span class="math display">\[\theta _{sex} =\frac{\mu_{21} +\mu_{22} }{2} -\frac{\mu_{11} +\mu_{12} }{2} \]</span>
and
<span class="math display">\[\theta _{int} =\left(\mu_{22} -\mu_{12} \right)-\left(\mu_{21} -\mu_{11} \right)\]</span>
Using PROC GLM, we estimate these effects as</p>
<pre><code>proc glm data=rat;
  class trt sex fat;
  model food = trt;
  estimate &#39;Fresh v. rancid&#39; fat -1 1 sex*fat -.5 .5 -.5 .5; 
  estimate &#39;Male v. female&#39;  sex -1 1 sex*fat -.5 -.5 .5 .5; 
  estimate &#39;Interaction&#39;     sex*fat 1 -1 -1 1;
run;         
                       
Parameter                   Estimate  Standard Error    t Value    Pr &gt; |t|
Fresh v. rancid          -142.833333      22.0479276      -6.48      0.0002
Male v. female            -35.500000      22.0479276      -1.61      0.1460
Interaction                35.000000      44.0958552       0.79      0.4503</code></pre>
<p>Thus, to summarize our analysis of this experiment: There is no evidence of a statistically significant sex effect. There is strong evidence of an effect of fat on food consumption (<span class="math inline">\(F_{1,8} = 41.97\)</span>, <span class="math inline">\(p=0.0002\)</span>). Rats fed rancid fat consumed on average 142.9g (s.e. 22.0g) less food than rats fed fresh fat.</p>
<p>Just as with one-factor ANOVA, the effects notation also gives us another way to write the null hypotheses for the main effects and interaction effects. The test of the main effects of factor A is equivalent to a test of
<span class="math display">\[
H_{0} :\alpha _{1} =\alpha _{2} =...=\alpha _{a} =0.
\]</span>
The test of the main effects of factor B is equivalent to a test of
<span class="math display">\[
H_{0} :\beta _{1} =\beta _{2} =...=\beta _{b} =0.
\]</span>
Finally, the test the interaction between factors A and B is equivalent to a test of
<span class="math display">\[
H_{0} :\left(\alpha \beta \right)_{11} =\left(\alpha \beta \right)_{12} =...=\left(\alpha \beta \right)_{ab} =0.
\]</span></p>
</div>
<div id="a-second-example" class="section level3 hasAnchor" number="6.3.4">
<h3><span class="header-section-number">6.3.4</span> A second example<a href="factorial-experiments.html#a-second-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This example is taken from <span class="citation">Steel, Torrie, and Dickey (<a href="#ref-steel1997principles" role="doc-biblioref">1997</a>)</span>. In their words,</p>
<blockquote>
<p>Wilkinson (1954) reports the results of an experiment to study the influence of time of bleeding and diethylstilbestrol (an estrogenic compound) on plasma phospholipid in lambs. Five lambs were assigned at random to each of four treatment groups; treatment combinations are for morning and afternoon bleeding and with and without diethylstilbestrol treatment.</p>
</blockquote>
<!-- The data are: -->
<!-- \begin{center} -->
<!--    \begin{tabular}{|p{1.1in}|p{1.1in}|p{1.1in}|p{1.1in}|} \hline  -->
<!--        \multicolumn{2}{|p{1in}|}{} & \multicolumn{2}{|p{2.2in}|}{Time of bleeding} \\ \hline  -->
<!--        \multicolumn{2}{|p{1in}|}{} & AM & PM \\ \hline  -->
<!--        Diethylstilbestrol trt & Control & 8.53, 20.53, 12.53, 14.00, 10.80 & 39.14, 26.20, 31.33, 45.80, 40.20 \\ \hline  -->
<!--        & Treated & 17.53, 21.07, 20.80, 17.33, 20.07 & 32.00, 23.80, 28.87, 25.06, 29.33 \\ \hline  -->
<!--    \end{tabular} -->
<!-- \end{center} -->
<p>An interaction plot of the data is shown below.
<img src="06-FactorialExperiments_files/figure-html/unnamed-chunk-8-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>Here is the output of a two-factor ANOVA model using PROC GLM:</p>
<pre><code>proc glm data=sheep;
  class time drug trt;
  model y = time|drug;
run;

                                        Sum of
Source                      DF         Squares     Mean Square    F Value    Pr &gt; F
Model                        3     1539.406600      513.135533      21.61    &lt;.0001
Error                       16      379.923280       23.745205
Corrected Total             19     1919.329880

Source                      DF     Type III SS     Mean Square    F Value    Pr &gt; F
time                         1     1256.746580     1256.746580      52.93    &lt;.0001
drug                         1        8.712000        8.712000       0.37    0.5532
time*drug                    1      273.948020      273.948020      11.54    0.0037</code></pre>
<p>In contrast to the rat example, the interaction here is statistically significant. Because the interaction is significant, the <span class="math inline">\(F\)</span>-tests of the main effects may no longer have a clear interpretation. Instead, we’ll analyze the simple effects of the two factors by estimating the linear combinations associated with each:</p>
<pre><code>proc glm data=sheep;
  class time drug trt;
  model y = time|drug;
  estimate &#39;Simple effect of time, drug=no&#39; time 1 -1 time*drug 1 0 -1 0;
  estimate &#39;Simple effect of time, drug=yes&#39; time 1 -1 time*drug 0 1 0 -1;
  estimate &#39;Simple effect of drug, time=AM&#39; drug -1 1 time*drug -1 1 0 0;
  estimate &#39;Simple effect of drug, time=PM&#39; drug -1 1 time*drug 0 0 -1 1;
run;

Parameter                              Estimate           Error    t Value    Pr &gt; |t|
Simple effect of time, drug=no      -23.2560000      3.08189585      -7.55      &lt;.0001
Simple effect of time, drug=yes      -8.4520000      3.08189585      -2.74      0.0145
Simple effect of drug, time=AM        6.0820000      3.08189585       1.97      0.0660
Simple effect of drug, time=PM       -8.7220000      3.08189585      -2.83      0.0121</code></pre>
<p>Here is a partial interpretation of these contrasts. Sheep with blood drawn in the afternoon have more plasma phospholipid than sheep with blood drawn in the morning, regardless of whether the sheep were given the drug. However, the magnitude of the effect of timing is smaller on sheep given the drug (estimated effect = 8.5 units, s.e. = 3.1) than it is on sheep not given the drug (estimated effect = 23.3 units, s.e.=3.1). For sheep with blood drawn in the afternoon, the drug decreases plasma phospholipid relative to the control (estimated effect = 8.7 units less with the drug, s.e. = 3.1). For sheep with blood drawn in the morning, there is only weak evidence that the drug has an effect on plasma phospholipid (estimated effect = 6.1 units more with the drug, s.e. = 3.1, <span class="math inline">\(p=0.066\)</span>).</p>
<!-- ## The 2 $\times$ 2 factorial design with unbalanced data -->
<!-- Remember that {\em balance} in our usage refers to the number of replicates per treatment combination.  Balanced data have the same number of replicates per treatment combination, while unbalanced data do not.  Unbalanced data present two additional complications for analysis.  Balanced data permit a clean sums-of-squares decomposition:  -->
<!-- \[ -->
<!-- SS_{Groups} = SS[A]+ SS[B] + SS[AB] -->
<!-- \] -->
<!-- This decomposition gives rise to $F$-tests for main effects and an interaction.  With unbalanced data, the sum-of-squares decomposition above does not hold.  There are several different (but equally valid) alternative definitions of sums-of-squares in the unbalanced case, but they give rise to different hypothesis tests about treatment effects.  To illustrate, suppose that several sheep in the above example went missing, so that now the data are: -->
<!-- \begin{center} -->
<!--    \begin{tabular}{|p{1.1in}|p{1.1in}|p{1.1in}|p{1.1in}|} \hline  -->
<!--        \multicolumn{2}{|p{1in}|}{} & \multicolumn{2}{|p{2.2in}|}{Time of bleeding} \\ \hline  -->
<!--        \multicolumn{2}{|p{1in}|}{} & AM & PM \\ \hline  -->
<!--        Diethylstilbestrol trt & Control & 8.53, 20.53 & 39.14, 26.20, 31.33, 45.80, 40.20 \\ \hline  -->
<!--        & Treated & 17.53, 21.07, 20.80 & 32.00, 23.80, 28.87, 25.06 \\ \hline  -->
<!--    \end{tabular} -->
<!-- \end{center} -->
<!-- To illustrate the differences among sums of squares, several different model fits and their SSE's are presented below: -->
<!-- \begin{center} -->
<!--    \begin{tabular}{|p{1.9in}|p{1.2in}|} \hline  -->
<!--        Model & SS${}_{Error}$ \\ \hline  -->
<!--        y = ; & 1282.7 \\ \hline  -->
<!--        y = time;  & 578.9 \\ \hline  -->
<!--        y = drug;  & 1153.0 \\ \hline  -->
<!--        y = time drug; & 522.4 \\ \hline  -->
<!--        $^\dagger$y = time time*drug & 375.9 \\ \hline  -->
<!--        $^\dagger$y = drug time*drug & 1048.9 \\ \hline  -->
<!--        y = time drug time*drug; & 361.5 \\ \hline  -->
<!--    \end{tabular} -->
<!-- \end{center} -->
<!-- $^\dagger$Note: These are not models that one would typically consider, because they include an interaction but do not include both of the main effects for the interacting factors.  They are only here for the purposes of sum-of-squares calculations. -->
<!-- Below are Type I and III sum-of-squares decompositions for the unbalanced sheep data.  In each case, the MS is equal to the sum-of-squares divided by the df associated with the effect (here, 1 in all cases), while the $F$-statistic associated with each effect is equal to the MS for the effect divided by the MSE (not shown, but identical regardless of whether one uses Type I or III SS).   -->
<!-- Type I sum-of-squares are {\em sequential} SS.  They are calculated as the reduction in the $SS_{Error}$  achieved by adding a term to a model that doesn't already contain that term.  They depend on the order in which the terms are specified in the model. -->
<!-- ```{} -->
<!-- proc glm data=sheep; -->
<!--   class time drug trt; -->
<!--   model y = time drug time*drug / ss1; -->
<!-- run; -->
<!-- Source                      DF       Type I SS     Mean Square    F Value    Pr > F -->
<!-- time                         1     703.7611740     703.7611740      19.47    0.0013 -->
<!-- drug                         1      56.4701046      56.4701046       1.56    0.2398 -->
<!-- time*drug                    1     160.9402693     160.9402693       4.45    0.0610 -->
<!-- ``` -->
<!-- That is to say, the Type I sum-of-squares for `time' is the reduction in SSE between the model with nothing on the right-hand side to the model ``y = time''.  (Verify this using the table above.)  The Type I sum-of-squares for `drug' (in this model) is then the difference in SSE between the models ``y = time'' and ``y = time drug''.  (Again, verify this using the table above.)  Finally, the Type I sum-of-squares for the interaction is the difference in SSE between the models ``y = time drug'' and ``y = time drug time*drug''.   -->
<!-- Changing the order in which the treatments appear in the model statement changes the Type I SS.  This is undesirable.  Compare the SS table above with the one below: -->
<!-- ```{} -->
<!-- proc glm data=sheep; -->
<!--   class time drug trt; -->
<!--   model y = drug time time*drug / ss1; -->
<!-- run; -->
<!-- Source                      DF       Type I SS     Mean Square    F Value    Pr > F -->
<!-- drug                         1     129.6257143     129.6257143       3.59    0.0875 -->
<!-- time                         1     630.6055643     630.6055643      17.45    0.0019 -->
<!-- time*drug                    1     160.9402693     160.9402693       4.45    0.0610 -->
<!-- ``` -->
<!-- % Type II sum-of-squares are {\em hierarchical }SS.  They are calculated as the reduction in the SS${}_{Error}$  achieved by adding a term to a model that contains all other terms of equal or lower order.  (Here, main effects are first-order terms, and interactions between two factors are second-order terms.) -->
<!-- Type III sum-of-squares are {\em partial }SS.  They are calculated as the reduction in the $SS_{Error}$  achieved by adding a term to a model that contains all other terms. -->
<!-- ```{} -->
<!-- proc glm data=sheep; -->
<!--   class time drug trt; -->
<!--   model y = time drug time*drug / ss3; -->
<!-- run; -->
<!-- Source                      DF     Type III SS     Mean Square    F Value    Pr > F -->
<!-- time                         1     684.4068563     684.4068563      18.93    0.0014 -->
<!-- drug                         1      11.4392667      11.4392667       0.32    0.5861 -->
<!-- time*drug                    1     160.9402693     160.9402693       4.45    0.0610 -->
<!-- ``` -->
<!-- Hypotheses tested by $F$-tests using Type I (and Type II) sum-of-squares are affected by the lack of balance in the data.  These hypotheses are rarely scientifically interesting unless the lack of balance in the data is representative of the frequencies with which treatment combinations occur in the population being studied.  Hypotheses tested by $F$-tests using Type III sum-of-squares are weighted to adjust for lack of balance, and are typically the scientific hypotheses of interest in an experimental study.  In ST512, we recommend using Type III sum-of-squares to test hypotheses of interest. -->
<!-- When data are unbalanced, one also has the option of treating the data as if they arose from a one-factor treatment structure.  This removes complications about different types of SS, but eliminates the ability to attribute differences in the response to one factor vs.\ the other. -->
</div>
</div>
<div id="a-times-b-factorial-designs" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> <span class="math inline">\(a \times b\)</span> factorial designs<a href="factorial-experiments.html#a-times-b-factorial-designs" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="example-without-a-significant-interaction" class="section level3 hasAnchor" number="6.4.1">
<h3><span class="header-section-number">6.4.1</span> Example without a significant interaction<a href="factorial-experiments.html#example-without-a-significant-interaction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
Oehlert (problem 8.5) reports the following data. <span class="citation">Low and Bin Mohd. Ali (<a href="#ref-low1985experimental" role="doc-biblioref">1985</a>)</span> studied the collection of pine oleoresin by tapping the trunks of pine trees. Tapping involves cutting a hole in the tree trunk and collecting resin that seeps out. This experiment compared four shapes of holes (circle, diagonal, check, or rectangle) and the efficacy of acid (added vs. control) in collecting resin. Twenty-four pine trees were selected from a plantation and were randomly assigned to each of the 8 possible combinations of hole shape and acid. The response is total grams of resin collected from the hole. The data that we will work with are not the actual data but are instead a hypothetical data set for the same design.
<!-- \begin{center} -->
<!--    \begin{tabular}{|p{0.9in}|p{0.9in}|p{0.9in}|p{0.9in}|p{0.9in}|} \hline  -->
<!--        & Circle ($i=1$) & Diagonal ($i=2$) & Check ($i=3$) & Rectangle ($i=4$) \\ \hline  -->
<!--        Control ($j=1$) & 9, 13, 12  & 43, 48, 57  & 60, 65, 70  & 77, 70, 91  \\ \hline  -->
<!--        Acid added ($j=2$) & 15, 13, 20  & 66, 58, 73  & 75, 78, 90  & 97, 108, 99  \\ \hline  -->
<!--    \end{tabular} -->
<!-- \end{center} -->
This is a balanced, replicated 4 <span class="math inline">\(\times\)</span> 2 factorial experiment with treatment combinations assigned in a CRD. We’ll extend our ideas from the analysis of the 2 <span class="math inline">\(\times\)</span> 2 factorial experiments to this 4 <span class="math inline">\(\times\)</span> 2 factorial experiment. First, visualize the effects of the two factors with an interaction plot:
<img src="06-FactorialExperiments_files/figure-html/unnamed-chunk-9-1.png" width="480" style="display: block; margin: auto;" />
Our notation here will be similar to the 2 <span class="math inline">\(\times\)</span> 2 factorial design, except that now we’ll allow an arbitrary number of levels for the two crossed experimental factors. Our strategy for analyzing data from a <span class="math inline">\(a \times b\)</span> factorial design will still be to use a sum-of-squares decomposition to test for interaction effects and/or main effects of the two factors. As with a 2 <span class="math inline">\(\times\)</span> 2 design, when the data are balanced, the sum-of-squares for the groups can be exactly partitioned into the three components using the formulas that we introduced before:
<span class="math display">\[
SS_{Groups} = SS[A] + SS[B] + SS[AB]
\]</span>
For the oleoresin data, this decomposition is
<table>
<thead>
<tr>
<th style="text-align:left;">
source
</th>
<th style="text-align:left;">
df
</th>
<th style="text-align:left;">
SS
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Shape
</td>
<td style="text-align:left;">
3
</td>
<td style="text-align:left;">
19407
</td>
</tr>
<tr>
<td style="text-align:left;">
Acid
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
1305
</td>
</tr>
<tr>
<td style="text-align:left;">
Shape*Acid
</td>
<td style="text-align:left;">
3
</td>
<td style="text-align:left;">
237
</td>
</tr>
<tr>
<td style="text-align:left;">
Error
</td>
<td style="text-align:left;">
16
</td>
<td style="text-align:left;">
721
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
23
</td>
<td style="text-align:left;">
21672
</td>
</tr>
</tbody>
</table>
Now, we can use this sum-of-squares breakdown to test for the statistical significance of the interaction and/or the main effects. As before, although we can always define these tests mathematically, the tests of the main effects will only be meaningful if the interaction is not significant. Also, as before, tests of the main and interaction effects are <span class="math inline">\(F\)</span>-tests that compare the mean-square for the effect of interest to the mean squared error. That is, the <span class="math inline">\(F\)</span>-statistic to test for the interaction is
<span class="math display">\[
F = MS[AB] / MSE
\]</span>
and the <span class="math inline">\(F\)</span>-statistics to test for the main effects are
<span class="math display">\[
F = MS[A] / MSE
\]</span>
and
<span class="math display">\[
F = MS[B] / MSE.
\]</span>
In the usual way, all <span class="math inline">\(F\)</span>-tests yield one-tailed <span class="math inline">\(p\)</span>-values. All of the information for these tests can be compiled into an ANOVA table:
<table>
<thead>
<tr>
<th style="text-align:left;">
source
</th>
<th style="text-align:left;">
df
</th>
<th style="text-align:left;">
SS
</th>
<th style="text-align:left;">
MS
</th>
<th style="text-align:left;">
<span class="math inline">\(F\)</span>
</th>
<th style="text-align:left;">
<span class="math inline">\(p\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Shape
</td>
<td style="text-align:left;">
3
</td>
<td style="text-align:left;">
19407
</td>
<td style="text-align:left;">
6469
</td>
<td style="text-align:left;">
143.5
</td>
<td style="text-align:left;">
&lt;0.0001
</td>
</tr>
<tr>
<td style="text-align:left;">
Acid
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
1305
</td>
<td style="text-align:left;">
1305
</td>
<td style="text-align:left;">
29
</td>
<td style="text-align:left;">
&lt;0.0001
</td>
</tr>
<tr>
<td style="text-align:left;">
Shape*Acid
</td>
<td style="text-align:left;">
3
</td>
<td style="text-align:left;">
237
</td>
<td style="text-align:left;">
79.2
</td>
<td style="text-align:left;">
1.76
</td>
<td style="text-align:left;">
0.1961
</td>
</tr>
<tr>
<td style="text-align:left;">
Error
</td>
<td style="text-align:left;">
16
</td>
<td style="text-align:left;">
721
</td>
<td style="text-align:left;">
45.1
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
23
</td>
<td style="text-align:left;">
21672
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
</tbody>
</table>
<p>Thus, this two-factor ANOVA shows that there is no statistical evidence of an interaction (<span class="math inline">\(F_{3, 16} = 1.76\)</span>, <span class="math inline">\(p = 0.20\)</span>). Because the interaction is not significant, it makes sense to analyze main effects. There is very strong evidence of a main effect of shape (<span class="math inline">\(F_{3, 16} = 143.5\)</span>, <span class="math inline">\(p &lt; 0.0001\)</span>) and a main effect of the acid treatment (<span class="math inline">\(F_{1, 16}\)</span> = 29.0, <span class="math inline">\(p &lt; 0.0001\)</span>).</p>
<div id="linear-contrasts-and-multiple-comparisons-in-the-a-times-b-design" class="section level4 hasAnchor" number="6.4.1.1">
<h4><span class="header-section-number">6.4.1.1</span> Linear contrasts and multiple comparisons in the <span class="math inline">\(a \times b\)</span> design<a href="factorial-experiments.html#linear-contrasts-and-multiple-comparisons-in-the-a-times-b-design" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Because the main effects are significant (and the interaction is not significant), then we can use linear contrasts and multiple comparisons procedures to analyze the marginal means of both factors just as we would in a one-factor layout. We’ll illustrate both here for completeness, although the concepts should be familiar.</p>
<p>Example of a linear contrast: Suppose that we wanted to ask if the average amount of resin collected from circular holes was different from the average resin collected from diagonal holes. We can first define a linear contrast that quantifies the difference between these two marginal means:
<span class="math display">\[
\theta =\frac{\mu_{11} +\mu_{12} }{2} -\frac{\mu_{21} +\mu_{22} }{2}
\]</span>
Using all of our formulas from before, we could estimate this linear combination by plugging in sample means. We could then estimate the standard error, and use the estimate and the standard error to test <span class="math inline">\(H_{0} :\theta =0\)</span> vs. <span class="math inline">\(H_{a} :\theta \ne 0\)</span>. To coerce SAS to do this for us, we need to do algebra with the effects parameters:
<span class="math display">\[\begin{eqnarray*}
\theta  &amp; = &amp; \frac{\mu_{11} +\mu_{12} }{2} -\frac{\mu_{21} +\mu_{22} }{2} \\
  &amp; = &amp; \frac{\left(\mu +\alpha _{1} +\beta _{1} +\left(\alpha \beta \right)_{11} \right)+...}{2} -\frac{\left(\mu +\alpha _{2} +\beta _{1} +\left(\alpha \beta \right)_{21} \right)+...}{2}  \\
  &amp; = &amp; \alpha _{1} -\alpha _{2} +\frac{\left(\alpha \beta \right)_{11} +\left(\alpha \beta \right)_{12} }{2} -\frac{\left(\alpha \beta \right)_{21} +\left(\alpha \beta \right)_{22} }{2}
\end{eqnarray*}\]</span>
Thus, in SAS:</p>
<pre><code>proc glm data=resin;
  class shape acid;
  model resin = shape acid shape*acid / solution;
  estimate &#39;Circle vs. Diagonal&#39; shape 0 1 -1 0 shape*acid 0 0 .5 .5 -.5 -.5 0 0;
run;

                                            Standard
Parameter                   Estimate           Error    t Value    Pr &gt; |t|
Circle vs. Diagonal      -43.8333333      3.87656778     -11.31      &lt;.0001</code></pre>
<p>Thus, we conclude that there is strong evidence that diagonal holes produce more resin than circular holes.</p>
<p>Notes:
1. PROC GLM orders the levels of experimental factors alphabetically. Thus, the ordering for the shape factor above is check, circle, diagonal, slash. Thus, the zeros are needed in the estimate statement because in SAS’s coding circle corresponds to <span class="math inline">\(i=2\)</span> and diagonal corresponds to <span class="math inline">\(i=3\)</span>.</p>
<ol start="2" style="list-style-type: decimal">
<li>Also, if any of the coefficients have a training string of zeros, we can omit those from the ESTIMATE statement. SAS will replace any missing coefficients with zeros. The above ESTIMATE statement could have been equivalently written as</li>
</ol>
<pre><code>estimate &#39;Circle vs. Diagonal&#39; shape 0 1 -1 shape*acid 0 0 .5 .5 -.5 -.5;</code></pre>
<p>Alternatively, we could use a MEANS statement to compare treatment means using multiple comparisons. Here is code for a Tukey’s HSD comparison of the marginal means for the hole shapes:</p>
<pre><code>proc glm data=resin;
  class shape acid;
  model resin = shape acid shape*acid;
  means shape / tukey;
run;

Tukey&#39;s Studentized Range (HSD) Test for resin
Means with the same letter are not significantly different.

Tukey 
Grouping          Mean      N    shape
A               90.333      6    rectangl
B               73.000      6    check
C               57.500      6    diagonal
D               13.667      6    circular</code></pre>
<p>Thus, all of the shapes are significantly different from one another.</p>
</div>
</div>
<div id="example-with-a-significant-interaction" class="section level3 hasAnchor" number="6.4.2">
<h3><span class="header-section-number">6.4.2</span> Example with a significant interaction<a href="factorial-experiments.html#example-with-a-significant-interaction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here’s a second example of a <span class="math inline">\(a \times b\)</span> factorial design where the interaction is significant (based off of exercise 15.24 in <span class="citation">Ott and Longnecker (<a href="#ref-ott2015introduction" role="doc-biblioref">2015</a>)</span>): An experiment was performed to compare the effect of soil pH and calcium additives on trunk diameters of orange trees. 36 trees were selected at random from an orange grove. Experimental treatments were arranged in a factorial design, with 4 levels of soil pH and 3 levels of calcium supplement. Treatment combinations were assigned to trees at random in a balanced CRD, with three trees per treatment combination.</p>
<!-- The data are: -->
<!-- \begin{center} -->
<!-- \begin{tabular}{|p{1.1in}|p{0.8in}|p{0.7in}|p{0.8in}|p{0.8in}|} \hline  -->
<!--    & pH = 4.0($i$=1) & pH = 5.0($i$=2) & pH = 6.0($i$=3) & pH = 7.0($i$=4) \\ \hline  -->
<!--    Calcium = 100 ($j$=1) & 5.2, 5.9, 6.3 & 7.1, 7.4, 7.5 & 7.6, 7.2, 7.4 & 7.2, 7.5, 7.2 \\ \hline  -->
<!--    Calcium = 200 ($j$=2) & 7.4, 7.0, 7.6 & 7.4, 7.3, 7.1 & 7.6, 7.5, 7.8 & 7.4, 7.0, 6.9 \\ \hline  -->
<!--    Calcium = 300 ($j$=3) & 6.3, 6.7, 6.1 & 7.3, 7.5, 7.2 & 7.2, 7.3, 7.0 & 6.8, 6.6, 6.4 \\ \hline  -->
<!-- \end{tabular} -->
<!-- \end{center} -->
<p><img src="06-FactorialExperiments_files/figure-html/unnamed-chunk-12-1.png" width="480" style="display: block; margin: auto;" />
Here is a two-factor ANOVA analysis in SAS:</p>
<pre><code>proc glm data=orange; 
  class pH calcium; 
  model diameter = pH calcium pH*calcium;
run;

The GLM Procedure
Dependent Variable: diameter

                                        Sum of
Source                      DF         Squares     Mean Square    F Value    Pr &gt; F
Model                       11      9.18305556      0.83482323      12.32    &lt;.0001
Error                       24      1.62666667      0.06777778
Corrected Total             35     10.80972222

Source                      DF     Type III SS     Mean Square    F Value    Pr &gt; F
pH                           3      4.46083333      1.48694444      21.94    &lt;.0001
calcium                      2      1.46722222      0.73361111      10.82    0.0004
pH*calcium                   6      3.25500000      0.54250000       8.00    &lt;.0001</code></pre>
<p>There is strong evidence of an interaction. Because of the significant interaction, we analyze simple effects. There are multiple ways to analyze simple effects. With more than two levels of each factor, we might consider <span class="math inline">\(F\)</span>-tests of the cell means associated with each level of one of the two factors. For example, we could ask whether the responses differ among the pH levels when calcium = 100. That is, we could test
<span class="math display">\[
H_0: \ \ \mu_{11} =\mu_{21} =\ldots =\mu_{41}.
\]</span>
More generally, we can test for equality across the levels of factor A for each level of factor B:
<span class="math display">\[
H_0: \ \  \mu_{1j} =\mu_{2j} = \ldots =\mu_{aj}.
\]</span>
We can also test for equality across the levels of factor B for each level of factor A:
<span class="math display">\[
H_0: \ \ \mu_{i1} =\mu_{i2} =...=\mu_{ib}.
\]</span>
These tests are implemented in SAS using the SLICE option of the LSMEANS statement. (Note: LSMEANS calculates the Least Squares Means of each treatment level. LSMEANS and MEANS coincide for balanced data, but only LSMEANS has the SLICE option implemented.)</p>
<pre><code>proc glm data=orange; 
  class pH calcium; 
  model diameter = pH calcium pH*calcium;
  lsmeans pH*calcium / slice=pH slice=calcium; 
run;

pH*calcium Effect Sliced by pH for diameter

                     Sum of
pH       DF         Squares     Mean Square    F Value    Pr &gt; F
4         2        3.606667        1.803333      26.61    &lt;.0001
5         2        0.008889        0.004444       0.07    0.9367
6         2        0.326667        0.163333       2.41    0.1112
7         2        0.780000        0.390000       5.75    0.0091

                           Sum of
calcium        DF         Squares     Mean Square    F Value    Pr &gt; F
100             3        5.382500        1.794167      26.47    &lt;.0001
200             3        0.446667        0.148889       2.20    0.1146
300             3        1.886667        0.628889       9.28    0.0003</code></pre>
<p>The first table above tests for differences among the calcium levels within each of the 4 pH levels. This table shows that there are significant differences between calcium levels for pH=4 and pH=7, but not for pH=5 or pH=6. The second table shows tests for differences between the pH levels within each of the three calcium levels. This table shows that there are significant differences between the pH levels when calcium=100 and when calcium=300, but not when calcium=200.</p>
<p>We could then explore each of the significant differences above in more depth by using linear contrast or multiple comparisons. For example, suppose we wanted to compare calcium=200 vs. calcium=100 when pH=4. We define a suitable linear contrast as
<span class="math display">\[
\theta =\mu_{11} -\mu_{12}
\]</span>
and estimate it in SAS by figuring out the appropriate effects parameters:
<span class="math display">\[\begin{eqnarray*}
\theta  &amp; = &amp; \mu_{11} -\mu_{12}  \\
&amp; = &amp; \left( \mu + \alpha_1 + \beta_1 + (\alpha \beta)_{11} \right) -  \left( \mu + \alpha_1 + \beta_2 + (\alpha \beta)_{12} \right) \\
&amp; = &amp; \beta_1 - \beta_2 + (\alpha \beta)_{11} - (\alpha \beta)_{12}
\end{eqnarray*}\]</span></p>
<pre><code>proc glm data=orange; 
  class pH calcium; 
  model diameter = pH calcium pH*calcium;
  estimate &#39;Ca100 vs Ca200 when pH=4&#39; calcium 1 -1 pH*calcium 1 -1; 
run;

                                                Standard
Parameter                       Estimate           Error    t Value    Pr &gt; |t|
Ca100 vs Ca200 when pH=4     -1.53333333      0.21256807      -7.21      &lt;.0001</code></pre>
</div>
</div>
<div id="unreplicated-factorial-designs" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Unreplicated factorial designs<a href="factorial-experiments.html#unreplicated-factorial-designs" class="anchor-section" aria-label="Anchor link to header"></a></h2>
Consider an experiment with 5 levels of factor A, 3 levels of factor B, and a single observation for each treatment combination. This is called an unreplicated design because there is only a single replicate for each treatment combination. Let’s try a df accounting for a model that includes main effects of both factors and an interaction:
<table>
<thead>
<tr>
<th style="text-align:left;">
source
</th>
<th style="text-align:left;">
df
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Factor A
</td>
<td style="text-align:left;">
4
</td>
</tr>
<tr>
<td style="text-align:left;">
Factor B
</td>
<td style="text-align:left;">
2
</td>
</tr>
<tr>
<td style="text-align:left;">
A*B interaction
</td>
<td style="text-align:left;">
8
</td>
</tr>
<tr>
<td style="text-align:left;">
Error
</td>
<td style="text-align:left;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
14
</td>
</tr>
</tbody>
</table>
<p>This model has no df remaining to estimate the experimental error. Consequently, we cannot estimate <span class="math inline">\(MS_{Error}\)</span> and hence we cannot conduct <span class="math inline">\(F\)</span>-tests of the treatment effects.</p>
One option with unreplicated designs is to assume that there is no interaction between the two experimental factors. A model without an interaction is called an additive model. In effects notation, the model is:
<span class="math display">\[
y_{ijk} =\mu +\alpha_i +\beta_j +\varepsilon _{ijk}
\]</span>
With an additive model, we use the df that had been allocated to the interaction to estimate the experimental error instead:<br />

<table>
<thead>
<tr>
<th style="text-align:left;">
source
</th>
<th style="text-align:left;">
df
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Factor A
</td>
<td style="text-align:left;">
4
</td>
</tr>
<tr>
<td style="text-align:left;">
Factor B
</td>
<td style="text-align:left;">
2
</td>
</tr>
<tr>
<td style="text-align:left;">
Error
</td>
<td style="text-align:left;">
8
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
14
</td>
</tr>
</tbody>
</table>
<p>The additive model can be used to test for effects of factors A and B. Obviously, these tests are only trustworthy if the assumption of no interactions is appropriate. In biology, it is usually risky to assume that there are no interactions between experimental factors. Additive models for unreplicated designs are more common in industrial statistics.</p>
<p>John Tukey developed a test for additivity with unreplicated factorial designs, sometimes called Tukey’s single degree-of-freedom test. We will not cover Tukey’s test in ST512, although you may want to read about it on your own if you need to analyze an unreplicated factorial design in your own research.</p>
</div>
<div id="missing-cells" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> Missing cells<a href="factorial-experiments.html#missing-cells" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>An extreme case of unbalanced data occurs when there are no observations for one or more treatment combinations. For example, consider the sheep data used to illustrate an interaction in a 2 <span class="math inline">\(\times\)</span> 2 factorial design, but suppose that there were no data for control sheep bled in the morning. We say that the ``cell’’ representing the treatment combination of control x morning bleeding is missing.</p>
<p>There are three options for missing cells designs:</p>
<ol style="list-style-type: decimal">
<li><p>Remove one of the levels of one experimental factor to eliminate the missing cell. In the example above, we could eliminate the data from the drugged sheep bled in the morning, and just compare the sheep bled in the afternoon to look for a simple effect of drug vs. control. Or, we could eliminate the data from the control sheep bled in the afternoon, and just look for a simple effect of morning vs. afternoon bleeding for sheep given the drug.</p></li>
<li><p>Treat the design as a one-factor layout, where each treatment combination is a separate level of the single experimental factor.</p></li>
<li><p>Use an additive model.</p></li>
</ol>
<!-- ## LSMEANS for unbalanced data -->
<!-- In PROC GLM, the MEANS statement compares raw marginal means for the specified factors.  With balanced data, this is clearly the correct calculation.  With unbalanced data, the sample means compared by MEANS are affected by the lack of balance.  Sometimes it is desirable to "correct" for the lack of balance in the data.  (The usual guidance here is that one wants to correct for lack of balance if the lack of balance is not scientifically meaningful.  If the data come from an observational study and the lack of balance reflects true differences in abundance among treatment groups, then one may want to use the uncorrected marginal means.) To adjust for lack of balance in SAS, it's necessary to use an LSMEANS statement (the LS stands for Least-Squares).  With balanced data, the LSMEANS and MEANS statement produce identical output (although the output is formatted differently).   -->
<!-- To illustrate, let's consider the rat diet example again, but now with the data modified to create an unbalanced data set.   -->
<!-- The modified data are: -->
<!-- \begin{center} -->
<!--    \begin{tabular}{|p{1.1in}|p{1.1in}|p{1.1in}|p{1.1in}|} \hline  -->
<!--        \multicolumn{2}{|p{1in}|}{} & \multicolumn{2}{|p{2.2in}|}{Fat} \\ \hline  -->
<!--        \multicolumn{2}{|p{1in}|}{} & Fresh & Rancid \\ \hline  -->
<!--        Sex & Male & 709 & 592, 538, 476 \\ \hline  -->
<!--        & Female & 657, 594 & 508, 505 \\ \hline  -->
<!--    \end{tabular} -->
<!-- \end{center} -->
<!-- Suppose we are interested in comparing the marginal means for male vs.\ female rats.  Using a MEANS statement yields -->
<!-- ```{} -->
<!-- proc glm data=rat2; -->
<!--   class sex fat; -->
<!--   model food = sex fat sex*fat; -->
<!--   means sex; -->
<!-- run; -->
<!-- Level of           -------------food------------ -->
<!-- sex          N             Mean          Std Dev -->
<!-- female       4       566.000000       73.3712023 -->
<!-- male         4       578.750000       98.9254770  -->
<!-- ``` -->
<!-- Observe that the average for male rats is just the marginal average of the four rats in the study, i.e., 578.75 = (709 + 592 + 538 + 476) / 4.  But, we know that rats fet fresh fat gained more weight than rats fed rancid fat.  Thus, this mean is influenced by the fact that 3 of the 4 male rats in this (modified) study received rancid fat.  Thus, we may not want to compare this mean to the uncorrected mean for females, because of the 4 female rats, two were fed fresh fat and two were fed rancid fat.

An LSMEANS statement corrects for this imbalance:

```{}
proc glm data=rat2;
  class sex fat;
  model food = sex fat sex*fat;
  lsmeans sex;
run;

Least Squares Means

sex        food LSMEAN
female      566.000000
male        622.166667
```

Another way to think about the LSMEANS is that they are the averages of the cell means predicted by the parameters of the effects model.  For example, the LSMEAN for male rats is the average of the predicted cell means for male rats fed fresh fat, and for male rats fed rancid fat:
\begin{eqnarray*}
 \frac{\mu_{11} +\mu_{12} }{2}  & = & \frac{\left(\mu +\alpha _{1} +\beta _{1} +\left(\alpha \beta \right)_{11} \right)+\left(\mu +\alpha _{1} +\beta _{2} +\left(\alpha \beta \right)_{12} \right)}{2} \\ 
 & = & \mu +\alpha _{1} +\frac{\beta _{1} +\beta _{2} }{2} +\frac{\left(\alpha \beta \right)_{11} +\left(\alpha \beta \right)_{12} }{2}.  
\end{eqnarray*}
Plugging in the corresponding parameter estimates produces the LSMEAN.

When the model includes all possible interactions, the LSMEAN is equivalent to averaging the data within each cell first, and then averaging the cell means.  For example, the cell means of the unbalanced rat data are:
\begin{center}
\begin{tabular}{|p{1.1in}|p{1.1in}|p{1.1in}|p{1.1in}|} \hline 
    \multicolumn{2}{|p{1in}|}{} & \multicolumn{2}{|p{2.2in}|}{Fat} \\ \hline 
    \multicolumn{2}{|p{1in}|}{} & Fresh & Rancid \\ \hline 
    Sex & Male & 709 & 535.3 \\ \hline 
    & Female & 625.5 & 506.5 \\ \hline 
\end{tabular}
\end{center}
Then we average the cell means, so that the LSMEAN for males is (709 + 535.3)/2 = 622.2. -->
</div>
<div id="more-than-two-factors" class="section level2 hasAnchor" number="6.7">
<h2><span class="header-section-number">6.7</span> More than two factors<a href="factorial-experiments.html#more-than-two-factors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>All of these ideas can be extended to factorial experiments with more than two factors.</p>
<p>Example (based off of an example in <span class="citation">Rao (<a href="#ref-rao1998statistical" role="doc-biblioref">1998</a>)</span>): An investigator is interested in understanding the effects of water temperature (cold vs. lukewarm vs. warm), light (low vs. high), and water movement (still vs. flowing) on weight gain in fish. She has 24 aquaria to serve as experimental units. Each of the 3 x 2 x 2 = 12 treatment combinations are randomly assigned to 2 of the 24 aqauria, and the average weight gain of the fish in each aquaria is measured. This is a balanced three-way factorial design with a CRD randomization structure.</p>
<!-- The averages of the two replicates for each treatment combination are: -->
<!-- \begin{center} -->
<!--    \begin{tabular}{|p{1.1in}|p{1.1in}|p{1.1in}|p{1.1in}|} \hline  -->
<!--        Light & Temperature & \multicolumn{2}{|p{2.2in}|}{Movement} \\ \hline  -->
<!--        &  & still & flowing \\ \hline  -->
<!--        Low & cold & 4.6 & 3.2 \\ \hline  -->
<!--        & lukewarm & 5.6 & 5.3 \\ \hline  -->
<!--        & warm & 4.8 & 6.0 \\ \hline  -->
<!--        High & cold & 5.6 & 4.1 \\ \hline  -->
<!--        & lukewarm & 6.1 & 5.0 \\ \hline  -->
<!--        & warm & 6.4 & 5.6 \\ \hline  -->
<!--    \end{tabular} -->
<!-- \end{center} -->
<p>As a first attempt to get a handle on these data, let’s make three different interaction plots, one for each water temperature:
<img src="06-FactorialExperiments_files/figure-html/unnamed-chunk-15-1.png" width="768" style="display: block; margin: auto;" />
The interaction plot suggests that for some water temperatures, there is an interaction between light levels and water movement. Thus, the way in which the effect of light depends on water movement depends in turn on temperature. Yikes! This is a three-factor interaction. We need a test to see if this interaction is statistically significant, or if it can be attributed to experimental error.</p>
<p>To develop notation for the three-factor model, we’ll extend our ideas from two factor models. For example, <span class="math inline">\(\mu_{ijk}\)</span> will denote the unknown population mean for the combination of level <span class="math inline">\(i\)</span> of factor A, level <span class="math inline">\(j\)</span> of factor B, and level <span class="math inline">\(k\)</span> of factor C.</p>
<p>With three factors, there are two possible types of interactions:</p>
<ul>
<li><p>First-order interactions}: Interactions between two factors</p></li>
<li><p>Second-order interactions}: Interactions between first-order interactions</p></li>
</ul>
<p>For example, in this experiment the first-order interaction between light level and water movement might describes how the effect of light depends on water movement and vice versa. The second-order interaction describes how this first order interaction may in turn depend on water temperature.</p>
<!-- Here is how the df are partitioned among the various components of the ANOVA model: -->
<!-- \begin{center} -->
<!--    \begin{tabular}{lr}  -->
<!--        Source & df \\ \hline -->
<!--        Temperature & 2 \\ -->
<!--        Light & 1 \\ -->
<!--        Movement & 1 \\ -->
<!--        Temperature $\times$ Light & 2 \\ -->
<!--        Temperature $\times$ Movement & 2 \\ -->
<!--        Light $\times$ Movement & 1 \\ -->
<!--        Temperature $\times$ Light $\times$ Movement & 2 \\ -->
<!--        Error & 12 \\ \hline -->
<!--        Total & 23 -->
<!--    \end{tabular} -->
<!-- \end{center} -->
<p>Note that the rules for determining the df associated with higher-order interactions are the same as for first-order interactions: the df are always equal to the product of the df associated with each constituent factor. (Think of this in terms of regression with indicator variables again.)</p>
<p>In effects notation, we can write the cell means as
<span class="math display">\[
\mu_{ijkl} =\mu +\alpha_i +\beta_j +\gamma _{k} +\left(\alpha \beta \right)_{ij} +\left(\alpha \gamma \right)_{ik} +\left(\beta \gamma \right)_{jk} +\left(\alpha \beta \gamma \right)_{ijk}
\]</span>
Here, the parameters denoted by <span class="math inline">\(\left(\alpha \beta \gamma \right)_{ijk}\)</span> capture the second-order interaction among the three factors.</p>
<pre><code>proc glm data=fishgrowth; 
  class light temp movement; 
  model gain = light|temp|movement;
run;

Example of 3x2x2 factorial from Rao                                                      2
Dependent Variable: gain

                                        Sum of
Source                      DF         Squares     Mean Square    F Value    Pr &gt; F
Model                       11     17.94458333      1.63132576       5.60    0.0030
Error                       12      3.49500000      0.29125000
Corrected Total             23     21.43958333

Source                      DF     Type III SS     Mean Square    F Value    Pr &gt; F
light                        1      2.10041667      2.10041667       7.21    0.0198
temp                         2      7.64333333      3.82166667      13.12    0.0010
light*temp                   2      0.64333333      0.32166667       1.10    0.3629
movement                     1      2.47041667      2.47041667       8.48    0.0130
light*movement               1      1.35375000      1.35375000       4.65    0.0521
temp*movement                2      2.89333333      1.44666667       4.97    0.0268
light*temp*movement          2      0.84000000      0.42000000       1.44    0.2746</code></pre>
<p>The analysis strategy with a three-way factorial design is similar to the analysis strategy with a two-way factorial design:</p>
<ul>
<li><p>Test for the significance of the second-order interaction.</p></li>
<li><p>If the second-order interaction is significant, either unpack the factorial treatment structure and treat the design a one-factor ANOVA, or ``divide and conquer’’ by analyzing the effects of two factors at each level of the third factor.</p></li>
<li><p>If the second-order interaction is not significant, you may remove the second-order interaction and re-fit the model, although this is not necessary. Test for the significance of the first-order interactions. If any of the first-order interactions are significant, analyze simple effects. If none of the first-order interactions are significant, analyze main effects.</p></li>
</ul>
<p>In the example above, the second-order interaction is not statistically significant (<span class="math inline">\(F_{2, 12} = 1.44\)</span>, <span class="math inline">\(p = 0.27\)</span>). The only statistically significant first-order interaction is the interaction between water temperature and movement (<span class="math inline">\(F_{2,12} = 4.97\)</span>, <span class="math inline">\(p = 0.027\)</span>). Neither of the first-order interactions involving light are statistically significant (although the light-by-movement interaction is on the border of statistical significance, <span class="math inline">\(F_{1,12} = 4.65\)</span>, <span class="math inline">\(p = 0.052\)</span>). The main effect of light is statistically significant (<span class="math inline">\(F_{1,12} = 7.21\)</span>, <span class="math inline">\(p = 0.020\)</span>). We could then proceed by quantifying the main effect of light with a linear combination, and quantifying the simple effects of movement at different water temperatures.</p>
<p>Main effect of light:
<span class="math display">\[\theta _{light} =\bar{\mu}_{1++} -\bar{\mu}_{2++} \]</span>
Simple effect of movement when temperature = cold:
<span class="math display">\[\theta _{m/C} =\bar{\mu}_{+11} -\bar{\mu}_{+12} \]</span>
Simple effect of movement when temperature = lukewarm:
<span class="math display">\[\theta _{m/L} =\bar{\mu}_{+21} -\bar{\mu}_{+22} \]</span>
Simple effect of movement when temperature = warm:
<span class="math display">\[\theta _{m/W} =\bar{\mu}_{+31} -\bar{\mu}_{+32} \]</span></p>
<!-- Expressing these linear combinations in terms of effects parameters can be incredibly tedious.  Here's a SAS trick: Create a new CLASS variable that distinguishes each of the unique treatment combinations.  For instance, in this example, create a variable called 'trt' that takes the following values: -->
<!-- \begin{center} -->
<!--    \begin{tabular}{|p{1.1in}|p{1.1in}|p{1.1in}|p{1.1in}|} \hline  -->
<!--        Light  & Temperature  & \multicolumn{2}{|p{2.2in}|}{Movement } \\ \hline  -->
<!--        &  & still  & flowing  \\ \hline  -->
<!--        Low  & cold  & A  & B  \\ \hline  -->
<!--        & lukewarm  & C  & D  \\ \hline  -->
<!--        & warm  & E  & F  \\ \hline  -->
<!--        High  & cold  & G  & H  \\ \hline  -->
<!--        & lukewarm  & I  & J  \\ \hline  -->
<!--        & warm  & K  & L  \\ \hline  -->
<!--    \end{tabular} -->
<!-- \end{center} -->
<!-- Now, fit a one-factor ANOVA model using the variable 'trt' as the classification variable, and then write ESTIMATE statements using the 'trt' variable.  That is,  -->
<!-- ```{} -->
<!-- proc glm data=fishgrowth;  -->
<!--   class trt light temp movement;  -->
<!--   model gain = trt; -->
<!--   estimate 'main effect of light, high-low' trt -1 -1 -1 -1 -1 -1 1 1 1 1 1 1 / divisor=6; -->
<!--   estimate 'effect of movement when temp=cold' trt 1 -1 0 0 0 0 1 -1 0 0 0 0 / divisor=2; -->
<!--   estimate 'effect of movement when temp=luke' trt 0 0 1 -1 0 0 0 0 1 -1 0 0 / divisor=2; -->
<!--   estimate 'effect of movement when temp=warm' trt 0 0 0 0 1 -1 0 0 0 0 1 -1 / divisor=2; -->
<!-- run; -->
<!-- Example of 3x2x2 factorial from Rao                                                      -->
<!--                                                          Standard -->
<!-- Parameter                                Estimate           Error    t Value    Pr > |t| -->
<!-- main effect of light, high-low         0.59166667      0.22032173       2.69      0.0198 -->
<!-- effect of movement when temp=cold      1.47500000      0.38160844       3.87      0.0022 -->
<!-- effect of movement when temp=luke      0.67500000      0.38160844       1.77      0.1023 -->
<!-- effect of movement when temp=warm     -0.22500000      0.38160844      -0.59      0.5664 -->
<!-- ``` -->
<!-- Interpretation: Fish gain 0.59 units more mass at high temperatures (s.e. = 0.22, $t_{12}=2.69$, $p=0.020$).  At cold temperatures, fish gain 1.48 units more mass in still water (s.e.=0.38, $t_{12}=3.87$, $p=0.002$).  There is at best weak evidence that water movement affects weight gain in lukewarm water (estimated effect = 0.68 units, s.e. = 0.38, $t_{12}=1.77$, $p=0.102$), and there is no evidence that water movement affects weight gain in warm water (estimated effect = -0.23 units, s.e. = 0.38, $t_{12}=-0.59$, $p=0.57$). -->
<p>A final note: unreplicated three-way factorial designs are not uncommon in the life sciences. To analyze these designs, one typically assumes that there is no second-order interaction, and uses the df that would have been absorbed by the second-order interaction as the df for error. Some will argue that higher-order interactions are rare in nature, and thus assuming that they do not occur is justified. Whether you agree with this or view it as a just-so rationalization is up to you.</p>
<!-- ## Advanced topic: equivalence between two-factor ANOVA and regression with indicators} -->
<!-- As we have emphasized before, ANOVA is mathematically equivalent to regression with indicator variables.  Thinking about ANOVA in this way helps illuminate the set-to-zero constraints on effects-model parameters.  Consider again the pine oleoresin data, which is an example of a 4x2 factorial design.  Suppose we were to analyze these data using regression with indicator variables.  For the shape treatment, set 'rectangle' as the reference level, and define the following three indicator variables: -->
<!-- \[x_{1} =\left\{\begin{array}{cc} {1} & {{\rm circle}} \\ {0} & {{\rm otherwise}} \end{array}\right. , x_{2} =\left\{\begin{array}{cc} {1} & {{\rm diagonal}} \\ {0} & {{\rm otherwise}} \end{array}\right. , x_{3} =\left\{\begin{array}{cc} {1} & {{\rm check}} \\ {0} & {{\rm otherwise}} \end{array}\right. \]  -->
<!-- For the acid treatment, set 'acid' as the control level, and define the following indicator for the control: -->
<!-- \[x_{4} =\left\{\begin{array}{cc} {1} & {{\rm control}} \\ {0} & {{\rm acid\; added}} \end{array}\right. \]  -->
<!-- Now consider a regression model that includes each of the four indicators above as well as interactions between the indicators for shape and the indicator for the acid treatment: -->
<!-- \[y=\beta _{0} +\beta _{1} x_{1} +\beta _{2} x_{2} +\beta _{3} x_{3} +\beta _{4} x_{4} +\beta _{5} x_{1} x_{4} +\beta _{6} x_{2} x_{4} +\beta _{7} x_{3} x_{4} +\varepsilon \]  -->
<!-- Each of the partial regression coefficients in the model above can be matched to one of the constrained parameters of the effects model.  Below is the two-factor effects model, where we use ? instead of ? to avoid confusion with regression coefficients: -->
<!-- \[y_{ijk} =\mu +\alpha_i +\gamma _{j} +\left(\alpha \gamma \right)_{ij} +\varepsilon _{ijk} \]  -->
<!-- The correspondence between the partial regression coefficients in the regression model and the constrained effects parameters in the ANOVA model is -->
<!-- \begin{center} -->
<!-- \begin{tabular}{|p{2.1in}|p{1.3in}|} \hline  -->
<!--    Regression parameter & ANOVA parameter \\ \hline  -->
<!--    $\beta _{0} $ & $\mu $ \\ \hline  -->
<!--    $\beta _{1} $ & $\alpha _{1} $ \\ \hline  -->
<!--    $\beta _{2} $ & $\alpha _{2} $ \\ \hline  -->
<!--    $\beta _{3} $ & $\alpha _{3} $ \\ \hline  -->
<!--    $\beta _{4} $ & $\gamma _{1} $ \\ \hline  -->
<!--    $\beta _{5} $ & $\left(\alpha \gamma \right)_{11} $ \\ \hline  -->
<!--    $\beta _{6} $ & $\left(\alpha \gamma \right)_{12} $ \\ \hline  -->
<!--    $\beta _{7} $ & $\left(\alpha \gamma \right)_{13} $ \\ \hline  -->
<!-- \end{tabular} -->
<!-- \end{center} -->
<!-- Similarly, the hypotheses tested by the $F$-tests of the main effects and interaction can be expressed either in terms of regression parameters or in terms of ANOVA parameters.  In the table below, keep in mind that, under the set-to-zero constraint, the ANOVA parameters $\alpha _{4} $, $\gamma _{2} $, $\left(\alpha \gamma \right)_{14} $, $\left(\alpha \gamma \right)_{21} $,$\left(\alpha \gamma \right)_{22} $,$\left(\alpha \gamma \right)_{23} $, and $\left(\alpha \gamma \right)_{24} $ are all constrained to be 0. -->
<!-- \begin{center} -->
<!-- \begin{tabular}{|p{1.4in}|p{1.4in}|p{1.6in}|} \hline  -->
<!--    & Regression $H_0$ & ANOVA $H_0$, \newline with set-to-zero constraint \\ \hline  -->
<!--    Main effect of shape & $H_{0} :\beta _{1} =\beta _{2} =\beta _{3} =0$ & $H_{0} :\alpha _{1} =\alpha _{2} =\alpha _{3} =0$ \\ \hline  -->
<!--    Main effect of acid & $H_{0} :\beta _{4} =0$ & $H_{0} :\gamma _{1} =0$ \\ \hline  -->
<!--    Shape x acid interaction & $H_{0} :\beta _{5} =\beta _{6} =\beta _{7} =0$ & $H_{0} :\left(\alpha \gamma \right)_{11} =\left(\alpha \gamma \right)_{12} =\left(\alpha \gamma \right)_{13} =0$ \\ \hline  -->
<!-- \end{tabular} -->
<!-- \end{center} -->
<!-- Thus, we see that the $F$-tests in ANOVA are fundamentally the same as the $F$-tests that we discussed in the context of multiple regression. -->

</div>
</div>
<h3>Bibliography<a href="bibliography.html#bibliography" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-low1985experimental" class="csl-entry">
Low, C. K., and A. R. Bin Mohd. Ali. 1985. <span>“Experimental Tapping of Pine Oleoresin.”</span> <em>The Malaysian Forester</em> 48: 248–53.
</div>
<div id="ref-ott2015introduction" class="csl-entry">
Ott, R Lyman, and Micheal T Longnecker. 2015. <em>An Introduction to Statistical Methods and Data Analysis</em>. Cengage Learning.
</div>
<div id="ref-rao1998statistical" class="csl-entry">
Rao, Pejaver Vishwamber. 1998. <em>Statistical Research Methods in the Life Sciences</em>. Duxbury Press.
</div>
<div id="ref-sokal1995biometry" class="csl-entry">
Sokal, Robert R, and F James Rohlf. 1995. <em>Biometry</em>. 3rd ed. New York: W.H. Freeman.
</div>
<div id="ref-steel1997principles" class="csl-entry">
Steel, R. G. D., J. H. Torrie, and D. A. Dickey. 1997. <span>“Principles and Procedures of Statistics: A Biometrical Approach.”</span>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="one-factor-anova.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ancova.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
