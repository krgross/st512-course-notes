<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Random effects | Statistical analysis of designed experiments: yesterday, today, and tomorrow</title>
  <meta name="description" content="An online text in statistical analysis using the linear model" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Random effects | Statistical analysis of designed experiments: yesterday, today, and tomorrow" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="An online text in statistical analysis using the linear model" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Random effects | Statistical analysis of designed experiments: yesterday, today, and tomorrow" />
  
  <meta name="twitter:description" content="An online text in statistical analysis using the linear model" />
  

<meta name="author" content="Kevin Gross" />


<meta name="date" content="2025-08-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ancova.html"/>
<link rel="next" href="blocked-designs.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#philosophy"><i class="fa fa-check"></i>Philosophy</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#scope-and-coverage"><i class="fa fa-check"></i>Scope and coverage</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#mathematical-level"><i class="fa fa-check"></i>Mathematical level</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#computing"><i class="fa fa-check"></i>Computing</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#llms-in-statistical-analysis"><i class="fa fa-check"></i>LLMs in statistical analysis</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#format-of-the-notes"><i class="fa fa-check"></i>Format of the notes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#a-word-on-the-title"><i class="fa fa-check"></i>A word on the title</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments-and-license"><i class="fa fa-check"></i>Acknowledgments and license</a></li>
</ul></li>
<li class="part"><span><b>Part I: Regression modeling</b></span></li>
<li class="chapter" data-level="1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>1</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-basics-of-slr"><i class="fa fa-check"></i><b>1.1</b> The basics of SLR</a></li>
<li class="chapter" data-level="1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#least-squares-estimation"><i class="fa fa-check"></i><b>1.2</b> Least-squares estimation</a></li>
<li class="chapter" data-level="1.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#inference-for-the-slope"><i class="fa fa-check"></i><b>1.3</b> Inference for the slope</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#standard-errors"><i class="fa fa-check"></i><b>1.3.1</b> Standard errors</a></li>
<li class="chapter" data-level="1.3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#confidence-intervals"><i class="fa fa-check"></i><b>1.3.2</b> Confidence intervals</a></li>
<li class="chapter" data-level="1.3.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#statistical-hypothesis-tests"><i class="fa fa-check"></i><b>1.3.3</b> Statistical hypothesis tests</a></li>
<li class="chapter" data-level="1.3.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#inference-for-the-intercept"><i class="fa fa-check"></i><b>1.3.4</b> Inference for the intercept</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#sums-of-squares-decomposition-and-r2"><i class="fa fa-check"></i><b>1.4</b> Sums of squares decomposition and <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="1.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#diagnostic-plots"><i class="fa fa-check"></i><b>1.5</b> Diagnostic plots</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#residuals-vs.-fitted-values"><i class="fa fa-check"></i><b>1.5.1</b> Residuals vs. fitted values</a></li>
<li class="chapter" data-level="1.5.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#residuals-vs.-predictors"><i class="fa fa-check"></i><b>1.5.2</b> Residuals vs. predictor(s)</a></li>
<li class="chapter" data-level="1.5.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#residuals-vs.-other-variables"><i class="fa fa-check"></i><b>1.5.3</b> Residuals vs. other variables</a></li>
<li class="chapter" data-level="1.5.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#normal-probability-plot"><i class="fa fa-check"></i><b>1.5.4</b> Normal probability plot</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#consequences-of-violating-model-assumptions-and-possible-fixes"><i class="fa fa-check"></i><b>1.6</b> Consequences of violating model assumptions, and possible fixes</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#linearity"><i class="fa fa-check"></i><b>1.6.1</b> Linearity</a></li>
<li class="chapter" data-level="1.6.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#independence"><i class="fa fa-check"></i><b>1.6.2</b> Independence</a></li>
<li class="chapter" data-level="1.6.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#constant-variance"><i class="fa fa-check"></i><b>1.6.3</b> Constant variance</a></li>
<li class="chapter" data-level="1.6.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#normality"><i class="fa fa-check"></i><b>1.6.4</b> Normality</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#prediction-with-regression-models"><i class="fa fa-check"></i><b>1.7</b> Prediction with regression models</a></li>
<li class="chapter" data-level="1.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#regression-design"><i class="fa fa-check"></i><b>1.8</b> Regression design</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#choice-of-predictor-values"><i class="fa fa-check"></i><b>1.8.1</b> Choice of predictor values</a></li>
<li class="chapter" data-level="1.8.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#powerSLR"><i class="fa fa-check"></i><b>1.8.2</b> Power</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#centering-the-predictor"><i class="fa fa-check"></i><b>1.9</b> <span class="math inline">\(^\star\)</span>Centering the predictor</a></li>
<li class="chapter" data-level="" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#appendix-a-fitting-the-slr-model-in-r"><i class="fa fa-check"></i>Appendix A: Fitting the SLR model in R</a></li>
<li class="chapter" data-level="" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#appendix-b-regression-models-in-sas-proc-reg"><i class="fa fa-check"></i>Appendix B: Regression models in SAS PROC REG</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#multiple-regression-basics"><i class="fa fa-check"></i><b>2.1</b> Multiple regression basics</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="multiple-regression.html"><a href="multiple-regression.html#ideas-that-carry-over-from-slr-to-multiple-regression"><i class="fa fa-check"></i><b>2.1.1</b> Ideas that carry over from SLR to multiple regression</a></li>
<li class="chapter" data-level="2.1.2" data-path="multiple-regression.html"><a href="multiple-regression.html#interpreting-partial-regression-coefficients."><i class="fa fa-check"></i><b>2.1.2</b> Interpreting partial regression coefficients.</a></li>
<li class="chapter" data-level="2.1.3" data-path="multiple-regression.html"><a href="multiple-regression.html#visualizing-a-multiple-regression-model"><i class="fa fa-check"></i><b>2.1.3</b> Visualizing a multiple regression model</a></li>
<li class="chapter" data-level="2.1.4" data-path="multiple-regression.html"><a href="multiple-regression.html#statistical-inference-for-partial-regression-coefficients"><i class="fa fa-check"></i><b>2.1.4</b> Statistical inference for partial regression coefficients</a></li>
<li class="chapter" data-level="2.1.5" data-path="multiple-regression.html"><a href="multiple-regression.html#prediction"><i class="fa fa-check"></i><b>2.1.5</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#F-test"><i class="fa fa-check"></i><b>2.2</b> <span class="math inline">\(F\)</span>-tests for several regression coefficients</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#basic-machinery"><i class="fa fa-check"></i><b>2.2.1</b> Basic machinery</a></li>
<li class="chapter" data-level="2.2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#model-utility-test"><i class="fa fa-check"></i><b>2.2.2</b> Model utility test</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="multiple-regression.html"><a href="multiple-regression.html#categorical-predictors"><i class="fa fa-check"></i><b>2.3</b> Categorical predictors</a></li>
<li class="chapter" data-level="2.4" data-path="multiple-regression.html"><a href="multiple-regression.html#regression-interactions"><i class="fa fa-check"></i><b>2.4</b> Interactions between predictors</a></li>
<li class="chapter" data-level="2.5" data-path="multiple-regression.html"><a href="multiple-regression.html#collinearity"><i class="fa fa-check"></i><b>2.5</b> (Multi-)Collinearity</a></li>
<li class="chapter" data-level="2.6" data-path="multiple-regression.html"><a href="multiple-regression.html#variable-selection-choosing-the-best-model"><i class="fa fa-check"></i><b>2.6</b> Variable selection: Choosing the best model</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="multiple-regression.html"><a href="multiple-regression.html#model-selection-and-inference"><i class="fa fa-check"></i><b>2.6.1</b> Model selection and inference</a></li>
<li class="chapter" data-level="2.6.2" data-path="multiple-regression.html"><a href="multiple-regression.html#ranking-methods"><i class="fa fa-check"></i><b>2.6.2</b> Ranking methods</a></li>
<li class="chapter" data-level="2.6.3" data-path="multiple-regression.html"><a href="multiple-regression.html#sequential-methods"><i class="fa fa-check"></i><b>2.6.3</b> Sequential methods</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="multiple-regression.html"><a href="multiple-regression.html#leverage-influential-points-and-standardized-residuals"><i class="fa fa-check"></i><b>2.7</b> Leverage, influential points, and standardized residuals</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="multiple-regression.html"><a href="multiple-regression.html#leverage"><i class="fa fa-check"></i><b>2.7.1</b> Leverage</a></li>
<li class="chapter" data-level="2.7.2" data-path="multiple-regression.html"><a href="multiple-regression.html#standardized-residuals"><i class="fa fa-check"></i><b>2.7.2</b> Standardized residuals</a></li>
<li class="chapter" data-level="2.7.3" data-path="multiple-regression.html"><a href="multiple-regression.html#cooks-distance"><i class="fa fa-check"></i><b>2.7.3</b> Cook’s distance</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="multiple-regression.html"><a href="multiple-regression.html#appendix-regression-as-a-linear-algebra-problem"><i class="fa fa-check"></i>Appendix: Regression as a linear algebra problem</a>
<ul>
<li class="chapter" data-level="" data-path="multiple-regression.html"><a href="multiple-regression.html#singular-or-pathological-design-matrices"><i class="fa fa-check"></i>Singular, or pathological, design matrices</a></li>
<li class="chapter" data-level="" data-path="multiple-regression.html"><a href="multiple-regression.html#additional-results"><i class="fa fa-check"></i>Additional results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="non-linear-regression-models.html"><a href="non-linear-regression-models.html"><i class="fa fa-check"></i><b>3</b> Non-linear regression models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="non-linear-regression-models.html"><a href="non-linear-regression-models.html#polynomial-regression"><i class="fa fa-check"></i><b>3.1</b> Polynomial regression</a></li>
<li class="chapter" data-level="3.2" data-path="non-linear-regression-models.html"><a href="non-linear-regression-models.html#nls"><i class="fa fa-check"></i><b>3.2</b> Non-linear least squares</a></li>
<li class="chapter" data-level="3.3" data-path="non-linear-regression-models.html"><a href="non-linear-regression-models.html#starsmoothing-methods"><i class="fa fa-check"></i><b>3.3</b> <em><span class="math inline">\(^\star\)</span>Smoothing methods</em></a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="non-linear-regression-models.html"><a href="non-linear-regression-models.html#loess-smoothers"><i class="fa fa-check"></i><b>3.3.1</b> Loess smoothers</a></li>
<li class="chapter" data-level="3.3.2" data-path="non-linear-regression-models.html"><a href="non-linear-regression-models.html#splines"><i class="fa fa-check"></i><b>3.3.2</b> Splines</a></li>
<li class="chapter" data-level="3.3.3" data-path="non-linear-regression-models.html"><a href="non-linear-regression-models.html#generalized-additive-models-gams"><i class="fa fa-check"></i><b>3.3.3</b> Generalized additive models (GAMs)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>4</b> Generalized linear models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson-regression"><i class="fa fa-check"></i><b>4.1</b> Poisson regression</a></li>
<li class="chapter" data-level="4.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#binary-responses"><i class="fa fa-check"></i><b>4.2</b> Binary responses</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#individual-binary-responses-tb-in-boar"><i class="fa fa-check"></i><b>4.2.1</b> Individual binary responses: TB in boar</a></li>
<li class="chapter" data-level="4.2.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#grouped-binary-data-industrial-melanism"><i class="fa fa-check"></i><b>4.2.2</b> Grouped binary data: Industrial melanism</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#implementation-in-sas"><i class="fa fa-check"></i><b>4.3</b> Implementation in SAS</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#complete-separation"><i class="fa fa-check"></i><b>4.3.1</b> Complete separation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part II: Designed experiments</b></span></li>
<li class="chapter" data-level="5" data-path="one-factor-anova.html"><a href="one-factor-anova.html"><i class="fa fa-check"></i><b>5</b> One-factor ANOVA</a>
<ul>
<li class="chapter" data-level="5.1" data-path="one-factor-anova.html"><a href="one-factor-anova.html#grouped-data-and-the-design-of-experiments-doe-an-overview"><i class="fa fa-check"></i><b>5.1</b> Grouped data and the design of experiments (DoE): an overview</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="one-factor-anova.html"><a href="one-factor-anova.html#a-vocabulary-for-describing-designed-experiments"><i class="fa fa-check"></i><b>5.1.1</b> A vocabulary for describing designed experiments</a></li>
<li class="chapter" data-level="5.1.2" data-path="one-factor-anova.html"><a href="one-factor-anova.html#roadmap"><i class="fa fa-check"></i><b>5.1.2</b> Roadmap</a></li>
<li class="chapter" data-level="5.1.3" data-path="one-factor-anova.html"><a href="one-factor-anova.html#the-simplest-experiment"><i class="fa fa-check"></i><b>5.1.3</b> The simplest experiment</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="one-factor-anova.html"><a href="one-factor-anova.html#one-factor-anova-the-basics"><i class="fa fa-check"></i><b>5.2</b> One-factor ANOVA: The basics</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="one-factor-anova.html"><a href="one-factor-anova.html#f-test-to-compare-means"><i class="fa fa-check"></i><b>5.2.1</b> <span class="math inline">\(F\)</span>-test to compare means</a></li>
<li class="chapter" data-level="5.2.2" data-path="one-factor-anova.html"><a href="one-factor-anova.html#connections-between-one-factor-anova-and-other-statistical-procedures"><i class="fa fa-check"></i><b>5.2.2</b> Connections between one-factor ANOVA and other statistical procedures</a></li>
<li class="chapter" data-level="5.2.3" data-path="one-factor-anova.html"><a href="one-factor-anova.html#assumptions-in-anova"><i class="fa fa-check"></i><b>5.2.3</b> Assumptions in ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="one-factor-anova.html"><a href="one-factor-anova.html#contrasts"><i class="fa fa-check"></i><b>5.3</b> Linear contrasts of group means</a></li>
<li class="chapter" data-level="5.4" data-path="one-factor-anova.html"><a href="one-factor-anova.html#using-sas-the-effects-parameterization-of-the-one-factor-anova"><i class="fa fa-check"></i><b>5.4</b> Using SAS: The effects parameterization of the one-factor ANOVA</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="one-factor-anova.html"><a href="one-factor-anova.html#effects-model-parameterization-of-the-one-factor-anova-model"><i class="fa fa-check"></i><b>5.4.1</b> Effects-model parameterization of the one-factor ANOVA model</a></li>
<li class="chapter" data-level="5.4.2" data-path="one-factor-anova.html"><a href="one-factor-anova.html#sas-implementation-of-the-one-factor-anova-model-in-proc-glm"><i class="fa fa-check"></i><b>5.4.2</b> SAS implementation of the one-factor ANOVA model in PROC GLM</a></li>
<li class="chapter" data-level="5.4.3" data-path="one-factor-anova.html"><a href="one-factor-anova.html#using-the-estimate-and-contrast-statements-for-linear-contrasts-in-proc-glm"><i class="fa fa-check"></i><b>5.4.3</b> Using the ESTIMATE and CONTRAST statements for linear contrasts in PROC GLM</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="one-factor-anova.html"><a href="one-factor-anova.html#linear-contrasts-revisited-testing-multiple-simultaneous-contrasts"><i class="fa fa-check"></i><b>5.5</b> Linear contrasts revisited: Testing multiple simultaneous contrasts</a></li>
<li class="chapter" data-level="5.6" data-path="one-factor-anova.html"><a href="one-factor-anova.html#multiple-comparisons"><i class="fa fa-check"></i><b>5.6</b> Multiple comparisons</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="one-factor-anova.html"><a href="one-factor-anova.html#multiple-testing-in-general"><i class="fa fa-check"></i><b>5.6.1</b> Multiple testing in general</a></li>
<li class="chapter" data-level="5.6.2" data-path="one-factor-anova.html"><a href="one-factor-anova.html#bonferroni-and-bonferroni-like-procedures"><i class="fa fa-check"></i><b>5.6.2</b> Bonferroni and Bonferroni-like procedures</a></li>
<li class="chapter" data-level="5.6.3" data-path="one-factor-anova.html"><a href="one-factor-anova.html#multiple-comparisons-in-anova"><i class="fa fa-check"></i><b>5.6.3</b> Multiple comparisons in ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="one-factor-anova.html"><a href="one-factor-anova.html#general-strategy-for-analyzing-data-from-a-crd-with-a-one-factor-treatment-structure"><i class="fa fa-check"></i><b>5.7</b> General strategy for analyzing data from a CRD with a one-factor treatment structure</a></li>
<li class="chapter" data-level="5.8" data-path="one-factor-anova.html"><a href="one-factor-anova.html#starpower-and-sample-size-determination-in-anova"><i class="fa fa-check"></i><b>5.8</b> <span class="math inline">\(^\star\)</span>Power and sample-size determination in ANOVA</a></li>
<li class="chapter" data-level="5.9" data-path="one-factor-anova.html"><a href="one-factor-anova.html#starorthogonal-contrasts"><i class="fa fa-check"></i><b>5.9</b> <span class="math inline">\(^\star\)</span>Orthogonal contrasts</a></li>
<li class="chapter" data-level="5.10" data-path="one-factor-anova.html"><a href="one-factor-anova.html#starpolynomial-trends"><i class="fa fa-check"></i><b>5.10</b> <span class="math inline">\(^\star\)</span>Polynomial trends</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="factorial-experiments.html"><a href="factorial-experiments.html"><i class="fa fa-check"></i><b>6</b> Factorial experiments</a>
<ul>
<li class="chapter" data-level="6.1" data-path="factorial-experiments.html"><a href="factorial-experiments.html#combining-factors-crossed-vs.-nested-designs"><i class="fa fa-check"></i><b>6.1</b> Combining factors: Crossed vs. nested designs</a></li>
<li class="chapter" data-level="6.2" data-path="factorial-experiments.html"><a href="factorial-experiments.html#times-2-factorial-design"><i class="fa fa-check"></i><b>6.2</b> 2 <span class="math inline">\(\times\)</span> 2 factorial design</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="factorial-experiments.html"><a href="factorial-experiments.html#example-weight-gain-in-rats"><i class="fa fa-check"></i><b>6.2.1</b> Example: Weight gain in rats</a></li>
<li class="chapter" data-level="6.2.2" data-path="factorial-experiments.html"><a href="factorial-experiments.html#two-factor-anova-hypothesis-tests"><i class="fa fa-check"></i><b>6.2.2</b> Two-factor ANOVA hypothesis tests</a></li>
<li class="chapter" data-level="6.2.3" data-path="factorial-experiments.html"><a href="factorial-experiments.html#analysis-using-proc-glm-in-sas"><i class="fa fa-check"></i><b>6.2.3</b> Analysis using PROC GLM in SAS</a></li>
<li class="chapter" data-level="6.2.4" data-path="factorial-experiments.html"><a href="factorial-experiments.html#effects-notation-for-the-two-factor-anova"><i class="fa fa-check"></i><b>6.2.4</b> Effects notation for the two-factor ANOVA</a></li>
<li class="chapter" data-level="6.2.5" data-path="factorial-experiments.html"><a href="factorial-experiments.html#a-second-example-when-the-interaction-is-significant"><i class="fa fa-check"></i><b>6.2.5</b> A second example when the interaction is significant</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="factorial-experiments.html"><a href="factorial-experiments.html#a-times-b-factorial-designs"><i class="fa fa-check"></i><b>6.3</b> <span class="math inline">\(a \times b\)</span> factorial designs</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="factorial-experiments.html"><a href="factorial-experiments.html#example-without-a-significant-interaction"><i class="fa fa-check"></i><b>6.3.1</b> Example without a significant interaction</a></li>
<li class="chapter" data-level="6.3.2" data-path="factorial-experiments.html"><a href="factorial-experiments.html#example-with-a-significant-interaction"><i class="fa fa-check"></i><b>6.3.2</b> Example with a significant interaction</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="factorial-experiments.html"><a href="factorial-experiments.html#unreplicated-factorial-designs"><i class="fa fa-check"></i><b>6.4</b> Unreplicated factorial designs</a></li>
<li class="chapter" data-level="6.5" data-path="factorial-experiments.html"><a href="factorial-experiments.html#missing-cells"><i class="fa fa-check"></i><b>6.5</b> Missing cells</a></li>
<li class="chapter" data-level="6.6" data-path="factorial-experiments.html"><a href="factorial-experiments.html#more-than-two-factors"><i class="fa fa-check"></i><b>6.6</b> More than two factors</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ancova.html"><a href="ancova.html"><i class="fa fa-check"></i><b>7</b> ANCOVA</a></li>
<li class="chapter" data-level="8" data-path="random-effects.html"><a href="random-effects.html"><i class="fa fa-check"></i><b>8</b> Random effects</a>
<ul>
<li class="chapter" data-level="8.1" data-path="random-effects.html"><a href="random-effects.html#fixed-vs.-random-effects-the-big-picture"><i class="fa fa-check"></i><b>8.1</b> Fixed vs. random effects: the big picture</a></li>
<li class="chapter" data-level="8.2" data-path="random-effects.html"><a href="random-effects.html#random-effects-models"><i class="fa fa-check"></i><b>8.2</b> Random-effects models</a></li>
<li class="chapter" data-level="8.3" data-path="random-effects.html"><a href="random-effects.html#subsampling"><i class="fa fa-check"></i><b>8.3</b> Subsampling</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="random-effects.html"><a href="random-effects.html#equal-subsamples-per-eu"><i class="fa fa-check"></i><b>8.3.1</b> Equal subsamples per EU</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="random-effects.html"><a href="random-effects.html#starmathematical-foundations"><i class="fa fa-check"></i><b>8.4</b> <span class="math inline">\(^\star\)</span>Mathematical foundations</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="random-effects.html"><a href="random-effects.html#probability-refresher"><i class="fa fa-check"></i><b>8.4.1</b> Probability refresher</a></li>
<li class="chapter" data-level="8.4.2" data-path="random-effects.html"><a href="random-effects.html#application-to-models-with-random-effects"><i class="fa fa-check"></i><b>8.4.2</b> Application to models with random effects</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="blocked-designs.html"><a href="blocked-designs.html"><i class="fa fa-check"></i><b>9</b> Blocked designs</a>
<ul>
<li class="chapter" data-level="9.1" data-path="blocked-designs.html"><a href="blocked-designs.html#randomized-complete-block-designs"><i class="fa fa-check"></i><b>9.1</b> Randomized complete block designs</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="blocked-designs.html"><a href="blocked-designs.html#should-a-blocking-factor-be-a-fixed-or-random-effect"><i class="fa fa-check"></i><b>9.1.1</b> *Should a blocking factor be a fixed or random effect?</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="blocked-designs.html"><a href="blocked-designs.html#latin-squares-designs"><i class="fa fa-check"></i><b>9.2</b> Latin-squares designs</a></li>
<li class="chapter" data-level="9.3" data-path="blocked-designs.html"><a href="blocked-designs.html#split-plot-designs"><i class="fa fa-check"></i><b>9.3</b> Split-plot designs</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="blocked-designs.html"><a href="blocked-designs.html#denominator-degrees-of-freedom-in-split-plot-designs-and-the-satterthwaite-approximation"><i class="fa fa-check"></i><b>9.3.1</b> Denominator degrees of freedom in split-plot designs and the Satterthwaite approximation</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="blocked-designs.html"><a href="blocked-designs.html#repeated-measures"><i class="fa fa-check"></i><b>9.4</b> Repeated measures</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical analysis of designed experiments: yesterday, today, and tomorrow</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="random-effects" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Random effects<a href="random-effects.html#random-effects" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="fixed-vs.-random-effects-the-big-picture" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Fixed vs. random effects: the big picture<a href="random-effects.html#fixed-vs.-random-effects-the-big-picture" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In statistics, we regard each data point as a single observation from an underlying distribution, or population, of possible values. A statistical model characterizes the structure of the underlying distributions from which the observed data are drawn. Loosely, we think of a statistical model as combining two distinct components. One component of the statistical model describes the mean<a href="#fn26" class="footnote-ref" id="fnref26"><sup>26</sup></a> of the underlying distributions, and how the mean depends on (for example) predictors in a regression model, or on the particular levels of an experimental factor in an ANOVA. The other component of the model describes how the underlying distributions vary around their respective mean, both with respect to the underlying distribution for a single data point, and the mutual relationship among the underlying distributions for several data points (i.e., their correlations). In colloquial terms, this distinction is often referred to as the “signal” and the “noise” in a statistical model. To this point, we have spent nearly all of our effort thinking about how the mean, or “signal”, in a statistical model depends on predictors, treatment groups, etc. We have not yet given much thought to the noise component of the model, beyond asserting that the residual errors were iid draws from a normal distribution with a constant variance. The concept of “random effects” will give us our first tool for building richer models for the statistical noise.</p>
<p>So far, although we have not used the term, all of the treatment effects in the ANOVA models that we have considered have been <em>fixed effects</em>. We use fixed effects when we are interested in specific levels of the corresponding treatment factor. For example, in the hot dog data, were are interested in the three different types of hot dogs included in the experiment. As a second example, in the reading data, we were interested in drawing inferences about the three different teaching methods.</p>
<p>In contrast to fixed effects, we use <em>random effects</em> when the levels of a factor included in an experiment can be regarded as a representative (random) sample from a population of levels, and we are interested in drawing inferences about that population, not just the specific levels included in the experiment. The residual errors that we have included in all of our models so far is an example of a random effect. For example, in the reading data, we are not interested in drawing inferences about the particular 66 students included in the study, but instead we regard these students as a random sample from a larger population, and we want to learn about that population. Similarly, in the pine-resin data, we are not interested in limiting our inferences to the particular 24 trees included in the experiment, but we regard those particular trees as a representative sample from a population of trees to which we wish to extend our inferences.</p>
<p>One (imperfect) way to determine whether a factor should be treated with fixed or random effects is to ask: if the experiment were repeated, would the same levels of the factor be included in the experiment, or would the experiment include a potentially different set of levels? If the experiment would include the same levels, then this suggests that the experimenter is interested in drawing inferences about these particular levels, and therefore fixed effects should be used. On the other hand, if the experiment could potentially include different levels, then this suggests that the levels have been drawn from some larger population of levels, and thus random effects should be used.</p>
<p>Whether we treat an effect as a fixed effect or a random effect determines the parameters that we estimate to characterize that effect. For fixed effects, we estimate parameters that quantify the differences among the specific levels included in the experiment. For example, with the reading data, we use parameters that allow us to draw inferences about the differences among the three reading methods in the experiment. For random effects, we estimate a variance parameter that quantifies the spread in the distribution of the population from which the levels included in the experiment were drawn. Thus, for the reading data, we also estimated an error variance that quantifies the variation in responses among students who were taught with the same method.</p>
<p>As a bit of terminology, statistical models can be classified by whether they contain just fixed effects, just random effects, or both fixed and random effects (not counting either the intercept or the residual error term, which all models have). A model with only fixed effects is called (sensibly enough) a fixed effects model, and a model with only random effects is called a random effects model. Models with both fixed and random effects are called <em>mixed-effects models</em>, or sometimes just ``mixed models’’ for short.</p>
<p>To sum up, we draw inferences about the levels of the fixed effects included in the experiment, and the scope of those inferences extends to the population(s) from which the random effects were selected.</p>
</div>
<div id="random-effects-models" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Random-effects models<a href="random-effects.html#random-effects-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Models with no fixed effects (except for the intercept) are called random-effects models. Pure random-effects models are not frequently encountered in the life sciences, but they are occasionally useful for characterizing hierarchical variability.</p>
<p>This example is taken from the on-line SAS documentation. The description given there reads</p>
<blockquote>
<p>In the following example from <span class="citation">Snedecor and Cochran (<a href="#ref-snedecor67">1967</a>)</span>, an experiment is conducted to study the variability of calcium concentration in turnip greens. Four plants are selected at random; then three leaves are randomly selected from each plant. Two 100-mg samples are taken from each leaf. The amount of calcium is determined by microchemical methods.</p>
</blockquote>
<!-- The data are: -->
<!-- \begin{center} -->
<!--    \begin{tabular}{c|c|c|c|c|c|c|} -->
<!--        & \multicolumn{6}{c}{Leaf} \\ -->
<!--        Plant & \multicolumn{2}{c}{1} & \multicolumn{2}{c}{2} & \multicolumn{2}{c}{3} \\ \hline -->
<!--        1 & 3.28 & 3.09 &   3.52 &  3.48 &  2.88     & 2.80 \\  \hline -->
<!--        2 & 2.46 & 2.44  & 1.87  & 1.92  & 2.19   & 2.19 \\  \hline -->
<!--        3 & 2.77 &  2.66 &  3.74 &  3.44 &  2.55 & 2.55 \\   \hline -->
<!--        4 & 3.78 &  3.87 &  4.07 &  4.12 &  3.31 &  3.31 \\   \hline -->
<!--    \end{tabular} -->
<!-- \end{center} -->
<p>A partial data listing is shown here:</p>
<pre><code>plant   leaf    y
p1    l1      3.28
p1    l1      3.09
p1    l2      3.52
p1    l2      3.48
...
p2    l1      2.46
p2    l1      2.44
...
p4    l3      3.31
p4    l3      3.31</code></pre>
<p>With these data, we may want to ask: how does sample-to-sample variation within a leaf compare to lea<span class="math inline">\(F\)</span>-to-leaf variation within a plant, and how do each of these sources of variation compare to plant-to-plant variation?</p>
<p>We can answer these questions with a random-effects model. Our goal here is to quantify the sources of variation that contribute to variation among the samples. We are not interested in quantifying the differences among these particular four plants, or among these particular leaves on these particular plants, or among these particular subsamples on these particular leaves. Instead, each factor in our experiment — plant, leaf and sample — contains levels that are regarded as a random sample from a population of levels. If we were to repeat the experiment, we might choose different plants, or different leaves, or different samples.</p>
<p>Let <span class="math inline">\(y_{ijk}\)</span> be the response for sample <span class="math inline">\(k\)</span> from leaf <span class="math inline">\(j\)</span> of pant <span class="math inline">\(i\)</span>. Let’s try the model:
<span class="math display">\[
y_{ijk} = \mu + P_i + L_{ij} + \varepsilon_{ijk}
\]</span>
where <span class="math inline">\(\mu\)</span> is the reference level, <span class="math inline">\(P_i\)</span> is a random effect associated with plant <span class="math inline">\(i\)</span>, <span class="math inline">\(L_{ij}\)</span> is a random effect associated with leaf <span class="math inline">\(j\)</span> of plant <span class="math inline">\(i\)</span>, and <span class="math inline">\(\varepsilon_{ijk}\)</span> is the residual error . Because the plants and leaves have been randomly selected from populations of plants and leaves, we assume
<span class="math display">\[\begin{eqnarray*}
P_i &amp; \sim &amp; \mathcal{N}\left(0, \sigma^2_P \right) \\
L_{ij} &amp; \sim &amp; \mathcal{N}\left(0, \sigma^2_L \right) \\
\varepsilon_{ijk} &amp; \sim &amp; \mathcal{N}\left(0, \sigma^2_{\varepsilon} \right)
\end{eqnarray*}\]</span></p>
<p>In words, each of the random effects are (independently) drawn from normal distributions with distinct variances. Note that we can think of the residual <span class="math inline">\(\varepsilon_{ijk}\)</span> as the random effect associated with sample <span class="math inline">\(k\)</span> from leaf <span class="math inline">\(j\)</span> of plant <span class="math inline">\(i\)</span>. There’s something else going on with this experiment. Namely, the “leaf” factor and the “plant” factor are not crossed. That is, each leaf is associated with one and only one plant. Thus, the “leaf” factor is nested within the “plant” factor. Similarly, each subsample is associated with one and only one leaf, so the subsample is nested within the leaf.<a href="#fn27" class="footnote-ref" id="fnref27"><sup>27</sup></a></p>
<!-- You may also observe that the samples are nested within leaves.  Thus, to be consistent, perhaps we should have written the error terms as $\varepsilon_{k(ij)}$.  While this is true, it is true of every experiment that we have considered so far.   (For example, in the hotdog data, replicate 1 of the 'beef' hotdogs was different from replicate "1" of the meat hotdogs.)  Because the replicates are always nested within the individual treatment combinations, the parenthetical notation for the subscripts is superfluous for the error term. -->
<p>In a random effects model, our goals are not to estimate the individual <span class="math inline">\(P_i\)</span>’s or <span class="math inline">\(L_{ij}\)</span>’s themselves. Instead, our goal is to estimate the variance parameters (or variance components) <span class="math inline">\(\sigma^2_P\)</span>, <span class="math inline">\(\sigma^2_L\)</span> and <span class="math inline">\(\sigma^2_{\varepsilon}\)</span> . These parameters quantify the plant-to-plant variability, the lea<span class="math inline">\(F\)</span>-to-leaf variability within each plant, and the sample-to-sample variability within each leaf, respectively. Here is an analysis within PROC MIXED:</p>
<pre><code>proc mixed; 
  class Plant Leaf; 
  model Calcium = ;
  random Plant Leaf(Plant); 
run;

Covariance Parameter
Estimates

Cov Parm        Estimate
Plant             0.3652
Leaf(Plant)       0.1611
Residual        0.006654</code></pre>
<p>In PROC MIXED, we only list fixed effects on the right-hand side of the equals sign in the MODEL statement. In this model, there are no fixed effects, so nothing is placed on the right-hand side of the equals sign. (The reference level is a fixed effect, but it is included in every model, so it is never specified.) In the RANDOM statement, we list the random effects. Here, we include random effects for the individual plants and for the leaves nested within plants. Note that SAS uses the parenthetical notation to denote nesting as well, so that we write LEAF(PLANT) to tell SAS that the leaf factor is nested within the plant factor. We do not have to specify a random effect for the sample error, because the sample error is equivalent to the residual error, and this is always included in the model as well.</p>
<p>Our parameter estimates are <span class="math inline">\(\hat{\sigma}^2_P = 0.3652\)</span>, <span class="math inline">\(\hat{\sigma}^2_L = 0.1611\)</span>, and <span class="math inline">\(\hat{\sigma}^2_{\varepsilon} = 0.0067\)</span>. Thus, we conclude that plant-to-plant variation is roughly twice as large as lea<span class="math inline">\(F\)</span>-to-leaf variation within plants, and that both of these are much larger than sample-to-sample variation within leaves.</p>
<p>It is possible to generate an ANOVA table for the random-effects model. We could define sums-of-squares for each of the model terms in the following way:
<span class="math display">\[\begin{eqnarray*}
SS(Plant) &amp; = &amp; \sum_{i=1}^4 \sum_{j=1}^3 \sum_{k=1}^2 \left( \bar{y}_{i++} - \bar{y}_{+++} \right)^2 \\
SS(Leaf(Plant)) &amp; = &amp; \sum_{i=1}^4 \sum_{j=1}^3 \sum_{k=1}^2 \left( \bar{y}_{ij+} - \bar{y}_{i++} \right)^2 \\
SS(Error) &amp; = &amp; \sum_{i=1}^4 \sum_{j=1}^3 \sum_{k=1}^2 \left( \bar{y}_{ijk} - \bar{y}_{ij+} \right)^2 \\
\end{eqnarray*}\]</span>
Each sum-of-squares is associated with a certain number of df:</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
source
</th>
<th style="text-align:left;">
df
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Plant
</td>
<td style="text-align:left;">
3 (= 4 - 1)
</td>
</tr>
<tr>
<td style="text-align:left;">
Leaf(Plant)
</td>
<td style="text-align:left;">
8 (= 12 - 1- 3)
</td>
</tr>
<tr>
<td style="text-align:left;">
Error
</td>
<td style="text-align:left;">
12 (= 24 - 1- 3 - 8)
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
23
</td>
</tr>
</tbody>
</table>
<p>Something different has happened here. Because leaf is nested within plant, we have to deduct the df for the SS(Plant)from the df for SS(Leaf(Plant)). This is the general rule for nested factors. If factor ‘B’ is nested in factor ‘A’, then the df for SS(A) are subtracted from the df for SS(B(A)). (In fact, this explains why we’ve been deducting df for factorial effects from the df for error all along. The replicates are always nested within the factorial treatments, even though we haven’t thought of it in this way until now.)</p>
<p>In models in which the residual error is the only random effect, we have learned that the estimate of that error, <span class="math inline">\(\hat{\sigma}^2_{\varepsilon}\)</span>, is just equal to the MSE. Does this mean that we can equate MS(Plant) and MS(Leaf(Plant)) with <span class="math inline">\(\hat{\sigma}^2_P\)</span> and <span class="math inline">\(\hat{\sigma}^2_L\)</span>, respectively? Unfortunately, in this case life isn’t that simple. As it turns out, there is a relationship between each of the mean squares and each of the variance components, but it’s complicated. One way to generate an ANOVA and learn about this relationship in SAS is to use the METHOD = TYPE3 option in PROC MIXED:</p>
<pre><code>proc mixed data = turnip method = type3;
  class Plant Leaf;
  model y = ;
  random Plant Leaf(Plant);
run;

Type 3 Analysis of Variance

                           Sum of
Source           DF       Squares   Mean Square  Expected Mean Square

Plant             3      7.560346      2.520115  Var(Residual) + 2 Var(Leaf(Plant))
                                                 + 6 Var(Plant)
Leaf(Plant)       8      2.630200      0.328775  Var(Residual) + 2 Var(Leaf(Plant))
Residual         12      0.079850      0.006654  Var(Residual)

Covariance Parameter
Estimates

Cov Parm        Estimate
Plant             0.3652
Leaf(Plant)       0.1611
Residual        0.006654</code></pre>
<p>Here, we see the ANOVA table with SS, MS and df for each variance component. We also see a column labeled “Expected Mean Square”. Briefly, the expected mean square is a formula that equates the “expectation” of the mean square for each variance component to each of the model parameters. (Remember that an “expectation” is the average value that you would observe over many runs of the same experiment.) The derivation of this formula is beyond the scope of ST512. With the formula in hand, though, it suggests one method of estimating the variance components. We can estimate the variance components by equating each MS with its expected mean square, and solving for the variance-component parameters.</p>
<p>Sometimes, this “method of moments” approach to parameter estimation works well. Other times, however, it can generate negative estimates of variance parameters. (To see this, suppose that in the example above we had a data set where MS(Leaf) &lt; MS(Residual). Then we would have to have <span class="math inline">\(\hat{\sigma}^2_L &lt; 0\)</span>, which makes no sense.) Consequently, several other approaches have been developed to estimate variance-component parameters. The default in PROC MIXED is to use REML, which is an acronym for REstricted Maximum Likelihood. Although we will not discuss REML, do be aware that REML is a iterative numerical method. In some cases, the method can fail. The output to PROC MIXED presents an ‘Iteration History’ that gives you some information about the success or failure of REML. It is good practice to review the Iteration History and the Log file to make sure the procedure converged successfully. (We also saw a numerical method for estimating parameters when we considered non-linear regression.)</p>
<p>For some nice data sets (such as the one above), the method-of-moments estimates of the variance components and the REML estimates are the same. In other cases, they may differ. General practice is to use the REML estimates unless a good reason exists to use another method of estimation.</p>
</div>
<div id="subsampling" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> Subsampling<a href="random-effects.html#subsampling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A second technique for reducing experimental error is subsampling. Subsampling refers to measuring the same experimental unit multiple times. This is the first occasion we have encountered in ST512 where the experimental unit (EU) and the measurement unit (MU) differ. Remember that the EU is the unit to which the experimental factor is applied, and the MU is the unit that is measured.</p>
<p><em>Example</em>: A former ST512 student studied the effect of 6 different vase solutions on the shelf lives of roses. She had 30 different vases at her disposal. The 6 different solutions were randomly assigned to the 30 vases in a balanced CRD with a one-way treatment structure. Three roses (or stems) were placed in each vase, and the shelf life of each rose was recorded. Here are some of the data for the cultivar “Freedom”:</p>
<pre><code>cultivar trt     vase    stem    lifetime
Freedom  1       1       1       13     
Freedom  1       1       2       13     
Freedom  1       1       3       11     
Freedom  1       2       1       14     
Freedom  1       2       2       11     
Freedom  1       2       3       12     
Freedom  1       3       1       14     
...
Freedom  1       5       3       13     
Freedom  2       1       1       16     
Freedom  2       1       2       16     
Freedom  2       1       3       12     
Freedom  2       2       1       13    
...</code></pre>
<p>Here, the EU is the vase, and the MU is the stem. The stem is not the EU! The different stems are subsamples, in the sense that they are repeated samples of the same EU. Subsampling does not increase the number of EUs (and hence does not increase the number of df available to estimate the experimental error), but it does reduce the experimental error.</p>
<div id="equal-subsamples-per-eu" class="section level3 hasAnchor" number="8.3.1">
<h3><span class="header-section-number">8.3.1</span> Equal subsamples per EU<a href="random-effects.html#equal-subsamples-per-eu" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If the number of subsamples (stems) is the same for each EU (vase), we can average the subsample data (stem lifetimes) in each EU to generate 1 data point per EU, and then analyze the EU-averages with the usual one-way ANOVA.</p>
<p>To calculate an average response for each EU, we can use whatever software package is most convenient (e.g., Excel). To calculate EU-level averages in SAS, we can use the following procedure. Suppose we had loaded the data from a file with the following DATA step:</p>
<pre><code>data freedom;
   infile &quot;.../rose.txt&quot; firstobs=2;
   input cultivar$ trt vase stem lifetime;
run;</code></pre>
<p>We can then calculate average lifetimes per vase with PROC MEANS:</p>
<pre><code>/* This procedure averages the stems in each vase 
   and produced a new data set named &#39;rosemeans&#39; 
   that contains the newly calculated average lifetimes */

proc means data=freedom noprint;
  by cultivar trt vase;
  var lifetime;
  output out=rosemeans mean=avglife;
run;</code></pre>
<p>We can use PROC PRINT to examine the newly created data set:</p>
<pre><code>proc print data=rosemeans;
run;

Obs  cultivar    trt    vase    _TYPE_    _FREQ_    avglife
1    Freedom      1       1        0         3      12.3333
2    Freedom      1       2        0         3      12.3333
3    Freedom      1       3        0         3      13.3333
4    Freedom      1       4        0         3      12.6667
5    Freedom      1       5        0         3      12.6667
6    Freedom      2       1        0         3      14.6667
7    Freedom      2       2        0         3      13.6667
...
30   Freedom      6       5        0         3      12.0000</code></pre>
<p>Note that there is now one observation per vase. Now we analyze the per-vase averages using a standard one-way ANOVA analysis in PROC GLM:</p>
<pre><code>proc glm data = rosemeans;
  class trt;
  model avglife = trt;
  means trt / tukey;
run;

Dependent Variable: avglife

                                        Sum of
Source                      DF         Squares     Mean Square    F Value    Pr &gt; F
Model                        5     122.9185185      24.5837037       9.88    &lt;.0001
Error                       24      59.6888889       2.4870370
Corrected Total             29     182.6074074

Source                      DF     Type III SS     Mean Square    F Value    Pr &gt; F
trt                          5     122.9185185      24.5837037       9.88    &lt;.0001

Tukey&#39;s Studentized Range (HSD) Test for avglife
Means with the same letter are not significantly different.

Tukey 
Grouping   Mean      N    trt
A       13.8667      5    2
A       13.6000      5    3
A       13.0667      5    6
A       12.6667      5    1
A       11.8667      5    5
B        7.8667      5    4</code></pre>
<p>For comparison, suppose that instead only one rose stem had been included in each vase. The data set ‘rose1’ was formed by extracting the first rose stem listed for each vase from the original data set:</p>
<pre><code>cultivar trt     vase    stem    lifetime
Freedom  1       1       1       13     
Freedom  1       2       1       14     
Freedom  1       3       1       14     
...
Freedom  2       1       1       16     
Freedom  2       2       1       13    
...</code></pre>
<p>Here is the same one-factor ANOVA analysis for this smaller data set:</p>
<pre><code>proc glm data = freedom1;
  class trt;
  model lifetime = trt;
run;

                                        Sum of
Source                      DF         Squares     Mean Square    F Value    Pr &gt; F
Model                        5      84.5666667      16.9133333       2.30    0.0765
Error                       24     176.4000000       7.3500000
Corrected Total             29     260.9666667

Source                      DF     Type III SS     Mean Square    F Value    Pr &gt; F
trt                          5     84.56666667     16.91333333       2.30    0.0765</code></pre>
<p>Now the differences among the vase treatments are no longer significant! Note that the df’s have not changed: this is because both the full and reduced data sets contain the same number of EUs. What has changed is that the MSE for the reduced data set is much larger than the MSE for the full data set. This is because the `unexplained variability’ in the reduced data set includes both variability among vases assigned to the same treatment, and variability among individual rose stems. In the full data set, however, the unexplained variability in each data point consists of variability among vases assigned to the same treatment, and variability in the average lifetime of three individual rose stems. Because averaging reduces variability, the unexplained variability in the full data set is smaller than the unexplained variability in the reduced data set.</p>
<p>Thus, subsampling reduces experimental error by averaging out the variability among individual measurements from the same EU.</p>
<p>The wrong way to analyze these data is to treat the stem as the EU:</p>
<pre><code>/* This is the WRONG analysis */

proc glm data = freedom;
  class trt;
  model lifetime = trt;
run;
                                        Sum of
Source                      DF         Squares     Mean Square    F Value    Pr &gt; F
Model                        5     368.7555556      73.7511111      10.38    &lt;.0001
Error                       84     597.0666667       7.1079365
Corrected Total             89     965.8222222

Source                      DF     Type III SS     Mean Square    F Value    Pr &gt; F
trt                          5     368.7555556      73.7511111      10.38    &lt;.0001</code></pre>
<p>This analysis is wrong because it assumes that vase solutions were randomized to stems and not vases. This is <em>pseudoreplication</em>, because the analysis incorrectly inflates the amount of replication in the experiment. Pseudoreplication is a common and insidious mistake.</p>
<div id="a-second-look-analysis-with-a-mixed-effects-model" class="section level4 hasAnchor" number="8.3.1.1">
<h4><span class="header-section-number">8.3.1.1</span> A second look: Analysis with a mixed effects model<a href="random-effects.html#a-second-look-analysis-with-a-mixed-effects-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The method of averaging subsamples works well when each EU has the same number of subsamples. However, averaging subsamples does not work well when the number of subsamples per EU differs. This is because EUs with more subsamples will have less variability than EUs with fewer subsamples, and thus the equal variance assumption of ANOVA will be violated with averaged data. (As always, the seriousness of this violation is a matter of degree. If, for example, 29 vases had 3 stems and one vase had 2 stems, the unequal variance caused by averaging might be sufficiently mild that one could still justify averaging.) The best way to analyze data with unequal numbers of subsamples is to use a model that explicitly accounts for both the variation among EUs and the variation among MUs.</p>
<p>To keep things from getting too complicated, we won’t look here at an example with an unequal number of subsamples per EU, but will instead consider an alternative analysis of the rose data above. This analysis can be easily modified to accommodate unequal numbers of subsamples per EU.</p>
<p>Here is a model that we can use that models the data for the individual stems explicitly. This model will work regardless of whether we have the same number of stems in each vase. Let be the lifetime of the kth stem from the jth vase assigned to the ith treatment. Consider the model
<span class="math display">\[
y_{ijk} = \mu_i + V_{ij} + \varepsilon_{ijk}
\]</span>
where</p>
<ul>
<li><span class="math inline">\(i = 1, 2, \ldots, 6\)</span> is an index for the treatment</li>
<li><span class="math inline">\(j = 1, 2, \ldots, 5\)</span> is an index for the vase in each treatment</li>
<li><span class="math inline">\(k = 1, 2, 3\)</span> is an index for the stem in each vase</li>
<li><span class="math inline">\(\mu_i\)</span> is the average response for treatment <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(V_{ij}\)</span> is a random effect for vase <span class="math inline">\(j\)</span> from treatment <span class="math inline">\(i\)</span>, <span class="math inline">\(V_{ij} \sim \mathcal{N}\left(0, \sigma^2_V \right)\)</span></li>
<li><span class="math inline">\(\varepsilon_{ijk}\)</span> is subsample (residual) error for stem <span class="math inline">\(k\)</span> from vase <span class="math inline">\(j\)</span> of treatment <span class="math inline">\(i\)</span>, <span class="math inline">\(\varepsilon_{ijk} \sim \mathcal{N}\left(0, \sigma^2_{\varepsilon} \right)\)</span></li>
</ul>
<p>The key feature of the model above is that it includes two random effects: one for the vase-to-vase error within treatments, and a second for the stem-to-stem error within vases. We assume that the random effects for the vases are drawn from a normal distribution with mean 0 and variance <span class="math inline">\(\sigma^2_V\)</span>, and that the random effects for the stems are drawn from a normal distribution with mean 0 and variance <span class="math inline">\(\sigma^2_{\varepsilon}\)</span>.</p>
We can still think about building an ANOVA table for this analysis through a SS decomposition. The df for each SS will be:
<table>
<thead>
<tr>
<th style="text-align:left;">
source
</th>
<th style="text-align:left;">
df
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Treatment
</td>
<td style="text-align:left;">
5
</td>
</tr>
<tr>
<td style="text-align:left;">
Vase (EU error)
</td>
<td style="text-align:left;">
24 (= 30 - 1- 5)
</td>
</tr>
<tr>
<td style="text-align:left;">
Stems (subsample error)
</td>
<td style="text-align:left;">
60 (= 90 - 1 - 5 - 24)
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
89
</td>
</tr>
</tbody>
</table>
<p>The key to this analysis is to realize that the treatment effects should be tested using the mean-squared error for the vases, not the stems. This is because the vases are the EUs (treatments were randomized to vases). Thus, any inferences that we draw about the treatments (i.e., <span class="math inline">\(F\)</span>-tests, linear contrasts, or multiple comparisons) should be based on the MSE for the vases, not the MSE for the stems. In particular, we anticipate that the <span class="math inline">\(F\)</span>-test for differences among the treatments will be based on an <span class="math inline">\(F\)</span>-distribution with 5 ndf and 24 ddf.</p>
<p>Here is PROC MIXED code and output for the Freedom rose data, using the default REML estimation:</p>
<pre><code>proc mixed data = freedom;
  class trt vase;
  model lifetime = trt;
  random vase(trt);
run;

Covariance Parameter
Estimates

Cov Parm      Estimate
vase(trt)       0.1648
Residual        6.9667

Type 3 Tests of Fixed Effects

              Num     Den
Effect         DF      DF    F Value    Pr &gt; F
trt             5      24       9.88    &lt;.0001</code></pre>
<p>Remarks:</p>
<ul>
<li><p>The MODEL statement in PROC MIXED only contains the fixed effects. In this model, the only fixed effects are the different treatments (vase solutions).</p></li>
<li><p>The RANDOM statement specifies the random effects to be included in the model. The residual (= the error term for the subsample) is always included by default, so a separate term for “stem” did not need to be specified.</p></li>
<li><p>We use the parenthetical syntax VASE(TRT) to tell SAS that VASE is nested within TRT.</p></li>
<li><p>The portion of the output labeled “Covariance Parameter Estimates” provides the estimates for <span class="math inline">\(\sigma^2_V\)</span> and <span class="math inline">\(\sigma^2_{\varepsilon}\)</span>. In this case, they are <span class="math inline">\(\sigma^2_V  = 0.165\)</span> and <span class="math inline">\(\sigma^2_{\varepsilon} = 6.97\)</span>. Although these estimates are not the primary focus of our analysis, they are still informative. In this case, they tell us that the stem-to-stem variability among stems in the same vase is much greater than the vase-to-vase variability among vases receiving the same treatment.</p></li>
<li><p>The portion of the output labeled “Type 3 Tests of Fixed Effects” provides <span class="math inline">\(F\)</span>-tests of the fixed effects. With REML estimation, PROC MIXED does not present the full ANOVA table. Note that the <span class="math inline">\(F\)</span>-test for the treatment effect is identical to the <span class="math inline">\(F\)</span>-test that we obtained by averaging subsamples within EUs. These two tests are identical because in this case we have the same number of stems per vase.</p></li>
<li><p>Note that the <span class="math inline">\(F\)</span>-test for the treatment effects has the correct ndf and ddf. Unless we specify otherwise, PROC MIXED uses the “containment” method for determining the appropriate error terms. In this case, the fixed-effects for TRT are contained within the random effect for VASE(TRT) (this is another way of saying that the vases are nested within the treatement), and so PROC MIXED uses the MSE for VASE(TRT) to test for the TRT fixed effects. The df for the <span class="math inline">\(F\)</span>-tests can be used as a check to make sure that you’ve coded the model correctly.</p></li>
</ul>
<p>Here is an example of using linear contrasts and multiple comparisons to explore differences among the treatments. Note that throughout the df used for inference are the df that correspond to the vase error, not to the stem error.</p>
<p>Linear contrasts:</p>
<pre><code>proc mixed data = freedom;
  class trt vase;
  model lifetime = trt;
  random vase(trt);
  estimate &#39;Difference b/w solutions 1 and 2&#39; trt 1 -1 0 0 0 0;
run;
                                                Standard
Label                               Estimate       Error      DF    t Value    Pr &gt; |t|
Difference b/w solutions 1 and 2     -1.2000      0.9974      24      -1.20      0.2407</code></pre>
<p>Multiple comparisons (there is no MEANS statement available in PROC MIXED, so we use LSMEANS instead):</p>
<pre><code>proc mixed data = freedom;
  class trt vase;
  model lifetime = trt;
  random vase(trt);
  lsmeans trt / pdiff adj=tukey;
run;

Least Squares Means

                             Standard
Effect    trt    Estimate       Error      DF    t Value    Pr &gt; |t|
trt       1       12.6667      0.7053      24      17.96      &lt;.0001
trt       2       13.8667      0.7053      24      19.66      &lt;.0001
trt       3       13.6000      0.7053      24      19.28      &lt;.0001
trt       4        7.8667      0.7053      24      11.15      &lt;.0001
trt       5       11.8667      0.7053      24      16.83      &lt;.0001
trt       6       13.0667      0.7053      24      18.53      &lt;.0001

Differences of Least Squares Means

                                 Standard
Effect   trt   _trt   Estimate      Error     DF   t Value   Pr &gt; |t|   Adjustment      Adj P

trt      1     2       -1.2000     0.9974     24     -1.20     0.2407   Tukey          0.8310
trt      1     3       -0.9333     0.9974     24     -0.94     0.3587   Tukey          0.9331
trt      1     4        4.8000     0.9974     24      4.81     &lt;.0001   Tukey          0.0008
trt      1     5        0.8000     0.9974     24      0.80     0.4304   Tukey          0.9644
trt      1     6       -0.4000     0.9974     24     -0.40     0.6919   Tukey          0.9985
trt      2     3        0.2667     0.9974     24      0.27     0.7915   Tukey          0.9998
trt      2     4        6.0000     0.9974     24      6.02     &lt;.0001   Tukey          &lt;.0001
trt      2     5        2.0000     0.9974     24      2.01     0.0563   Tukey          0.3685
trt      2     6        0.8000     0.9974     24      0.80     0.4304   Tukey          0.9644
trt      3     4        5.7333     0.9974     24      5.75     &lt;.0001   Tukey          &lt;.0001
trt      3     5        1.7333     0.9974     24      1.74     0.0951   Tukey          0.5217
trt      3     6        0.5333     0.9974     24      0.53     0.5978   Tukey          0.9941
trt      4     5       -4.0000     0.9974     24     -4.01     0.0005   Tukey          0.0060
trt      4     6       -5.2000     0.9974     24     -5.21     &lt;.0001   Tukey          0.0003
trt      5     6       -1.2000     0.9974     24     -1.20     0.2407   Tukey          0.8310</code></pre>
<p>If we want to generate an ANOVA table, we can use the METHOD=TYPE3 option for method-of-moments estimation:</p>
<pre><code>proc mixed data = freedom method = type3;
  class trt vase;
  model lifetime = trt;
  random vase(trt);
run;

Type 3 Analysis of Variance

                         Sum of
Source         DF       Squares   Mean Square  Expected Mean Square                 Error Term

trt             5    368.755556     73.751111  Var(Residual) + 3 Var(vase(trt))     MS(vase(trt))
                                               + Q(trt)
vase(trt)      24    179.066667      7.461111  Var(Residual) + 3 Var(vase(trt))     MS(Residual)
Residual       60    418.000000      6.966667  Var(Residual)                        .


Type 3 Analysis of Variance

            Error
Source         DF    F Value    Pr &gt; F
trt            24       9.88    &lt;.0001
vase(trt)      60       1.07    0.4015

Covariance Parameter
Estimates

Cov Parm      Estimate
vase(trt)       0.1648
Residual        6.9667</code></pre>
<p>Again, things work out nicely here because the data are balanced, but it is not guaranteed that the method-of-moments estimation and REML estimation will match up exactly.</p>
<!-- Variations with unbalanced data: Let's briefly consider how the df accounting might change with unbalanced data.  Consider an experiment identical to the one above, except that now a single stem was lost from each of 7 vases, so that there are 83 total stems and 30 vases.  The df accounting is now: -->
<!-- ```{r echo = FALSE} -->
<!-- rose_anova_table <- data.frame(cbind(sources = c("Treatment", "Vase (EU error)","Stems (subsample error)", "Total"), -->
<!--                                       df = c(5, "24 (= 30 - 1- 5)", "53 \ (= 83 - 1 - 5 - 24)", 82))) -->
<!-- colnames(rose_anova_table) = c("source", "df") -->
<!-- knitr::kable(format="html", -->
<!--              rose_anova_table,  -->
<!--              escape = FALSE)  -->
<!-- ``` -->
<!-- Now, suppose that two entire vases were lost, and that there are 3 stems in each of the remaining 28 vases.  Now the df accounting would become: -->
<!-- ```{r echo = FALSE} -->
<!-- rose_anova_table <- data.frame(cbind(sources = c("Treatment", "Vase (EU error)","Stems (subsample error)", "Total"), -->
<!--                                       df = c(5, "22 (= 28 - 1- 5)", "56 \ (= 84 - 1 - 5 - 24)", 83))) -->
<!-- colnames(rose_anova_table) = c("source", "df") -->
<!-- knitr::kable(format="html", -->
<!--              rose_anova_table,  -->
<!--              escape = FALSE)  -->
<!-- ``` -->
<!-- In all other respects, the analysis of unbalanced data would proceed identically to the analysis of balanced data. -->
<p>A final comment on subsampling: Often it is more expensive to have more EUs and is easier to have more subsamples. The extent to which increasing the number of subsamples will increase statistical power depends on the relative magnitudes of the variability among EUs and the variability among subsamples. If the variability among EUs is large relative to the variability among subsamples, then adding subsamples does little to increase statistical precision. But if the variability among subsamples is of the same or greater magnitude than the variability among EUs (as in the rose example), then adding subsamples can increase precision and statistical power.</p>
</div>
</div>
</div>
<div id="starmathematical-foundations" class="section level2 hasAnchor" number="8.4">
<h2><span class="header-section-number">8.4</span> <span class="math inline">\(^\star\)</span>Mathematical foundations<a href="random-effects.html#starmathematical-foundations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we introduce formal mathematical arguments for how random effects induce correlations among grouped responses. This section makes heavy uses of the mathematics of probability. Before proceeding, we will need to review several key probability concepts</p>
<div id="probability-refresher" class="section level3 hasAnchor" number="8.4.1">
<h3><span class="header-section-number">8.4.1</span> Probability refresher<a href="random-effects.html#probability-refresher" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(X\)</span> denote a random variable. Recall that the <em>expectation</em>, or <em>expected value</em>, of <span class="math inline">\(X\)</span> is one measure of the center of the probability distribution of <span class="math inline">\(X\)</span>. The expectation can also be thought of as the mean or average of <span class="math inline">\(X\)</span>. We denote the expectation of <span class="math inline">\(X\)</span> as <span class="math inline">\(\mathrm{E}\left[X\right]\)</span>, or sometimes <span class="math inline">\(\mu_X\)</span>. (The subscript on <span class="math inline">\(\mu\)</span> is only used when we need it to clarify to which random variable the expectation refers.)</p>
<p>The <em>variance</em> of <span class="math inline">\(X\)</span>, often denoted <span class="math inline">\(\mbox{Var}\left(X\right)\)</span> or <span class="math inline">\(\sigma^2_X\)</span>, is defined as the expectation of <span class="math inline">\((X - \mu_X)^2\)</span>. In other words,
<span class="math display">\[
\mbox{Var}\left(X\right) = \mathrm{E}\left[(X - \mu_X)^2\right].
\]</span>
The variance of a random variable is a measure of its dispersion, or spread.</p>
<p>Consider two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The <em>covariance</em> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, written as <span class="math inline">\(\mbox{Cov}\left(X, Y\right)\)</span>, is defined as the expectation of the product <span class="math inline">\((X - \mu_X) (Y - \mu_Y)\)</span>. In other words,
<span class="math display">\[
\mbox{Cov}\left(X, Y\right) = \mathrm{E}\left[(X - \mu_X)(Y - \mu_Y)\right].
\]</span>
Note that the covariance of a random variable with itself is just its variance, that is, <span class="math inline">\(\mbox{Cov}\left(X, X\right) = \mbox{Var}\left(X\right)\)</span>. The covariance of two random variables is a measure of their association. If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are positively associated, then values of <span class="math inline">\(X\)</span> larger (smaller) than <span class="math inline">\(\mu_X\)</span> will tend to co-occur with values of <span class="math inline">\(Y\)</span> larger (smaller) than <span class="math inline">\(\mu_Y\)</span>, thus the product <span class="math inline">\((X - \mu_X) (Y - \mu_Y)\)</span> will usually be positive, and so the expectation of that product — the covariance — will be positive as well. On the other hand, if the reverse is true, and values of <span class="math inline">\(X\)</span> larger (smaller) than <span class="math inline">\(\mu_X\)</span> tend to co-occur with values of <span class="math inline">\(Y\)</span> smaller (larger) than <span class="math inline">\(\mu_Y\)</span>, then the product <span class="math inline">\((X - \mu_X) (Y - \mu_Y)\)</span> will usually be negative, and so the covariance will be negative.</p>
<p>When <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then their covariance is zero, i.e., <span class="math inline">\(\mbox{Cov}\left(X, Y\right) = 0\)</span>. In general, the converse is not true: just because two random variables have a covariance of 0 does not necessarily imply that they are independent. However, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are two normally distributed random variables, then <span class="math inline">\(\mbox{Cov}\left(X, Y\right) = 0\)</span> does imply that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.</p>
<p>Covariance is not just a measure of the association between two random variables. A covariance is also a measure of the dispersion or spread of those variables. Sometimes, we wish to factor out the dispersion component of covariance, and thus isolate the portion of covariance that measures the strength of association. We can do so by considering the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, written <span class="math inline">\(\mbox{Cor}\left(X, Y\right)\)</span> or <span class="math inline">\(\rho_{X,Y}\)</span>, which scales the covariance by the dispersion of both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>:
<span class="math display">\[
\mbox{Cor}\left(X, Y\right) = \dfrac{\mbox{Cov}\left(X, Y\right)}{\sqrt{\mbox{Var}\left(X\right)\mbox{Var}\left(Y\right)}}.
\]</span>
Or, if we recognize that <span class="math inline">\(\sqrt{\mbox{Var}\left(X\right)} = \mbox{SD}\left(X\right)\)</span> (the standard deviation of <span class="math inline">\(X\)</span>), then we can write
<span class="math display">\[
\mbox{Cor}\left(X, Y\right) = \dfrac{\mbox{Cov}\left(X, Y\right)}{\mbox{SD}\left(X\right)\mbox{SD}\left(Y\right)}.
\]</span></p>
<p>Now here are some useful rules about the expectation and variance of a sum of random variables. First, the expectation of <span class="math inline">\(X+Y\)</span> is simply
<span class="math display">\[
\mathrm{E}\left[X+Y\right] = \mathrm{E}\left[X\right] + \mathrm{E}\left[Y\right].
\]</span>
The variance of <span class="math inline">\(X+Y\)</span> is more complicated:
<span class="math display">\[
\mbox{Var}\left(X+Y\right) = \mbox{Var}\left(X\right) + \mbox{Var}\left(Y\right) + 2 \mbox{Cov}\left(X, Y\right).
\]</span>
Thus, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(\mbox{Var}\left(X+Y\right) = \mbox{Var}\left(X\right) + \mbox{Var}\left(Y\right)\)</span>. But, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are not independent, then we have to factor in the covariance.</p>
</div>
<div id="application-to-models-with-random-effects" class="section level3 hasAnchor" number="8.4.2">
<h3><span class="header-section-number">8.4.2</span> Application to models with random effects<a href="random-effects.html#application-to-models-with-random-effects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here is how this applies to models with random effects. Consider our model for the turnip-green data, but consider a simplified data set where there is only one sample from each leaf. Let’s write the model for these data as
<span class="math display">\[
y_{ij} = \mu + P_i + \varepsilon_{ij}
\]</span>
where <span class="math inline">\(P_i \sim \mathcal{N}\left(0, \sigma^2_P \right)\)</span> represents the plant random effects, and <span class="math inline">\(\varepsilon_{ijk} \sim  \mathcal{N}\left(0, \sigma^2_{\varepsilon} \right)\)</span> represents the leaf random effects (the residual error in this case). Let’s first find an expression for the variance of <span class="math inline">\(y_{ij}\)</span>:
<span class="math display">\[\begin{eqnarray*}
\mbox{Var}\left(y_{ij}\right) &amp; = &amp;  \mbox{Var}\left(\mu + P_i + \varepsilon_{ij}\right) \\
       &amp; = &amp;  \mbox{Var}\left(P_i\right) + \mbox{Var}\left(\varepsilon_{ij}\right) + 2 \mbox{Cov}\left(P_i, \varepsilon_{ij}\right)\\
       &amp; = &amp;  \sigma^2_P + \sigma^2_{\varepsilon}.
\end{eqnarray*}\]</span>
Note that <span class="math inline">\(\mu\)</span> is a constant, and not a random variable, so its variance is 0. Note also that because the plant random effects and the leaf random effects are independent, then <span class="math inline">\(\mbox{Cov}\left(P_i, \varepsilon_{ij}\right) = 0\)</span>.</p>
<p>Now, let’s try to find an expression for the correlation between two data points. First, we’ll find the covariance between two data points. To do so, we’ll need to use a rule for the covariance between two sums of random variables. That rule is
<span class="math display">\[
\mbox{Cov}\left(X+Y, W+Z\right) = \mbox{Cov}\left(X, W\right) + \mbox{Cov}\left(X, Z\right) + \mbox{Cov}\left(Y, W\right) + \mbox{Cov}\left(Y, Z\right).
\]</span>
(Note that we can apply this rule to obtain <span class="math inline">\(\mbox{Var}\left(X+Y\right) = \mbox{Cov}\left(X+Y, X+Y\right) = \mbox{Var}\left(X\right) + \mbox{Var}\left(Y\right) + 2 \mbox{Cov}\left(X, Y\right)\)</span>.)</p>
<p>Now, write two different data points as <span class="math inline">\(y_{ij}\)</span> and <span class="math inline">\(y_{kl}\)</span>. Then,
<span class="math display">\[\begin{eqnarray*}
    \mbox{Cov}\left(y_{ij}, y_{kl}\right) &amp; = &amp;  \mbox{Cov}\left(\mu + P_i + \varepsilon_{ij}, \mu + P_k + \varepsilon_{kl}\right) \\
    &amp; = &amp;  \mbox{Cov}\left(P_i, P_k\right) + \mbox{Cov}\left(P_i, \varepsilon_{kl}\right) + \mbox{Cov}\left(\varepsilon_{ij}, P_k\right) + \mbox{Cov}\left(\varepsilon_{ij}, \varepsilon_{kl}\right).
\end{eqnarray*}\]</span>
Again, the <span class="math inline">\(\mu\)</span>’s drop out because they are constants and have no variance. Next, because the plant random effects are independent of the leaf random effects, <span class="math inline">\(\mbox{Cov}\left(P_i, \varepsilon_{kl}\right) = \mbox{Cov}\left(\varepsilon_{ij}, P_k\right) = 0\)</span>. Next, assuming that are two data points are from different leaves, then <span class="math inline">\(\mbox{Cov}\left(\varepsilon_{ij}, \varepsilon_{kl}\right) = 0\)</span> (because the lea<span class="math inline">\(F\)</span>-level random effects are independent of one another).</p>
<p>What about <span class="math inline">\(\mbox{Cov}\left(P_i, P_k\right)\)</span>? If the leaves come from the same plant, that is, if <span class="math inline">\(i=k\)</span>, then we have <span class="math inline">\(\mbox{Cov}\left(P_i, P_k\right) = \mbox{Cov}\left(P_i, P_i\right) = \mbox{Var}\left(P_i\right) = \sigma^2_P\)</span>. However, if the leaves come from different plants (<span class="math inline">\(i \neq k\)</span>), then the two plant random effects are independent, and thus <span class="math inline">\(\mbox{Cov}\left(P_i, P_k\right) = 0\)</span>. Putting it all together, we have
<span class="math display">\[
\mbox{Cov}\left(y_{ij}, y_{kl}\right) = \begin{cases} \sigma^2_P &amp; i = k \\ 0 &amp; i \neq k \end{cases}.
\]</span></p>
<p>Finally, to convert a covariance to a correlation, we just have to divide the covariance by <span class="math inline">\(\sqrt{\mbox{Var}\left(y_{ij}\right) \mbox{Var}\left(y_{kl}\right)} = \sigma^2_P + \sigma^2_{\varepsilon}\)</span>, giving
<span class="math display">\[
\mbox{Cor}\left(y_{ij}, y_{kl}\right) = \begin{cases} \dfrac{\sigma^2_P}{\sigma^2_P + \sigma^2_{\varepsilon}} &amp; i = k \\ 0 &amp; i \neq k \end{cases}.
\]</span></p>
<p>Thus, we see that including the plant-level random effect induces a positive correlation between leaves from the same plant. (Note that correlation must be positive because <span class="math inline">\(\sigma^2_P\)</span> and <span class="math inline">\(\sigma^2_{\varepsilon}\)</span> must both be positive.)</p>

</div>
</div>
</div>
<h3>Bibliography<a href="bibliography.html#bibliography" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-snedecor67" class="csl-entry">
Snedecor, G. W., and W. G. Cochran. 1967. <em>Statistical Methods</em>. 6th ed. Ames: Iowa State University Press.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="26">
<li id="fn26"><p>The mean of a distribution is such an important and central concept in statistics that we have many different synonyms for it. Although the following terms are not perfect synonyms (their technical definitions are all slightly different), all of the following tend to be used as loose synonyms for a mean: average, center of mass, expected value, and expectation.<a href="random-effects.html#fnref26" class="footnote-back">↩︎</a></p></li>
<li id="fn27"><p>In fact, it is always true that the random effect associated within the individual data point is nested within another term in the model. Because this is always true, we usually don’t call attention to it.<a href="random-effects.html#fnref27" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ancova.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="blocked-designs.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
