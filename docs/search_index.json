[["index.html", "ST 512 course notes Preface", " ST 512 course notes Kevin Gross 2022-12-06 Preface These are course notes for ST 512, Statistical Methods for Researchers II, taught at North Carolina State University. ST 512 is the second semester of a two-semester sequence in statistical methods. These notes are a proto-textbook, and are available for use by anyone associated with NCSU. Philosophy When it comes to statistics, researchers cannot necessarily rely on their common sense to lead them towards correct answers. Statistical reasoning is non-intuitive (Kahneman (2011)), and the foundational ideas of statistics are elusive. Therefore statistical literacy must be learned. The primary goal of this course is to sharpen students statistical literacy so that they may become more effective researchers. However, one does not develop a deep conceptual understanding merely by discussing concepts. Instead, conceptual understanding is honed in part by studying the details of particular methods to understand why those details matter. When we study the details of a method, the expectation is not that the student will remember those details in perpetuity. Indeed, practicing scientists are unlikely anyway to have the cognitive bandwidth to remember details about statistical methods that they do not use routinely. Instead, the point of studying statistical methods in detail is to strengthen conceptual understanding by exploring statistical methods at more than a superficial level. Examining details now will also make those details more recognizable if and when one faces a future data-analysis task that requires re-engaging those details. That said, the ultimate emphasis of this course is not on the details of the methods, but is instead on the ideas, concepts, and reasoning that underlie statistical thinking. I hope that students of this course will deepen their conceptual understanding of statistics and by doing so strengthen their effectiveness as scientists. Scope This classes focuses on linear statistical models, that is, regression and ANOVA. These linear models are the backbone of techniques for analyzing designed experiments in science. These methods can also be, and often are, used to analyze so-called observational data, that is, data collected outside an experimental context.1 Of course, most graduate students will need to learn specialized statistical methods that are popular in their own field of study. These notes are not meant to cover these specialized methods, and thus they are not meant to embody the whole of statistics. However, study of regression and ANOVA provides an opportunity to master core tools and provides a springboard to the study of more specialized and possibly discipline-specific methods. This class deals exclusively with so-called frequentist statistical inference. We do not engage Bayesian methods. This class is also firmly situated in the study of low-dimensional statistical models. We value parsimony, and we take the view that well constructed models are worthy objects of study in their own right. More concretely, we seek to construct statistical models with parameters that correspond to natural phenomena of interest. This view exists in some tension with the contrasting big data culture of statistics, which places more emphasis on the detection of patterns and prediction of future observations, and is less concerned with unpacking the black box that characterizes those patterns (Breiman (2001)). Assumed math background ST 512 does not assume knowledge of or use any math beyond arithmetic and basic probability. At NCSU, it is assumed that students have learned the necessary probability background in a first statistics class like ST 511. This basic probability includes an understanding of random variables, standard distributions  primarily Gaussian (normal) distributions, but also binomial and Poisson  and basic properties of means and variances. Students who are willing to engage the math a bit more deeply will find that doing so provides a more satisfying path through the material and leads to a more durable understanding. Without knowing the math underneath, one can only learn statistical methods as different recipes in a vast cookbook, a tedious task that taxes the memory and gives statistics courses their reputation for drudgery. For those who are so inclined, learning a bit of the mathematical theory reveals how the methods we study connect with one another, and thus provides a scaffolding to organize the methods sensibly and coherently. Moreover, the underpinning mathematics can be understood with a minimum of calculus. Linear algebra, however, is more essential. Indeed, the linear models that we study are, ultimately, exercises in linear algebra. This course assumes no previous familiarity with linear algebra, and so we will not emphasize the linear algebra underpinnings of the methods. In the fullness of time, I hope that these notes will eventually include sidebars that present the linear algebra underneath the methods, for interested readers. In this day and age, one might ask why its necessary to understand the math at all. Indeed, the internet makes it easy to find quickly code for any standard analysis. Moreover, while the robots takeover of humanity may or may not be imminent, we are probably not too far away from the rise of artificial intelligence-based statistical consulting, where anyone can upload a data set and answer a few questions about it in return for an AIs analysis. In such a world, the primary task facing an analyst is not so much to get the computer to give you an answer, but instead to confirm that the answer is in fact the one you want. Towards this end, knowing a bit about the math behind the methods makes it possible to determine whether the computer output youve obtained is indeed the analysis you hoped for. Throughout, we will try to emphasize simple, quick calculations that can be used to verify that computer output is correct, or indicate that something needs to be fixed. Computing ST 512 serves a broad clientele. Some students will be better served learning R, and others will be better served learning SAS. R and SAS code are used throughout these notes, but the notes are not meant to provide fully fledged instruction in R or SAS. For students in the class, instruction in R and SAS will be provided in the labs associated with the course. Eventually, I hope that these notes will eventually include computing companions that discuss implementation of the methods in both R and SAS. Format of these notes Material that appears in gray text and is offset by horizontal rules (like the one below this paragraph) is enrichment material to provide a deeper exploration for interested readers. This material may be skipped without loss. This structure is inspired by Mas-Colell et al. (1995)s lovely text. Bibliography "],["simple-linear-regression.html", "Chapter 1 Simple linear regression 1.1 The basics of SLR 1.2 Least-squares estimation 1.3 Inference for the slope 1.4 Sums of squares decomposition and \\(R^2\\) 1.5 Fitting the SLR model in R 1.6 Diagnostic plots 1.7 Consequences of violating model assumptions, and possible fixes 1.8 Prediction with regression models", " Chapter 1 Simple linear regression In statistics, regression models are those that relate the distribution of an output variable to the value(s) of one or several input variables. Characterizing the relationships among input and output variables is central to much of science, and indeed regression methods form the foundation for much of data analysis. Well have a lot to say about regression, but well begin with so-called simple linear regression (SLR). SLR models are simple in the sense that they contain only one predictor. In ST 512, we assume that you have encountered SLR models before. The purpose of our study here is twofold. First, we hope to understand the SLR model at a reasonably deep level. Much of this understanding will carry over to our study of more complicated models, so it is helpful to establish it here in a less complicated setting. Second, we will review many of the foundational concepts of (frequentist) statistical inference. While this should not be your first exposure to these ideas, its worth our time to review them again to solidify our understanding. 1.1 The basics of SLR Simple linear regression characterizes the relationship between two variables: a predictor variable and a response variable. We will begin with a simple example for context. Example: Individuals in this study consumed a certain number of beers, and their blood alcohol content (BAC) was measured. Data were obtained for \\(n=16\\) individuals. Here is a scatter plot of the data: Figure 1.1: BAC vs. beers consumed. To begin, lets observe that the two variables that a regression model associates are not on equal footing. One variable is designated as the predictor and the other variable is designated as the response. The predictor variable is denoted by the symbol \\(x\\), and the response variable is denoted by \\(y\\). In plotting, we almost always show the predictor on the horizontal axis and the response on the vertical axis.2 The predictor is also called the independent variable because, in a designed experiment, its values are determined by the investigator. The response is also called the dependent variable because its distribution depends on the value of the predictor variable, in a way that is determined by nature. For the BAC data, we will identify the number of beers consumed as the predictor and BAC as the response. The regression model associates each value of the predictor variable with a distribution for the response variable. Indeed, the fact that the output of the model is a distribution is what makes this a statistical model, as opposed to some other flavor of mathematical model. A simple linear regression (SLR) is a simple statistical model in which the association between the value of the predictor and the distribution of the response takes a specific form. In particular, in a SLR, the distribution of the response variable is Gaussian (or normal) with a mean that depends linearly on the value of the predictor and a variance that is independent of the value of the predictor. When we plot the fit of a regression model, we typically only plot the regression line. However, the line merely shows how the average of the distribution of the response depends on the predictor. The model has more structure than a plot of the regression line suggests. In terms of an equation, we can write the model using the regression equation \\[\\begin{equation} y_i =\\beta_0 +\\beta_1 x_i +\\varepsilon_i \\tag{1.1}. \\end{equation}\\] In words, we might re-write the equation as \\[ \\mbox{response = intercept + slope} \\times \\mbox{predictor + error}. \\] In the mathematical equation above, the i subscript distinguishes individual data points. For example, \\(y_1\\) is the value of the response associated with the first observation in the data set. Usually, we use the notation \\(n\\) for the total number of data points, and so to be precise we might also write \\(i = 1, \\ldots, n\\). In words, we say that \\(i\\) varies from 1 to \\(n\\) or \\(i\\) ranges from 1 to \\(n\\). Well suppress the \\(i\\) subscript when we dont need it. In the SLR model, the equation \\(\\beta_0 + \\beta_1 x\\) shows how the average of the response depends on the predictor value. The parameter \\(\\beta_0\\) is called the intercept, and it gives the value of the regression line when the predictor \\(x = 0\\). As we will see, the value of the regression line at \\(x=0\\) often isnt a scientifically meaningful quantity, even though we need to know the value to specify the model fully. The parameter \\(\\beta_1\\) is the slope. In SLR, the slope is a parameter tells us by how much regression line rises or falls as the predictor changes. Positive values of the slope indicate that the regression line increases as the predictor increases, and negative values of the slope indicate that the regression line decreases as the predictor increases. The regression line alone is not sufficient to fully specify the entire regression model. To the regression line we add a normally distributed error, denoted by \\(\\varepsilon\\). The error term is a catch-all that subsumes all the other factors that might influence the response that are not included in the predictors. In the context of the BAC example, these might include body weight, metabolism, and/or alcohol content of the beer (if it differed among subjects). Although they look similar, it is important to realize that \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\varepsilon\\) are different beasts. The quantities \\(\\beta_0\\) and \\(\\beta_1\\) are parameters. Recall that in statistics, parameters are quantities that characterize a population. We assume that true values of \\(\\beta_0\\) and \\(\\beta_1\\) exist; those values are just unknown to us. We will estimate these parameters and draw inferences about their values on the basis of data. In contrast, the error term \\(\\varepsilon\\) is a random variable. It does not have one single value, but instead takes a different value for every member of a population. We describe the distribution of the errors across the members of the population using a probability distribution. In simple linear regression, we assume that the random errors have a Gaussian (or normal, or bell-shaped) distribution with mean 0 and variance \\(\\sigma_{\\varepsilon}^{2}\\). We also assume that the random errors are independent among individuals in our sample. A succinct way of stating this is to state that the errors are Gaussian and independent and identically distributed (abbreviated iid). In notation, we write \\(\\varepsilon_{i} \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}\\left(0, \\sigma_{\\varepsilon }^2 \\right)\\), a statement which we would read as the errors have a normal (or Gaussian) distribution with mean 0 and variance \\(\\sigma^2_\\varepsilon\\). The error variance \\(\\sigma_{\\varepsilon }^2\\) is a parameter, and it measure of the variability in the response that is not explained by the predictor. We will also discuss how to estimate \\(\\sigma_{\\varepsilon }^2\\). (It is also possible to draw statistical inferences for \\(\\sigma_{\\varepsilon }^2\\), although we will not discuss how to do so in ST512.) Before moving on to discussing how to estimate the model parameters, lets reflect a bit on the slope, \\(\\beta_1\\), because this is the parameter that captures the linear association between the two variables. I recently learned of a particularly nice way to interpret the slope, due to Gelman, Hill, and Vehtari (2020). Their interpretation works like this. Consider two values of the response \\(y_1\\) and \\(y_2\\), associated respectively with two values of the predictor \\(x_1\\) and \\(x_2\\). The regression model says that, on average, the difference \\(y_1 - y_2\\) will equal \\(\\beta_1 \\times (x_1 - x_2)\\). The on average part of this interpretation is important because we realize that any two actual observations will also include their respective errors, and so we dont expect these two observations to differ by exactly \\(\\beta_1 \\times (x_1 - x_2)\\). Second, this interpretation also makes it clear that the regression model predicts that the average difference between two responses will increase or decrease linearly as the difference between their two associated predictor values grows or shrinks. Thus, if the SLR model is appropriate for the BAC data (something we have yet to verify), then the model suggests that the average BAC difference between two individuals who have consumed 1 vs. 2 beers is the same as the average BAC difference between two individuals who have consumed 4 vs. 5 beers, and that both of these differences are one-half as big as the average BAC difference between two individuals who have drank 2.5 vs. 4.5 beers. Our assumption of normally distributed errors has a deeper justification than may meet the eye. If youve studied probability, you may have encountered an important result called the Central Limit Theorem. For our purposes, the Central Limit Theorem tells us that if the error results from the combined effect of many small factors added together, then the errors distribution will be approximately normal. (We will see that regression models are not sensitive to moderate departures from normality, so approximately normal errors are good enough.) This result provides a strong justification for expecting normally distributed errors in many cases. The normality assumption begins to break down when the errors are dominated by only a few factors, or when the factors that contribute to the error combine multiplicitavely. This latter scenario  errors that result from the product of many small influences as opposed to their sum  frequently arises in biology when the response measures some form of population size. Populations grow or shrink multiplicitavely, and so population sizes tend to have right-skewed distributions. We might also note that the style of writing the regression model as the sum of the regression line (\\(\\beta_0 + \\beta_1 x\\)) and an error term with mean zero (\\(\\varepsilon\\)) works because we have assumed that the errors have a normal distribution. A normal distribution has the special property that we can take a normally distributed quantity, add a constant to it, and the sum will still have a normal distribution. Most statistical distributions do not have this property; for example, a Poisson random variate plus a non-zero constant does not yield a Poisson distributed sum. Some authors find it more natural to write the SLR model as \\(y \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x, \\sigma^2)\\), to emphasize that the response has a Gaussian distribution and that the predictor only affects the mean of this distribution. We will use the style of eq. (1.1), because this style lends itself more readily to mixed-effects models with multiple variance terms. However, the two styles of notation denote the same model. Feel free to use whichever style makes the most sense to you. 1.2 Least-squares estimation The parameters of an SLR model are estimated by the method of least-squares. That is, we find the values of the parameters that minimize the sum of the squared differences between the data points themselves and the line. The estimates are denoted by hats, i.e., \\(\\hat{\\beta}_0\\) is the estimate of \\(\\beta_0\\). Other authors use \\(b\\)s instead of \\(\\widehat{\\beta}\\)s for parameter estimates in regression. Both types of notation commonly appear in the scientific literature. If we were inventing SLR from scratch, we might imagine many possible criteria that we could use to determine the parameter values that provide the best fit of the SLR model. For example, we might contemplate fitting the line that minimized the average absolute difference between the data points and the line. The reason why we favor the least-squares criterion is a direct consequence of the assumption that the errors take a Gaussian distribution.3 In ST511 or a similar course, you may have derived formulas for calculating the least-squares estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) by hand. In ST512, we will use a computer, although one might note that one could derive the formulas for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) by using basic calculus tools to minimize the error sum-of squares. Using R, we obtain the least-squares fit of the regression model to the BAC data below.4 fm1 &lt;- with(beer, lm(BAC ~ Beers)) summary(fm1) ## ## Call: ## lm(formula = BAC ~ Beers) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.027118 -0.017350 0.001773 0.008623 0.041027 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.012701 0.012638 -1.005 0.332 ## Beers 0.017964 0.002402 7.480 2.97e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02044 on 14 degrees of freedom ## Multiple R-squared: 0.7998, Adjusted R-squared: 0.7855 ## F-statistic: 55.94 on 1 and 14 DF, p-value: 2.969e-06 The least-squares estimates of the intercept and slope for the BAC data are \\(\\hat{\\beta}_0 = -0.013\\) and \\(\\hat{\\beta}_1 = 0.018\\), respectively. Heres a picture of the scatter-plot with the least-squares line: with(beer, plot(BAC ~ Beers, xlab = &quot;beers consumed&quot;)) abline(fm1) Figure 1.2: SLR fit of BAC vs. beers consumed. The best fitting line shows a positive relationship between BAC and beers consumed. Using the interpretation that we introduced in the previous section, we would say that if we measure the BAC of two people, one of whom has consumed one more beer than the other, on average the BAC of the person who drank more beers will be 0.018 higher than the BAC of the persion who drank fewer beers. Similarly, if we compare the BAC of two people, one of whom drank four more beers than the other, on average the BAC of the person who drank more beers will be \\(0.4 \\times 0.018 = 0.072\\) higher than the person who drank fewer beers, and so on. Evaluating the fitted regression line for a given value of the predictor generates a fitted value for each data point. Fitted values are denoted \\(\\hat{y}_i\\). In notation, \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\). Why did the error term vanish in the equation for \\(\\hat{y}_i\\)? The residual for observation \\(i\\), denoted \\(e_i\\), is the difference between the actual observation and the fitted value. In notation, \\(e_i = y_i -\\hat{y}_i\\). In terms of the data plot, the residuals can be thought of as the vertical differences between the actual data points and the fitted line. In the figure below, the vertical line represents the residual for the individual who consumed 9 beers. Example: The first individual in the data set drank \\(x_1 = 5\\) beers and had a BAC of \\(y_1 = 0.1\\). Find the fitted value and residual for this data point. Answer: \\(\\hat{y}_1 = 0.077\\), \\(e_1 = 0.023\\). The error sum of squares (SSE) is \\[ SSE = \\sum_{i=1}^{n} e_i^{2} = \\sum_{i=1}^{n} \\left(y_{i} - \\hat{y}_i \\right)^{2}. \\] The SSE is a measure of the unexplained variability in the data. The least squares estimates, \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), are called the least squares estimates because they minimize the SSE. We can use the SSE to find an estimate of the error variance parameter by using the formula \\[ \\widehat{\\sigma}_\\varepsilon^2 = s_\\varepsilon^2 = \\dfrac{SSE}{n-2} = MSE \\] We divide by \\(n - 2\\) because there are \\(n - 2\\) degrees of freedom (df) associated with the SSE. When we divide an error sum-of-squares by its degrees of freedom, the resulting quotient is called the mean-squared error (MSE). For the BAC data, the SSE is 0.0058, yielding a MSE of \\(0.0058/(16-2) \\approx 0.0004\\). See the gray text at the end of this section for an explanation of why the number of degrees of freedom is \\(n-2\\). Variances are difficult to understand because they are on a squared scale. Thus, the units of the error variance are the units of the response, squared. To place this estimate on a more meaningful scale, we take the square root to obtain the estimate of the residual standard deviation \\(s_{\\varepsilon}\\): \\[ s_{\\varepsilon} =\\sqrt{\\dfrac{SSE}{n-2}} = \\sqrt{MSE} \\] For the BAC data, \\(s_{\\varepsilon} = \\sqrt{0.0004} = 0.020\\). This is a more useful number, as it suggests that a typical deviation between an observed BAC and the corresponding fitted value is 0.020%. (Take a look again at the magnitude of the residuals in the scatterplot of the BAC data, and convince yourself that 0.020% is a reasonable guide to the magnitude of a typical residual.) Degrees of freedom appear frequently in statistical modeling. We will spend quite a bit of effort in ST512 keeping track of degrees of freedom, so its helpful to understand this concept well. Well look carefully at df in the simple case of SLR to build intuition that will carry over into more complicated models. Most error terms, like the SLR error variance \\(\\sigma_\\varepsilon^2\\), are estimated by sums of squares. The concept of degrees of freedom quantifies how many `free differences are available to compute a sum of squares. Consider the following thought experiment. Suppose that, bizarrely, we knew the values of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in an SLR, and only needed to estimate the error variance \\(\\sigma_\\varepsilon^2\\). We could do so using the sum of squares \\(\\sum_{i=1}^{n}\\left(y_{i} -\\left[\\beta_0 +\\beta_1 x_i \\right]\\right)^2\\). In this case, each of our \\(n\\) data points would contribute a `free difference to the summation above, and so there would be \\(n\\) free differences with which we could estimate the error variance \\(\\sigma_\\varepsilon^2\\). However, we never know the values of \\(\\beta_0\\) and \\(\\beta_1\\) in advance. Instead, we have to use the data to estimate both \\(\\beta_0\\) and \\(\\beta_1\\). Now, because we have to estimate both \\(\\beta_0\\) and \\(\\beta_1\\), there are only \\(n - 2\\) free differences in the sum of squares \\(\\sum_{i=1}^{n}\\left(y_{i} -\\left[\\hat{\\beta}_0 +\\hat{\\beta}_1 x_i \\right]\\right)^{2}\\). One way to visualize this is to imagine fitting a line to a data set with only \\(n = 2\\) data points (with different \\(x\\) values). The line would be guaranteed to pass through both points, and consequently both residuals would equal 0. Because both residuals equal 0, the SSE would also equal 0. However, the SSE doesnt equal 0 because the actual value of \\(\\sigma_\\varepsilon^2\\) equals 0. Instead, the SSE equals 0 because there is no information remaining to estimate the residual variance. In general, when we have to use the same data set to estimate the parameters of the signal component to estimate a variance parameter, then each parameter that we have to estimate in the mean component eliminates a free difference from the sum of squares \\(\\sum_{i=1}^{n}\\left(y_{i} -\\hat{y}_{i} \\right)^{2}\\). To convert the sum of squares into an estimate of an error variance, we need to count the number of free differences (or degrees of freedom) correctly, and divide the sum of squares by the appropriate number of df to make sure we get a valid variance estimate. 1.3 Inference for the slope To draw statistical inferences about the slope parameter \\(\\beta_1\\), we make the following assumptions: Linearity: The population mean \\(\\mu \\left(x\\right)\\) is a linear function of \\(x\\). Equal variance (homoscedasticity): The variance of the error terms (the \\(\\varepsilon_i\\)s) is the same for all observations. Independence: The error terms are independent of one another. Normality. The errors have a normal (i.e., bell-shaped, or Gaussian) distribution. Note that assumption number 1 deals with the mean component of the model, while assumptions 24 deal with the error component of the model. 1.3.1 Standard errors As intelligent scientists, we realize that estimates are not exactly equal to the parameters that they seek to estimate. We can characterize the uncertainty in parameter estimates in different ways. One tool that we have for quantifying uncertainty in parameter estimates is to calculate a standard error. In general, a standard error quantifies the variability in an estimate that is attributable to random sampling. Most parameter estimates that we will encounter have known formulas for their standard errors. In most cases, these formulas are complicated, and we will rely on computers to calculate standard errors for us. However, the formula for the standard error of the slope parameter in SLR is interesting to examine because it contains a valuable insight that we can use when collecting data for a regression study. The standard error of \\(\\hat{\\beta}_1\\), denoted \\(s_{\\hat{\\beta}_1}\\), is given by the formula \\[ s_{\\hat{\\beta}_1} = \\dfrac{s_{\\varepsilon}}{\\sqrt{S_{xx} } } \\] where \\(S_{xx} =\\sum_{i}\\left(x_{i} -\\bar{x}\\right)^2\\) quantifies the dispersion in the predictor variables. Although this formula looks a bit daunting, theres some intuition to be gained here, and a lesson for experimental design. Suppose we had designed a regression experiment in which all of the individuals were assigned similar values of the predictor. In this case, \\(S_{xx}\\) would be small, and consequently the standard error \\(s_{\\hat{\\beta}_1}\\) would be large. Conversely, if the values of the predictor were very different among individuals in the study, then \\(S_{xx}\\) would be large and the standard error \\(s_{\\hat{\\beta}_1}\\) would be small. Thus, if we want a precise estimate of the slope, we should choose predictor values that span the range over which we want to learn. Thought question: Following this line of reasoning, is it a good idea to design a study so that half the individuals are assigned a very large value of the predictor, and the other half are assigned a very small value? Why or why not? For the BAC example, \\(s_{\\hat{\\beta}_1} = 0.0024\\). 1.3.2 Confidence intervals A second way in which we can measure the uncertainty in a parameter estimate is to calculate a confidence interval (CI). Recall that the general formula for a confidence interval associated with a statistic is: \\[ \\mathrm{estimate} \\pm \\mathrm{critical\\ value} \\times \\mathrm{standard\\ error} \\] Critical values are found either by consulting a table (and re-living the good old days) or using the internet or a computer program. Critical values depend on the {} that you want to associate with the CI. Although it seems a bit backwards, we typically denote the confidence level of a CI as \\(100 \\times \\left(1-\\alpha \\right)\\%\\). Thus, for a 95% confidence interval (a common choice), \\(\\alpha = 0.05\\). Alternatively, we might seek a 99% CI, in which case \\(\\alpha = 0.01\\). To construct a CI for \\(\\beta_1\\) , we find the appropriate critical values from a \\(t\\)-distribution with \\(n - 2\\) df. For a \\(100\\times \\left(1-\\alpha \\right)\\%\\) CI, the critical value is the value that ``cuts-off an upper tail of \\(\\alpha / 2\\) %. For example, to calculate a 99% CI for \\(\\beta_{1}\\), we need to find the critical value of a \\(t\\)-distribution with 14 df that cuts-off an upper 0.5%-tail. Using an online calculator, or another tool, we find that this critical value is 2.977. Thus, a 99% CI is 0.018 \\(\\pm\\) 2.977 \\(\\times\\) 0.0024 = (0.011, 0.025). Recall that the appropriate interpretation of the confidence level a CI is fairly tricky. A proper interpretation is that, if we were to repeat this experiment a large number of times, and calculate a 99% CI for each experiment, in the long run 99% of those CIs would contain the true value of \\(\\beta_1\\). Of course, in real life, well only do the experiment once, and we dont know if our experiment is one of the 99% in which the CI contains the true parameter value or not. It is often tempting to abbreviate this interpretation by saying that ``there is a 99% chance that \\(\\beta_1\\) is in the CI, although technically this interpretation is incorrect (because any single CI either contains the parameter or it doesnt). Note also that there is a trade-off between the confidence level and the width of the interval. If we wanted greater confidence that our interval contained the true parameter value, we could increase the confidence level. However, increasing the confidence level increases the width of the interval, and thus provides less information about the true parameter value in some sense. If we follow this argument to its (il)logical extreme, a 100% CI for \\(\\beta_1\\) covers the interval from negative infinity to positive infinity. Now we are fully confident that our interval contains \\(\\beta_1\\), but at the cost of having no information whatsoever about the actual value of \\(\\beta_1\\). 1.3.3 Statistical hypothesis tests Finally, a third way to characterize the statistical uncertainty in $_1 $ is to conduct a statistical hypothesis test. Recall that statistical hypotheses are statements about the values of unknown parameters, and a statistical hypothesis test is a way to measure the strength of evidence against a `null hypothesis. In the context of SLR, we are almost always interested in testing the null hypothesis that the true value of the slope parameter is equal to zero. In notation, we write this as \\(H_0: \\beta_1 = 0\\). Evidence against this null hypothesis is taken as evidence that the predictor is linearly related to the response. Recall that in statistical hypothesis testing, we must also specify an alternative hypothesis. In SLR, we are almost always interested in testing \\(H_0: \\beta_1 = 0\\) vs. the two-sided alternative \\(H_a: \\beta_1 \\ne 0\\). We conduct a statistical hypothesis test by first calculating a test statistic. In general, formulas for test statistics take the form: \\[ \\mbox{test statistic} = \\dfrac{\\mbox{parameter estimate} - \\mbox{value of parameter under }H_0} {\\mbox{standard error}} \\] Test statistics have the property that if the null hypothesis is true, then the test statistic has a known sampling distribution. In the case of testing \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\) in SLR, if the null hypothesis is true, then the test statistic will have a \\(t\\)-distribution with \\(n-2\\) df. In notation, the test statistic is \\[ t=\\frac{\\widehat{\\beta}_{1} -0}{s_{\\widehat{\\beta }_{1} } } =\\frac{\\widehat{\\beta }_{1} }{s_{\\widehat{\\beta }_{1} } } \\] In SLR, this test is so common that the value of the \\(t\\)-statistic is provided automatically by most statistical software packages, including R. For the BAC data, the \\(t\\)-statistic associated with the test of \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\) is \\(t = 7.48\\). Values of the test statistic by themselves are not terribly enlightening. Instead, we use the test statistic to find a \\(p\\)-value. \\(P\\)-values are famously difficult to interpret, and those difficulties in interpretation have impeded their proper use. In 2016, a blue-ribbon panel of experts were convened by the American Statistical Association (the leading professional organization for statisticians in the US) to take the remarkable step of issuing a policy statement regarding the use of \\(p\\)-values. That statement (Wasserstein &amp; Lazar, 2016, 70:129-133) defines a \\(p\\)-value as follows: ``Informally, a \\(p\\)-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value. (Bear in mind that this definition is the work of two dozen of the worlds leading statisticians.) In the context of the test of \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\) in SLR, this means finding the probability that a \\(t\\)-statistic with \\(n-2\\) df is at least as different from zero as the value observed. For a two-sided alternative hypothesis, we say ``different from zero because the sign (positive vs. negative) of the \\(t\\)-statistic is irrelevant. Be careful, though: for a one-sided alternative hypothesis, the sign of the observed \\(t\\)-statistic is critical! For the BAC data, we find the area under the tail of a \\(t\\)-distribution with 14 df that is greater than 7.48, and then (because the \\(t\\)-distribution is symmetric) multiply by 2. That is, \\[\\begin{align*} p &amp; = \\mathrm{Pr}\\!\\left\\{ t_{14} &lt; -7.48\\right\\} +\\mathrm{Pr}\\!\\left\\{ t_{14} &gt; 7.48\\right\\} \\\\ &amp; = 2 \\times \\mathrm{Pr}\\!\\left\\{ t_{14} &gt;7.48 \\right\\} \\\\ &amp; = 3\\times 10^{-6} \\end{align*}\\] Thus, there is exceedingly strong evidence that BAC is related to the number of beers consumed. The values above could be found by consulting a table, or by using statistical software such as R. Because the test of \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\) is sufficiently common in SLR, most computer packages will do this calculation for us. Well sweep a lot of acrimonious debate about statistical hypothesis testing under the rug and simply say that some scientists like to make a decision about whether or not to reject the null hypothesis. Although R.A. Fisher (a founding father of statistics) would roll over in his grave if he heard us saying this, most scientists make these reject or do not reject decisions by comparing the \\(p\\)-value to the tests {}, which is usually denoted by the Greek letter \\(\\alpha\\). The significance level of a test is the frequency with which one would erroneously reject a true null hypothesis; you might also think of it as the allowable false-positive rate. Consequently, tests with smaller significance levels require more evidence against the null to reject it (this sounds backwards at first, but makes sense when you think about it). Most scientists conventionally make reject / do not reject decisions with a significance level of \\(\\alpha = .05\\), but you are free to use whatever significance level you deem appropriate. If \\(p \\le \\alpha\\), we reject the null hypothesis; otherwise, we fail to reject it. (Remember that we never `accept the null hypothesis. We only fail to reject it.) Although it is rare, we can also entertain so-called one-sided alternative hypotheses. For example, suppose that we were uninterested in the (somewhat nonsensical) possibility that the numbers of beers consumed decreased BAC, and only were interested in measuring the evidence that the numbers of beers consumed increases BAC. To do so, we might test the same null hypothesis \\(H_0: \\beta_1 = 0\\) vs. the one-sided alternative \\(H_a: \\beta_1 &gt; 0\\). To conduct this test, the test statistic is still \\[ t=\\dfrac{\\widehat{\\beta }_{1} -0}{s_{\\widehat{\\beta }_{1} } } =\\dfrac{0.0180}{0.0024} =7.48. \\] However, because the alternative hypothesis is one-sided, to calculate a \\(p\\)-value, we interpret ``equal to or more extreme than its observed value\" as the probability of observing a test statistic than 7.48, i.e., \\[ p=\\mathrm{Pr}\\!\\left\\{ t_{14} &gt;7.48\\right\\} =1.5\\times 10^{-6} \\] We would then reject \\(H_0: \\beta_1 = 0\\) in favor of the one-sided alternative \\(H_a: \\beta_1 &gt; 0\\) at the \\(\\alpha = .05\\) significance level. Finally, although it doesnt make much sense in terms of what we know about alcohol, we could consider testing \\(H_0: \\beta_1 = 0\\) vs. the one-sided alternative \\(H_a: \\beta_1 &lt; 0\\). Again, the test statistic is the same (\\(t\\) = 7.48), but now evidence against the null and in favor of the alternative is provided by negative values of the test statistic, so the p-value is the probability of observing a test statistic than 7.48, i.e., \\[ p=\\mathrm{Pr}\\!\\left\\{ t_{14} &lt; 7.48\\right\\} = 1 - \\mathrm{Pr}\\!\\left\\{ t_{14} &gt; 7.48\\right\\} \\approx 0.9999. \\] Thus, there is no evidence that would allow us to reject \\(H_0: \\beta_1 = 0\\) in favor of the one-sided alternative \\(H_a: \\beta_1 &lt; 0\\). One final note: Although it is rarely done, there is no reason why we must restrict ourselves to testing \\(H_0: \\beta_1 = 0\\). We could in fact test any null hypothesis. For example, suppose conventional wisdom held that each additional beer consumed increased BAC by 0.02, and we were interested in asking if these data contain evidence that the conventional wisdom is false. Then we could test \\(H_0: \\beta_1 = 0.02\\) vs. \\(H_a: \\beta_1 \\ne 0.02\\), although we have to calculate the test statistic and \\(p\\)-value manually instead of relying on computer output: \\[\\begin{align*} t &amp; = \\dfrac{\\hat{\\beta}_1 -0.02}{s_{\\hat{\\beta}_1 }} \\\\ &amp; = \\dfrac{0.0180-0.02}{0.0024} \\\\ &amp; =-0.83 \\\\ \\\\ p &amp; = \\mathrm{Pr}\\!\\left\\{t_{14} &lt;-0.83\\right\\} +\\mathrm{Pr}\\!\\left\\{t_{14} &gt;0.83\\right\\} \\\\ &amp; = 2 \\times \\mathrm{Pr}\\!\\left\\{ t_{14} &gt;0.83\\right\\}\\\\ &amp; = 0.421. \\end{align*}\\] Thus, these data contain no evidence that would allow us to reject \\(H_0: \\beta_1 = 0.02\\) vs. \\(H_a: \\beta_1 \\ne 0.02\\) at any reasonable significance level. Confusion alert: Do be mindful of the distinction between a statistical hypothesis and a scientific hypothesis. The following excerpt from an article by B. Dennis and M.L. Taper puts it nicely: ``A statistical hypothesis is an assumption about the form of a probability model, and a statistical hypothesis test is the use of data to make a decision between two probability models. A scientific hypothesis, on the other hand, is an explanatory assertion about some aspect of nature. (Dennis and Taper (1994)). Thus, while a statistical hypothesis can often embody a scientific hypothesis, a scientific hypothesis does not always boil down to a statistical hypothesis. 1.3.4 Inference for the intercept Most statistical packages automatically provide the standard errors for the intercept, \\(s_{\\hat{\\beta}_0}\\), as well as a test of \\(H_0: \\beta_0 = 0\\) vs. \\(H_a: \\beta_0 \\ne 0\\). Sometimes this is a meaningful test, but usually it isnt. The scientific context of the problem will determine whether or not it makes sense to pay attention to this test. There is a special type of regression called ``regression through the origin that is appropriate when we can assume \\(\\beta_0 = 0\\) automatically. Should we use regression through the origin for the BAC example? 1.4 Sums of squares decomposition and \\(R^2\\) We have already seen that the SSE measures the unexplained variability in the response. \\[ {\\rm SSE}=\\sum _{i=1}^{n}e_{i}^{2} = \\sum _{i=1}^{n}\\left(y_{i} -\\hat{y}_{i} \\right)^{2} \\] We can also define the total sum of squares, SS(Total): \\[ {\\rm SS(Total)}=\\sum _{i=1}^{n}\\left(y_{i} -\\bar{y}\\right)^{2} \\] SS(Total) is a measure of the total variability in the response. Finally, we can define the regression sum of squares, SS(Regression), as \\[ {\\rm SS(Regression)}=\\sum _{i=1}^{n}\\left(\\hat{y}_{i} -\\bar{y}\\right)^{2} \\] SS(Regression) measures the variability in the response that is explained by the regression. The regression sum of squares is also called the model sum of squares, or SS(Model). By a small miracle (actually, by the Pythagorean Theorem), it happens to be true that: \\[ {\\rm SS(Total)=SS(Regression)+SSE} \\] The {}, or \\(R^2\\), is the proportion of the variability in the response explained by the regression model. The formula for \\(R^2\\) is \\[ R^2 = \\dfrac{{\\rm SS(Regression)}}{{\\rm SS(Total)}} = 1-\\frac{{\\rm SSE}}{{\\rm SS(Total)}} . \\] \\(R^2\\) is a nice metric because it quantifies how much of the variability in the response is explained by the predictor. Values of \\(R^2\\) close to 1 indicate that the regression model explains much of the variability in the response, while values of \\(R^2\\) close to 0 suggest the regression model explains little of the variability in the response. Well also see that \\(R^2\\) is not limited to SLR and in fact has the same interpretation for more complicated regression models that we will examine later. For the BAC example, \\(R^2\\) = 0.80, suggesting that variation in beers consumed explains roughly 80% of the variation in BAC. Mathematically, \\(R^2\\) can also be computed as square of the sample correlation coefficient between the fitted values and the response. In SLR, the fitted values and the predictor are perfectly correlated with one another, so \\(R^2\\) is also the square of the sample correlation coefficient between the predictor and the response. 1.5 Fitting the SLR model in R In this section of ST512, well learn a little about both R and SAS. Well be using R for the regression component of the course. The basic command in R for fitting a regression model is the function lm, short for [l]inear [l]odel. (As the name suggests, the `lm function can be used for more than just SLR.) The basic syntax is &gt; lm(response ~ predictor) where response and predictor would be replaced by the appropriate variable names. The &gt; is the R prompt, and is meant to show what you could type at the command line. Although the above command would work, it would fit the SLR and then forget the model fit. We want to keep the model fit around to analyze it, so well store it in memory under a name of our choosing. Here, well choose the name fm1, although any name would work. Anything proceeded by a pound sign (#) is a comment in R. Well assume that the BAC data have already been read into R and reside in memory, and that the variables in the BAC data are named BAC and Beers. Here is code for fitting a SLR model to these data: fm1 &lt;- lm(BAC ~ Beers, data = beer) # The &#39;&lt;-&#39; is the assignment operator. # Here, the output produced by the call to &#39;lm&#39; is stored in memory under # the name &#39;fm1&#39;. We can learn about &#39;fm1&#39; by asking for a summary. summary(fm1) ## ## Call: ## lm(formula = BAC ~ Beers, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.027118 -0.017350 0.001773 0.008623 0.041027 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.012701 0.012638 -1.005 0.332 ## Beers 0.017964 0.002402 7.480 2.97e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02044 on 14 degrees of freedom ## Multiple R-squared: 0.7998, Adjusted R-squared: 0.7855 ## F-statistic: 55.94 on 1 and 14 DF, p-value: 2.969e-06 Lets examine each portion of the R output above. The portion labeled Call simply tells us what command was used to generate the model. The portion labeled Residuals tells us a five-number summary (minimum, first quartile, median, third quartile, and maximum) of the residuals. The portion labeled Coefficients gives us a table of parameter estimates and standard errors. Each row of the table corresponds to a single parameter. The row labeled (Intercept) obviously corresponds to the intercept. The row labeled with the name of the predictor gives information about the slope parameter. In addition to parameter estimates and standard errors, R (like many computer packages) also automatically generates hypothesis tests of \\(H_0: \\beta_0 = 0\\) vs. \\(H_a: \\beta_0 \\ne 0\\) and \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\). It is up to you, the user, to determine whether or not these tests are informative. Finally, the last block of output provides a variety of additional information. The residual standard error (perhaps not the best terminology) is the estimate of the residual standard deviation, \\(s_{\\varepsilon}\\). R also provides two different \\(R^2\\) values; the \\(R^2\\) that we discussed above is labeled as the Multiple R-squared. We will discuss adjusted R-squared later. Finally, the \\(F\\)-statistic corresponds to a `model utility test, which we will discuss in the context of multiple regression. For now, you might notice that in SLR the p-value of the model-utility test is always equal to the p-value for the test of \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\). We will explain why this is so later. The SS decomposition for a regression model is also referred to as the analysis of variance for the regression model. We can use the `anova command in R to obtain the SS decomposition: anova(fm1) ## Analysis of Variance Table ## ## Response: BAC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Beers 1 0.0233753 0.0233753 55.944 2.969e-06 *** ## Residuals 14 0.0058497 0.0004178 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The \\(F\\)-statistic is the model utility test, which we will examine in more detail when we study multiple regression. 1.6 Diagnostic plots We have seen that, in order to draw statistical inferences from a simple linear regression, we need to make several assumptions. Although in everyday life assumptions can get a bad name, assumptions in statistics are necessary and appropriate. The statistician Don Rubin puts it nicely: ``Nothing is wrong with making assumptions  they are the strands that link statistics to science. It is the scientific quality of the assumptions, not their existence, that is critical. (Rubin (2005)). In regression, we can use diagnostic plots to investigate the scientific quality of our assumptions. The main idea of diagnostic plots is that if the assumptions are appropriate, then residuals should be independent draws from a normal distribution with constant variance. Any structure in the residuals indicates a violation of at least one assumption. We list commonly used diagnostic plots below. Although some types of plots are more useful for examining some assumptions than others, there isnt a strict correspondence between plot types and assumptions. Any plot can reveal a departure from any one of our assumptions. Examples of each for the BAC data and the R code used to generate the plots are provided as examples. 1.Residuals vs. fitted values. Check for non-constant variance (trumpeting). The BAC data shown here dont show an obvious increase or decrease in variance as the fitted values increase, although the fact that the largest residual is associated with the largest fitted value is notable. We might want to go back and check that data point out. plot(resid(fm1) ~ fitted(fm1), xlab = &quot;Fitted Values&quot;, ylab = &quot;Residuals&quot;) abline(h = 0, lty = &quot;dotted&quot;) Residuals vs. predictor. We can use this plot to check for non-linear trends. If we see a non-linear trend, like a hump-shaped pattern, it might suggest that the true relationship between predictor and response is actually non-linear. For the BAC data, youll note that the plot below looks exactly like the plot of residuals vs. fitted values above. This isnt just coincidence; in fact, residuals vs. fitted values and residuals vs. predictor will always generate exactly the same patterns in SLR. (The reason is because in SLR the fitted value is just a linear function of the predictor.) We want to get in the habit of checking both types of plots, however, because when we start entertaining multiple predictor variables in multiple regression, the plots will no longer be identical. plot(resid(fm1) ~ beer$Beers, xlab = &quot;Beers&quot;, ylab = &quot;Residuals&quot;) abline(h = 0, lty = &quot;dotted&quot;) Residuals vs. variables not in the model, e.g., other predictors, observer, order of observation. In the BAC data, the only other variable we have (for now at least) is the order in which the observations appear in the data set. Without knowing how the data were collected or recorded, its impossible to say whether this variable is meaningful. However, the plot suggests a distinctive downward trend  data points that appear early in the data set are associated with positive residuals, and data points that appear later in the data set are associated with negative residuals. What do you think might have caused this trend? plot(resid(fm1), xlab = &quot;Order&quot;, ylab = &quot;Residuals&quot;) abline(h = 0, lty = &quot;dotted&quot;) An obvious way to check the normality assumption is to plot a histogram of the residuals. While this is a straightforward idea, it suffers from the fact that the shape of the histogram depends strongly on how the residuals are grouped into bins. Note how the two histograms below of the BAC residuals provide different impressions about the suitability of the normality assumption. hist(resid(fm1), main = &quot;Bin width = 0.01&quot;, xlab = &quot;Residuals&quot;) hist(resid(fm1), main = &quot;Bin width = 0.02&quot;, xlab = &quot;Residuals&quot;, breaks = 4) An alternative to histograms is a normal probability plot of residuals, also known as a quantile-quantile, or Q-Q, plot. Q-Q plots calculate the empirical quantile of each residual, and compare this to the theoretical quantile from a normal distribution. If the normality assumption is appropriate, the empirical and theoretical quantiles will change at the same rate, so when plotted against one another, theyll fall on a line. If the normality assumption is not appropriate, the plot of empirical vs. theoretical quantiles will bend. As well see below, the normality assumption is the critical of the assumptions in regression. Thus, unless the Q-Q plot shows big and dramatic bends, we wont concern ourselves with small bumps and wiggles. The Q-Q plot for the BAC data below doesnt seem terribly problematic. qqnorm(resid(fm1)) qqline(resid(fm1)) 1.7 Consequences of violating model assumptions, and possible fixes Linearity When the linearity assumption is violated, the model has little worth. Whats the point of fitting a linear model to data when the relationship between predictor and response is clearly not linear? The best fix is to fit a non-linear model using non-linear regression. (We will discuss non-linear regression later.) A second-best option is to transform the predictor and / or the response to make the relationship linear. Independence Inference about regression parameters using naive standard errors is not trustworthy when errors are correlated (there is more uncertainty in the estimates than the naive standard errors suggest). The most common source of non-independence is either temporal or spatial structure in the data. Arguably, we have seen this with the BAC data, where one way to think about the downward trend of residuals vs. the order of observation is that residuals close together in time tend to be positively correlated. The best, and easiest, way to accommodate this type of dependence is to include (an)other predictor(s) in the model for time or space. A second-best solution is to use specific methods for time-series data or spatial data, which doable, but is fairly involved, and will require considerable additional study. Constant variance Like violations of the independence assumption, violations of the constant-variance assumption cause inference about regression parameters is not trustworthy. Non-constant variance causes there to be more uncertainty in the parameters estimates than the default CIs or \\(t\\)-tests suggest. There are two possible fixes for non-constant variance. If the non-constant variance arises because the response variable has a known, non-normal distribution, then one can use generalized linear models (such as logistic regression for binary data, or Poisson regression for count data). We will touch on generalized linear models briefly at the end of ST 512. Alternatively, if there is no obvious alternative distribution for the response, the usual approach is to transform the response variable to stabilize the variance. For better or worse, there used to be a bit of a cottage industry in statistics in developing variance-stabilizing transformations. Remember that transformations come with a cost of diminished interpretability, and be wary of exotic transformations. It is not uncommon to observe data where the variance increases as the mean response increases. Good transformations for this situation are either a log transformation or a square-root transformation.5 Another common non-constant variance problem arises when the response is a percentage or a proportion. In this case, the standard and appropriate transformation is the arcsin-square root transformation, i.e., if the observed response is 10%, the transformed response is \\(\\sin^{-1}(\\sqrt{.1})=0.322\\). Normality Perhaps surprisingly, the consequences of violating the normality assuption are minimal, unless departures from normality are severe (e.g., binary data). When one encounters decidedly non-normal data, the usual remedy is to entertain a so-called generalized linear models, i.e., logistic regression for binary data; Poisson regression for count data. Example: Box office take (in millions of US$) vs. a composite rating score from critics reviews: movie &lt;- read.table(&quot;data/movie.txt&quot;, head = T, stringsAsFactors = T) with(movie, plot(BoxOffice ~ Score, xlab = &quot;Average rating&quot;, ylab = &quot;Box office take&quot;)) fm1 &lt;- lm(BoxOffice ~ Score, data = movie) abline(fm1) The plots of residuals vs. fitted value show clear evidence of non-constant variance. The Q-Q plot indicates right-skew. Taking a square-root transformation of the response stabilizes the variance nicely: plot(resid(fm1) ~ fitted(fm1), xlab = &quot;Fitted value&quot;, ylab = &quot;Residual&quot;) abline(h = 0,lty = &quot;dashed&quot;) qqnorm(resid(fm1),main = &quot;QQ plot, movie data&quot;) qqline(resid(fm1)) Lets try a square-root transformation of the response: fm2 &lt;- lm(sqrt(BoxOffice) ~ Score, data = movie) summary(fm2) ## ## Call: ## lm(formula = sqrt(BoxOffice) ~ Score, data = movie) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.60533 -0.17889 -0.07339 0.17983 0.92065 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.114102 0.106000 1.076 0.284 ## Score 0.010497 0.001834 5.722 6.27e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3109 on 138 degrees of freedom ## Multiple R-squared: 0.1918, Adjusted R-squared: 0.1859 ## F-statistic: 32.74 on 1 and 138 DF, p-value: 6.272e-08 plot(fm2) Another commonly used transformation for right-skewed data is the log transformation. Here are residual plots and model output for log-transformed data: fm3 &lt;- lm(log(BoxOffice) ~ Score, data = movie) summary(fm3) ## ## Call: ## lm(formula = log(BoxOffice) ~ Score, data = movie) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.99268 -0.43135 0.00783 0.67263 1.81413 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.634451 0.323390 -8.146 2.01e-13 *** ## Score 0.029984 0.005596 5.358 3.44e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9484 on 138 degrees of freedom ## Multiple R-squared: 0.1722, Adjusted R-squared: 0.1662 ## F-statistic: 28.71 on 1 and 138 DF, p-value: 3.438e-07 plot(fm3) Which transformation do you think is more appropriate? Do the different transformations lead to different qualitative conclusions regarding the statistical significance of the relationship between reviewer rating and box office take? Example 2: Highway fuel efficiency (in mpg) vs. vehicle weight for 1999 model cars: cars &lt;- read.table(&quot;data/cars.txt&quot;, head = T) with(cars, plot(mpghw ~ weight, xlab = &quot;Vehicle weight (lbs)&quot;, ylab = &quot;Highway mpg&quot;)) fm1 &lt;- lm(mpghw ~ weight, data = cars) abline(fm1) plot(resid(fm1) ~ fitted(fm1), xlab = &quot;Fitted value&quot;, ylab = &quot;Residual&quot;) abline(h = 0,lty = &quot;dashed&quot;) The relationship between highway mpg and vehicle weight is clearly non-linear, although that is seen most clearly from the plot of residuals vs. fitted values. We will discuss modeling non-linear relationships later. Here are some additional comments: What about outliers? The famous statistician George Box was fond of saying that outliers can be the most informative points in the data set. If you have an outlier, try to figure out why that point is an outlier. Discard outliers only if a good reason exists for doing so  resist the temptation to ``scrub your data. Doing so is tantamount to cheating. If you absolutely must remove an outlier, at least report the model fits both with and without the outliers included. Be particularly wary of data points associated with extreme \\(x\\)-values. These points can be unduly influential. (See discussion in the multiple-regression installment of the notes on leverage, standardized residuals, and Cooks distance.) What about transforming the \\(x\\)-variable? Remember that there are no assumptions about the distribution of the \\(x\\)-variable. However, transformations of the \\(x\\)-variable can also make non-linear relationships into linear ones. Remember though that transformations tend to lessen interpretability. Dont extrapolate the regression line beyond the range of the \\(x\\)-variable observed in the data. Remember that statistical models are only valid to the extent that data exist to support them. Although its often overlooked, remember that the standard regression model also assumes that the predictor is measured without error. If theres error in the predictor as well as the response, then the estimated slope will be biased towards 0. If the error in the predictor is comparable to the error in the response, then consider a regression model that allows for variability in the predictor. These models go by multiple names, but they are most often called ``Model II regression. 1.8 Prediction with regression models Consider a new value of the predictor \\(x^\\star\\). There are two different types of predictions we could make: What is the average response of the population at \\(x^\\star\\)? What is the value of a single future observation at \\(x^\\star\\)? Point estimates (i.e., single best guesses) are the same for both predictions. They are found by simply plugging \\(x^\\star\\) into the fitted regression equation. Example. Suppose every grad student at NCSU drinks 2.5 beers. What do we predict the average BAC of this population to be? \\[\\begin{align*} \\hat{y}^\\star &amp; = \\widehat{\\beta }_{0} +\\widehat{\\beta }_{1} x^\\star \\\\ &amp; = -0.013 + 0.018 \\times 2.5\\\\ &amp; = 0.032 \\end{align*}\\] Suppose Danny drinks 2.5 beers. What do we predict Dannys BAC to be? \\[ \\hat{y}^\\star = 0.032 \\] However, the uncertainty in these two predictions is different. Predictions of single future observations are more uncertain than predictions of population averages (why?). We quantify the uncertainty in prediction 1 with a confidence interval. We quantify the uncertainty in prediction 2 with a prediction interval. A prediction interval (PI) is just like a confidence interval in the sense that you get to choose the coverage level. i.e., a 95% prediction interval will contain a single new prediction 95% of the time, while a 99% prediction interval will contain a single new prediction 99% of the time. All else being equal, a 99% prediction interval will be wider than a 95% prediction interval. Both confidence intervals and prediction intervals follow the same general prescription of \\[ \\mbox{estimate} \\pm \\mbox{critical value} \\times \\mbox{standard error} \\] Both also use the same point estimate, \\(\\hat{y}^\\star\\), and the same critical value (taken from a \\(t\\)-distribution with \\(n-2\\) df). However, the standard errors differ depending on whether we are predicting an average response or a single future observation. If you find formulas helpful, you might derive some insight from the formulas for these two standard errors. For an average population response, the standard error is \\[ s_{\\varepsilon} \\sqrt{\\frac{1}{n} +\\frac{\\left(x^\\star -\\bar{x}\\right)^{2} }{S_{xx} } } \\] while for a single future observation, the standard error is \\[ s_{\\varepsilon} \\sqrt{1+\\frac{1}{n} +\\frac{\\left(x^\\star -\\bar{x}\\right)^{2} }{S_{xx} } } \\] Thus, the width of a CI or PI depends on the following: The function can be used to calculate these intervals in R: Regression (solid line), 95% confidence intervals (dashed lines), and 95% prediction intervals (dotted lines) for the beer data. Note that both confidence and prediction intervals widen near the edges of the range of the predictor. Regression models can be used both for observational and experimental data. In some experiments, the experimenter has control over the values of the predictor included in the experiment. Gotelli &amp; Ellison (, pp. 167-9) give the following guidelines for a regression design with a single predictor: Once the values of the predictor to be included in the experiment have been chosen, these values should be randomly assigned to the experimental units. Note that randomization does require randomly choosing the values of the predictor to be included in the experiment! Bibliography "],["multiple-regression.html", "Chapter 2 Multiple regression 2.1 Multiple regression for associating multiple predictor variables with a single response 2.2 Statistical inference for partial regression coefficients 2.3 Indicator variables 2.4 The MLR model in matrix notation 2.5 Interactions between predictors 2.6 Polynomial regression 2.7 Testing multiple regression coefficients at once 2.8 (Multi-)Collinearity 2.9 Variable selection: Choosing the best model 2.10 Non-linear regression 2.11 Leverage, influential points, and standardized residuals", " Chapter 2 Multiple regression 2.1 Multiple regression for associating multiple predictor variables with a single response Just as SLR was used to characterize the relationship between a single predictor and a response, multiple regression can be used to characterize the relationship between several predictors and a response. Example. In the BAC data, we also know each individuals weight and gender: beer &lt;- read.csv(&quot;data/beer2.csv&quot;, head = T, stringsAsFactors = T) head(beer) ## BAC weight gender beers ## 1 0.100 132 female 5 ## 2 0.030 128 female 2 ## 3 0.190 110 female 9 ## 4 0.120 192 male 8 ## 5 0.040 172 male 3 ## 6 0.095 250 female 7 A plot of the residuals from the BAC vs. beers consumed model against weight strongly suggests that some of the variation in BAC is attributable to differences in weight: fm1 &lt;- with(beer, lm(BAC ~ beers)) plot(x = beer$weight, y = resid(fm1), xlab = &quot;weight&quot;, ylab = &quot;residual&quot;) abline(h = 0, lty = &quot;dotted&quot;) Figure 2.1: SLR residuals vs. weight. To simultaneously characterize the effect that the variables beers and weight have on BAC, we might want to entertain a model with both predictors. In words, the model is \\[ \\mbox{BAC} = \\mbox{intercept} + \\mbox{(parameter associated with beers)} \\times \\mbox{beers} + \\mbox{(parameter associated with weight)} \\times \\mbox{weight} + \\mbox{error} \\] where (for the moment) we are intentionally vague about what we mean by parameter associated with beers. As in SLR, the error term can be interpreted as a catch-all term that includes all the variation not accounted for by the predictors beers and weight. In mathematical notation, we can write the model as \\[ y_i = \\beta_0 +\\beta_1 x_{1i} +\\beta_2 x_{2i} +\\epsilon_i \\] Note that to distinguish individual observations, we require a double subscripting of the \\(x\\)s, with the first subscript is used to distinguish different predictors and the second subscript is used to distinguish individual observations. For example, \\(x_{2i}\\) is the value of the second predictor for the \\(i\\)th observation. There are a variety of ways to think about this model. As in SLR, we can separate this model into a mean or signal component \\(\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2\\) and an error component \\(\\epsilon_i\\). Note that the mean component is now a function of two variables, and suggests that the relationship between the average response and either predictor is linear. If we wish to make statistical inferences about the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\beta_2\\) (which we do), then we need to place the standard assumptions on the error component: independence, constant variance, and normality. In notation, \\(\\epsilon_i \\sim \\mathcal{N}\\left(0,\\sigma_{\\epsilon}^2 \\right)\\). We can also think about this model geometrically. Recall that in SLR, we could interpret the SLR model as a line passing through a cloud of data points. With 2 predictors, we are now fitting a plane to data points that exist in a three- dimensional data cloud. As in SLR, we use the least squares criteria to find the best-fitting parameter estimates. That is to say, we will agree that the best estimates of the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\beta_2\\) are the values that minimize \\[\\begin{eqnarray*} SSE &amp; = &amp; \\sum_{i=1}^n e_i^2 \\\\ &amp; = &amp; \\sum_{i=1}^n \\left(y_i -\\hat{y}_i \\right)^2 \\\\ &amp; = &amp; \\sum_{i=1}^n\\left(y_i -\\left[\\hat{\\beta}_0 +\\hat{\\beta}_1 x_{1i} +\\hat{\\beta}_{2} x_{2i} \\right]\\right)^2 \\end{eqnarray*}\\] In R, we can fit this model by adding a new term to the right-hand side of the model formula in the call to the function lm: fm2 &lt;- lm(BAC ~ beers + weight, data = beer) summary(fm2) ## ## Call: ## lm(formula = BAC ~ beers + weight, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.0162968 -0.0067796 0.0003985 0.0085287 0.0155621 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.986e-02 1.043e-02 3.821 0.00212 ** ## beers 1.998e-02 1.263e-03 15.817 7.16e-10 *** ## weight -3.628e-04 5.668e-05 -6.401 2.34e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.01041 on 13 degrees of freedom ## Multiple R-squared: 0.9518, Adjusted R-squared: 0.9444 ## F-statistic: 128.3 on 2 and 13 DF, p-value: 2.756e-09 Thus, we see that the LSEs are \\(\\widehat{\\beta}_0 = 0.040\\), \\(\\widehat{\\beta}_1 = 0.020\\), and \\(\\widehat{\\beta}_{2} = -0.00036\\). As in SLR, we can define the fitted value associated with the \\(i\\)th data point as \\(\\hat{y}_i =\\hat{\\beta}_0 +\\hat{\\beta}_1 x_{1i} +\\hat{\\beta}_{2} x_{2i}\\), and the residual associated with the \\(i\\)th data point as \\(e_i =y_i -\\hat{y}_i\\). Example. Find the fitted value and residual for the first observation in the data set, a \\(x_2=132\\) lb woman who drank \\(x_1=5\\) beers and had a BAC of \\(y=0.1\\). Answer: \\(\\hat{y}_1 =0.092\\) and \\(e_1 =0.008\\). We can define the error sum of squares as \\(SSE=\\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n \\left(y_i -\\hat{y}_i \\right)^2\\). How many df are associated with the SSE? In this model, there are \\(n-3\\) df associated with the SSE, because there were 3 parameters that needed to be estimated in the mean component of the model. As in SLR, we can estimate the error variance \\(\\sigma_{\\epsilon}^2\\) with the MSE, although now we must be careful to divide by the appropriate df: \\[ s_\\epsilon^2 = MSE = \\frac{SSE}{n-3}. \\] For the model above, \\(s_\\epsilon = 0.010\\). In general, the equation for an MLR model with any number of predictors can be written: \\[ y_i =\\beta_0 +\\beta_1 x_{1i} +\\beta_2 x_{2i} +\\ldots +\\beta_k x_{ki} +\\epsilon_i \\] The error term is subject to the standard assumptions of independence, constant variance, and normality. We will use the notation that \\(k\\) is the number of parameters that need to be estimated in the mean component of the model excluding the intercept. (When counting parameters, some texts include the intercept, while others do not. If you consult a text, check to make sure you know what definition is being used.) The SSE will be associated with \\(n - (k + 1)\\) df. Thus, the estimate of \\(\\sigma_{\\epsilon}^2\\) will be \\[ s_\\epsilon^2 = MSE = \\frac{SSE}{n-(k+1)}. \\] 2.1.1 Sums of squares decomposition and \\(R^2\\). The sums-of-squares decomposition also carries over from SLR. We still have \\({\\rm SS(Total)} = \\sum_{i=1}^n \\left(y_i -\\bar{y}\\right)^2\\), \\({\\rm SS(Regression)} = \\sum_{i=1}^n \\left(\\hat{y}_i -\\bar{y}\\right)^2\\), and \\({\\rm SS(Total) = SS(Regression) + SSE}\\). Thus, we can define \\(R^2\\) using the same formula: \\[ R^2 = \\frac{{\\rm SS(Regression)}}{{\\rm SS(Total)}} = 1- \\frac{{\\rm SSE}}{{\\rm SS(Total)}} \\] We still interpret \\(R^2\\) as a measure of the proportion of variability in the response that is explained by the regression model. In the BAC example, \\(R^2=0.952\\). 2.1.2 Partial regression coefficients. The \\(\\widehat{\\beta}\\)s in a MLR model are called partial regression coefficients (or partial regression slopes). Their interpretation is subtly different from SLR regression slopes. Misinterpretation of partial regression coefficients is one of the most common sources of statistical confusion in the scientific literature. We can interpret the partial regression coefficients geometrically. In this interpretation, \\(\\beta_j\\) is the slope of the regression plane in the direction of the predictor \\(x_j\\). Imagine taking a slice of the regression plane. In the terminology of calculus, \\(\\beta_j\\) is also the partial derivative of the regression plane with respect to the predictor $x_j $ (hence the term partial regression coefficient). Verbal interpretation: The partial regression coefficient \\(\\beta_j\\) associated with the predictor \\(x_j\\) is the slope of the linear association between \\(y\\) and \\(x_j\\) . Other ways to express this same idea include that \\(\\beta_j\\) is the quantifies the linear effect of predictor \\(j\\) while holding the other predictors constant, or while controlling for the effects of the other predictors. This is different from an SLR slope, which we can interpret as the slope of the linear association between \\(y\\) and \\(x_j\\) while ignoring all other predictors. Thus, the value of a multiple regression coefficient . Compare the estimated regression coefficients for the number of beers consumed in the SLR model and the MLR model that includes weight. Estimated SLR coefficient for no. of beers consumed: 0.018 Estimated MLR coefficient for no. of beers consumed: 0.020 The coefficients differ because they estimate different parameters that mean different things. The SLR coefficient estimates a slope that does not account for the effect of weight, while the MLR coefficient estimates a slope that does account for the effect of weight. Another example. As cheese ages, various chemical processes take place that determine the taste of the final product. These data are from the Moore and McCabe (1989). We will call these the cheese data. The response variable is the taste scores averaged from several tasters. There are three predictors that describe the chemical content of the cheese. They are: acetic: the natural log of acetic acid concentration h2s: the natural log of hydrogen sulfide concentration lactic: the concentration of lactic acid Here is a pairs plot of the data. In this plot, each panel is a scatterplot showing the relationship between two of the four variables in the model. Pairs plots are useful ways to gain a quick grasp of the structure in the data and how the constituent variables are related to one another. cheese &lt;- read.table(&quot;data/cheese.txt&quot;, head = T, stringsAsFactors = T) pairs(cheese) Lets entertain a model that uses all three predictors. fm1 &lt;- lm(taste ~ Acetic + H2S + Lactic, data = cheese) summary(fm1) ## ## Call: ## lm(formula = taste ~ Acetic + H2S + Lactic, data = cheese) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.390 -6.612 -1.009 4.908 25.449 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -28.8768 19.7354 -1.463 0.15540 ## Acetic 0.3277 4.4598 0.073 0.94198 ## H2S 3.9118 1.2484 3.133 0.00425 ** ## Lactic 19.6705 8.6291 2.280 0.03108 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.13 on 26 degrees of freedom ## Multiple R-squared: 0.6518, Adjusted R-squared: 0.6116 ## F-statistic: 16.22 on 3 and 26 DF, p-value: 3.81e-06 Compare this MLR model with each of the three possible SLR models: slr1 &lt;- lm(taste ~ Acetic, data = cheese) summary(slr1) ## ## Call: ## lm(formula = taste ~ Acetic, data = cheese) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.642 -7.443 2.082 6.597 26.581 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -61.499 24.846 -2.475 0.01964 * ## Acetic 15.648 4.496 3.481 0.00166 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.82 on 28 degrees of freedom ## Multiple R-squared: 0.302, Adjusted R-squared: 0.2771 ## F-statistic: 12.11 on 1 and 28 DF, p-value: 0.001658 slr2 &lt;- lm(taste ~ H2S, data = cheese) summary(slr2) ## ## Call: ## lm(formula = taste ~ H2S, data = cheese) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.426 -7.611 -3.491 6.420 25.687 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.7868 5.9579 -1.643 0.112 ## H2S 5.7761 0.9458 6.107 1.37e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.83 on 28 degrees of freedom ## Multiple R-squared: 0.5712, Adjusted R-squared: 0.5558 ## F-statistic: 37.29 on 1 and 28 DF, p-value: 1.374e-06 slr3 &lt;- lm(taste ~ Lactic, data = cheese) summary(slr3) ## ## Call: ## lm(formula = taste ~ Lactic, data = cheese) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.9439 -8.6839 -0.1095 8.9998 27.4245 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -29.859 10.582 -2.822 0.00869 ** ## Lactic 37.720 7.186 5.249 1.41e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.75 on 28 degrees of freedom ## Multiple R-squared: 0.4959, Adjusted R-squared: 0.4779 ## F-statistic: 27.55 on 1 and 28 DF, p-value: 1.405e-05 What do you make of the fact that an SLR analysis suggests that there is a (statistically significant) positive relationship between acetic acid concentration and taste, yet the partial regression coefficient associated with acetic acid concentration is not statistically significant in the MLR model? 2.2 Statistical inference for partial regression coefficients Statistical inference for partial regression coefficients proceeds in the same way as statistical inference for SLR slopes. Standard errors for both partial regression coefficients are provided in the R output: \\(s_{\\widehat{\\beta}_1 }\\)= 0.0013, \\(s_{\\widehat{\\beta}_{2} }\\)= 0.000057. Under the standard regression assumptions, the quantity \\(t=(\\hat{\\beta}_i -\\beta _i )/s_{\\hat{\\beta}_i }\\) has a \\(t\\)-distribution. The number of degrees of freedom is the number of df associated with the SSE. This fact can be used to construct confidence intervals and hypothesis tests. Example Form a 99% CI for \\(\\beta_2\\), the partial regression coefficient associated with weight. First we find the critical value in R: qt(.005, df = 13, lower = FALSE) ## [1] 3.012276 Solution: (-0.00053, -0.00019) Example Test \\(H_0\\): \\(\\beta_2 =0\\) vs. H\\(_{a}\\): \\(\\beta_2 \\ne 0\\) at the \\(\\alpha\\)=5% significance level. Solution: \\(t_{13} =-6.40\\), \\(p\\) = 0.000023, reject \\(H_0\\): \\(\\beta_2 =0\\) in favor of \\(H_a\\): \\(\\beta_2 \\ne 0\\) at the \\(\\alpha\\)=5% significance level. If we were reporting this analysis in scientific writing, we might state that after accounting for the effect of the number of beers consumed, BAC decreases 3.6 \\(\\times\\) 10\\(^{-4}\\) (s.e. 5.7 \\(\\times\\) 10\\(^{-5}\\)) for every 1-lb increase in weight (\\(t_{13} =-6.40\\), \\(p &lt; .001\\)). In MLR, hypothesis tests of the form \\(H_0\\): \\(\\beta_i =0\\) vs. \\(H_a\\): \\(\\beta_i \\ne 0\\) are almost always interesting (and are easy for a computer to calculate). Therefore, most computer packages provide them automatically. Most scientists interpret these tests as a measure of whether or not the associated predictor has a statistically significant (linear) association with the response after accounting for the (linear) effects of the other predictors in the model. For example, in the BAC model, we might conclude that both (a) the number of beers consumed has a statistically significant association with BAC after accounting for the effect of weight, and (b) weight has a statistically significant association with BAC after accounting for the effect of the number of beers consumed. 2.2.1 Prediction As in SLR, we distinguish between predictions of the average response of the population at a new value of the predictors vs. the value of a single future observation. The point estimates of the two predictions are identical, but the value of a single future observation is more uncertain. Therefore, we use a prediction interval for a single observation and a confidence interval for a population average. The width of a PI or CI is affected by the same factors as in SLR. In MLR, the width of the PI or CI depends on the distance between the new observation and the center of mass of the predictors in the data set. For example, if we now use the BAC model to predict the BAC of a 170-lb individual who consumes 4 beers: new.data &lt;- data.frame(weight = 170, beers = 4) predict(fm2, newdata = new.data, interval = &quot;prediction&quot;) ## fit lwr upr ## 1 0.05808664 0.03480226 0.08137103 predict(fm2, newdata = new.data, interval = &quot;confidence&quot;) ## fit lwr upr ## 1 0.05808664 0.05205732 0.06411596 2.3 Indicator variables So far, we have dealt exclusively with numeric (quantitative) predictors. Although we havent given it much thought, a key feature of quantitative predictors is that their values can be ordered, and that the distance between ordered values is meaningful. For example, in the BAC data, a predictor value of \\(x=3\\) beers consumed is greater than \\(x=2\\) beers consumed. Moreover, the difference between \\(x=2\\) and \\(x=3\\) beers consumed is exactly one-half of the distance between \\(x=2\\) and \\(x=4\\) beers consumed. Another way to think about quantitative predictors is that we could sensibly place all of their values on a number line. Categorical variables are variables that cannot be sensibly placed on a number line. Examples include gender, ethnicity, or brand of manufacture. We use indicator variables as devices to include categorical variables as predictors in regression models. Constructing indicator variables. Choose one level of the categorical variable as the reference level. Sometimes, the scientific context of a problem will suggest that one level should serve as the reference level. Oftentimes, though, the choice of the reference level is arbitrary. In all cases, the choice of the reference level will have no effect on the ensuing analysis. For every other level, create a separate indicator variable that is equal to 1 for that level, and is equal to 0 for all levels. Thus, to include a categorical variable with \\(c\\) levels, we need \\(c - 1\\) indicator variables. If R recognizes that a variable is categorical, it will construct indicators on its own. However, it takes a little detective work to find out what the reference level is. For example, in the BAC data, there is a separate variable called gender that takes the values male and female. If we tried the model fm3 &lt;- lm(BAC ~ beers + weight + gender, data = beer) summary(fm3) ## ## Call: ## lm(formula = BAC ~ beers + weight + gender, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.018125 -0.005713 0.001501 0.007896 0.014655 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.871e-02 1.097e-02 3.528 0.004164 ** ## beers 1.990e-02 1.309e-03 15.196 3.35e-09 *** ## weight -3.444e-04 6.842e-05 -5.034 0.000292 *** ## gendermale -3.240e-03 6.286e-03 -0.515 0.615584 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.01072 on 12 degrees of freedom ## Multiple R-squared: 0.9528, Adjusted R-squared: 0.941 ## F-statistic: 80.81 on 3 and 12 DF, p-value: 3.162e-08 To determine how to interpret the sign of the estimated coefficient for gendermale, we need to know what level R chose as the reference. To find out, we need to use the contrasts function: contrasts(beer$gender) ## male ## female 0 ## male 1 This tell us that the model is \\[ y=\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 + \\beta_3 x_3 +\\epsilon \\] where \\[ x_3 =\\left\\{\\begin{array}{cc} {1} &amp; {{\\rm male}} \\\\ {0} &amp; {{\\rm female}} \\end{array}\\right. \\] Thus the regression coefficient \\(\\beta_3\\) quantifies the difference in BAC for men vs. women after controlling for the effects of the number of beers consumed and weight. Interpretation: If we consider a man and a woman, both of whom weigh the same and have consumed the same number of beers, then the model estimates that the mans BAC will be 0.0032 lower. However, this effect is not statistically significant. That is, the data are consistent with the null hypothesis that the true size of the effect of gender is 0 (\\(p=0.62\\)). Consequently, after controlling for the effects of beers consumed and weight on BAC, there is no evidence for a further effect of gender on BAC. 2.4 The MLR model in matrix notation Here are the first few rows of the design matrix for the BAC regression that includes beers consumed and weight as the two predictors: \\[ \\mathbf{X}=\\left[\\begin{array}{ccc} {1} &amp; {5} &amp; {132} \\\\ {1} &amp; {2} &amp; {128} \\\\ {1} &amp; {9} &amp; {110} \\\\ {1} &amp; {8} &amp; {192} \\\\ {\\vdots } &amp; {\\vdots } &amp; {\\vdots } \\end{array}\\right] \\] We can use the standard equation \\(\\hat{\\mathbf{\\beta}}=\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39;\\mathbf{Y}\\) to find the least-squares estimates of the regression parameters. Note that the order of the columns in the design matrix is arbitrary. We could also have written the design matrix as \\[ \\mathbf{X}= \\left[\\begin{array}{ccc} {1} &amp; {132} &amp; {5} \\\\ {1} &amp; {128} &amp; {2} \\\\ {1} &amp; {110} &amp; {9} \\\\ {1} &amp; {192} &amp; {8} \\\\ {\\vdots } &amp; {\\vdots } &amp; {\\vdots } \\end{array}\\right] \\] The only difference this would make is that it changes the order in which the estimated partial regression coefficients appear in the vector \\(\\hat{\\mathbf{\\beta}}\\). Recall that some design matrices are pathological, in the sense that the matrix inverse \\(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\) does not exist. In MLR, you are more likely to encounter these pathological matrices. Here are two separate examples in which such a matrix might be encountered: Example. You are studying the effect of temperature on weight gain in fish, and measure temperature in both degrees Fahrenheit and Centigrade. (Recall that one can convert between Fahrenheit and Centigrade by the equation F = (9/5) C + 32.) The design matrix is \\[ \\mathbf{X}=\\left[\\begin{array}{ccc} {1} &amp; {5} &amp; {41} \\\\ {1} &amp; {10} &amp; {50} \\\\ {1} &amp; {15} &amp; {59} \\\\ {1} &amp; {20} &amp; {68} \\end{array}\\right] \\] where the predictor in the 2\\(^{nd}\\) column is degrees Centigrade and the predictor in the 3\\(^{rd}\\) column is degrees Fahrenheit. Example 2 With the beer data, suppose we created indicator variables for both males and females, and entertained the model: \\[ y_i =\\beta_0 +\\beta_1 x_{1i} +\\beta_2 x_{2i} +\\beta_3 x_{3i} +\\epsilon_i \\] where \\(x_1\\) is the number of beers consumed, \\(x_2\\) is an indicator variable for male and \\(x_3\\) is an indicator variable for female: \\[ x_2 =\\left\\{\\begin{array}{cc} {1} &amp; {{\\rm male}} \\\\ {0} &amp; {{\\rm female}} \\end{array}\\right. \\] \\[ x_3 =\\left\\{\\begin{array}{cc} {0} &amp; {{\\rm male}} \\\\ {1} &amp; {{\\rm female}} \\end{array}\\right. \\] For the sake of illustration, suppose we were fitting this model to just the first 5 data points in the data set. The design matrix would be \\[ \\mathbf{X}=\\left[\\begin{array}{cccc} {1} &amp; {5} &amp; {0} &amp; {1} \\\\ {1} &amp; {2} &amp; {0} &amp; {1} \\\\ {1} &amp; {9} &amp; {0} &amp; {1} \\\\ {1} &amp; {8} &amp; {1} &amp; {0} \\\\ {1} &amp; {3} &amp; {1} &amp; {0} \\end{array}\\right] \\] In both of these examples, it is impossible to fit the regression model because the matrix \\(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\) does not exist. What is the common feature of these design matrices that allows us to determine whether or not \\(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\) exists? In mathematical terms, we say that each matrix contains a . Lets write each column of the design matrix as the vector \\(\\mathbf{x}\\), i.e., for the temperature example above, \\[ x_0 =\\left[\\begin{array}{c} {1} \\\\ {1} \\\\ {1} \\\\ {1} \\end{array}\\right],x_1 =\\left[\\begin{array}{c} {5} \\\\ {10} \\\\ {15} \\\\ {20} \\end{array}\\right],x_2 =\\left[\\begin{array}{c} {41} \\\\ {50} \\\\ {59} \\\\ {68} \\end{array}\\right] \\] A design matrix with \\(k+1\\) columns has a linear dependency if and only if there exist a set of constants $_0 ,_1 ,_2 ,,_k $ such that \\[ \\lambda_0 x_0 +\\lambda_1 x_1 +...+\\lambda_k x_k =\\left[\\begin{array}{c} {0} \\\\ {0} \\\\ {\\vdots } \\\\ {0} \\end{array}\\right] \\] and at least one of the \\(\\lambda\\)s is \\(\\ne 0\\). Thus, the temperature example has a linear dependency because \\[ -32\\left[\\begin{array}{c} {1} \\\\ {1} \\\\ {1} \\\\ {1} \\end{array}\\right]-\\frac{9}{5} \\left[\\begin{array}{c} {5} \\\\ {10} \\\\ {15} \\\\ {20} \\end{array}\\right]+\\left[\\begin{array}{c} {41} \\\\ {50} \\\\ {59} \\\\ {68} \\end{array}\\right]=\\left[\\begin{array}{c} {0} \\\\ {0} \\\\ {0} \\\\ {0} \\end{array}\\right]. \\] Now the logic for constructing \\(c-1\\) indicator variables to represent a categorical variable with \\(c\\) different levels is clear. If we tried to create a separate indicator for every level of a categorical variable, then the \\(c\\) columns of the design matrix representing those indicators would add together to produce a column of 1s. Because every design matrix must already contain a column of 1s to correspond to the intercept, including \\(c\\) separate indicators would produce a design matrix that contained a linear dependency among its columns. 2.5 Interactions between predictors Consider (again) the BAC data, with our working model \\[ y_i =\\beta_0 +\\beta_1 x_{1i} +\\beta_2 x_{2i} +\\epsilon_i \\] where \\(y\\) is the response (BAC), \\(x_1\\) is beers consumed, \\(x_2\\) is weight, and \\(\\epsilon\\) is iid normal error. This model assumes that there is no interaction between the number of beers consumed and weight, in the sense that the association between beers consumed and BAC the same for everyone regardless of weight, and the association between weight and BAC is the same for everyone regardless of how many beers the person has consumed. In other words, the model above assumes that the effect of beers consumed on BAC does not depend on weight, and the effect of weight on BAC does not depend on the number of beers consumed. We might call this model an additive model, because the effects of beers consumed and weight on BAC add together in the mean or signal component of the model. (Note that a statistical interaction has nothing to do with whether there is an association between beers consumed and weight.) An interaction term between the two predictors would allow the effect of beers consumed to depend on weight, and vice versa. A model with an interaction can be written as: \\[ y_i =\\beta_0 +\\beta_1 x_{1i} +\\beta_2 x_{2i} +\\beta_3 x_{1i} x_{2i} +\\epsilon_i \\] There are two equally good ways to code this model in R: fm1 &lt;- lm(BAC ~ beers + weight + beers:weight, data = beer) or fm2 &lt;- lm(BAC ~ beers * weight, data = beer) In the first notation, the colon (:) tells R to include the interaction between the predictors that appear on either side of the colon. In the second notation, the asterisk (*) is shorthand for both the individual predictors and their interaction. summary(fm2) ## ## Call: ## lm(formula = BAC ~ beers * weight, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.0169998 -0.0070909 0.0008463 0.0084267 0.0164373 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.010e-02 3.495e-02 0.861 0.40601 ## beers 2.162e-02 5.760e-03 3.754 0.00275 ** ## weight -2.993e-04 2.241e-04 -1.336 0.20646 ## beers:weight -1.066e-05 3.627e-05 -0.294 0.77393 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0108 on 12 degrees of freedom ## Multiple R-squared: 0.9521, Adjusted R-squared: 0.9402 ## F-statistic: 79.57 on 3 and 12 DF, p-value: 3.453e-08 Interpreting the interaction. The partial regression coefficient associated with an interaction between two predictors (call them A and B) quantifies the effect that predictor A has on the linear association between predictor B and the response. Or, equivalently, the same partial regression coefficient quantifies the effect that predictor B has on the linear association between predictor A and the response. (It may not be obvious right away that the same regression coefficient permits both interpretations, but you might be able to convince yourself that this is true by doing some algebra with the regression model.) Thus, if we reject the null hypothesis that a partial regression coefficient associated with an interaction equals zero, then we conclude that the effects of the two predictors depend on one another. With the BAC data, the interaction term is not statistically significant. Thus, these data provide no evidence that the effect of beers consumed on BAC depends on weight, or vice versa. In other words, there is no evidence of an interaction between the effects of beer consumption and weight on BAC. For the sake of argument, lets set aside for the moment the fact that the interaction between beers consumed and weight is not statistically significant. How can we interpret the value \\(\\hat{\\beta}_3 = -1.07 \\times 10^{-5}\\)? This value tells us how the association between beers consumed and BAC changes as weight changes. In other words, the model predicts that a heavier persons BAC will increase less for each additional beer consumed, compared to a lighter person. How much less? If the heavier person weighs 1 lb more than the lighter person, then one additional beer will increase the heavier persons BAC by \\(1.07 \\times 10^{-5}\\) less than it increases the lighter persons BAC. This agrees with our expectations, but these data do not provide enough evidence to declare that this interaction is statistically significant. Alternatively, \\(\\beta_3\\) also tells us how the association between weight and BAC changes as the number of beers consumed changes. That is, as the number of beers consumed increases, then the association between weight and BAC becomes more steeply negative. Again, this coincides with our expectation, despite the lack of statistical significance. Interpreting partial regression coefficients in the presence of an interaction. In the presence of an interaction, the partial regression coefficients associated with individual predictors are often called the main effects of those predictors. When an interaction is present, the interpretation of these main effects is subtle. In this case, the partial regression coefficient associated with the individual predictor quantifies the relationship between the predictor and the response when the other predictor involved in the interaction equals 0. Sometimes this interpretation is scientifically meaningful, but usually it isnt. For example, in the BAC model that includes the interaction above, the parameter \\(\\beta_1\\) now quantifies the association between beers consumed and BAC for people who weigh 0 lbs. Obviously, this is a meaningless quantity. Alternatively, the parameter \\(\\beta_2\\) quantifies the association between weight and BAC for people who have consumed 0 beers. This is a bit less ridiculous  in fact, it makes a lot of sense  but still requires extrapolating the model fit outside the range of the observed data, because there are no data points here for people who have had 0 beers.6 To summarize, the subtlety here is that if we compare the additive model \\[ y =\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +\\epsilon \\] with the model that includes an interaction \\[ y =\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +\\beta_3 x_1 x_2 +\\epsilon, \\] the parameters \\(\\beta_1\\) and \\(\\beta_2\\) have completely different meanings in these two models. In other words, adding an interaction between two predictors changes the meaning of the regression coefficient associated with the individual predictors involved in the interaction. This is a subtlety that routinely confuses even top investigators. Its a hard point to grasp, but essential for interpreting models that include interactions correctly. A useful technique for easing the interpretation of partial regression coefficients in a model that includes an interaction is to center the predictors by subtracting off their means. For the BAC data, we could create centered versions of the two predictors by: \\[ \\begin{array}{l} {x_1^{c} =x_1 -\\bar{x}_1 } \\\\ {x_2^{c} =x_2 -\\bar{x}_{2} } \\end{array} \\] We then fit the model with the interaction, using the centered predictors instead: beer$beers.c &lt;- beer$beers - mean(beer$beers) beer$weight.c &lt;- beer$weight - mean(beer$weight) head(beer) ## BAC weight gender beers beers.c weight.c ## 1 0.100 132 female 5 0.1875 -39.5625 ## 2 0.030 128 female 2 -2.8125 -43.5625 ## 3 0.190 110 female 9 4.1875 -61.5625 ## 4 0.120 192 male 8 3.1875 20.4375 ## 5 0.040 172 male 3 -1.8125 0.4375 ## 6 0.095 250 female 7 2.1875 78.4375 fm3 &lt;- lm(BAC ~ beers.c * weight.c, data = beer) summary(fm3) ## ## Call: ## lm(formula = BAC ~ beers.c * weight.c, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.0169998 -0.0070909 0.0008463 0.0084267 0.0164373 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.402e-02 2.849e-03 25.984 6.45e-12 *** ## beers.c 1.980e-02 1.447e-03 13.685 1.10e-08 *** ## weight.c -3.506e-04 7.206e-05 -4.865 0.000388 *** ## beers.c:weight.c -1.066e-05 3.627e-05 -0.294 0.773927 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0108 on 12 degrees of freedom ## Multiple R-squared: 0.9521, Adjusted R-squared: 0.9402 ## F-statistic: 79.57 on 3 and 12 DF, p-value: 3.453e-08 Note that centering the predictors does not change the estimated interaction or its statistical significance. The main advantage of centering the predictors is that the partial regression coefficients associated with the centered versions of the predictors have a nice interpretation. Now, the partial regression coefficients associated with the main effects quantify the relationship between the predictor and the response when the other predictor involved in the interaction equals its average value. Perhaps there is an interaction between gender and the number of beers consumed, with respect to their effect on BAC.7 We test for such an association by including an interaction between the indicator for `male and the number of beers consumed. The model equation is: \\[ y=\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +\\beta_3 x_3 + \\beta_4 x_1 x_3 +\\epsilon \\] Compare this model for males: \\[ y=\\left(\\beta_0 +\\beta_3 \\right)+\\left(\\beta_1 +\\beta_4 \\right)x_1 +\\beta_2 x_2 +\\epsilon \\] and females: \\[ y=\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +\\epsilon \\] Thus, we see that the coefficient \\(\\beta_4\\) will quantify how the relationship between beers consumed and BAC differs for men vs. women (after accounting for the effect of weight). We fit the model in R: fm4 &lt;- lm(BAC ~ beers + weight + gender + beers:gender, data = beer) summary(fm4) ## ## Call: ## lm(formula = BAC ~ beers + weight + gender + beers:gender, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.0152421 -0.0032845 0.0006456 0.0043247 0.0181410 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.820e-02 1.203e-02 2.345 0.038861 * ## beers 2.173e-02 1.646e-03 13.201 4.34e-08 *** ## weight -3.323e-04 6.426e-05 -5.171 0.000308 *** ## gendermale 1.518e-02 1.251e-02 1.213 0.250523 ## beers:gendermale -3.946e-03 2.368e-03 -1.666 0.123824 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.01 on 11 degrees of freedom ## Multiple R-squared: 0.9623, Adjusted R-squared: 0.9486 ## F-statistic: 70.28 on 4 and 11 DF, p-value: 9.248e-08 (Note that we usually dont center indicator variables.) Thus, we see that after controlling for the effect of weight, there is no evidence for an interaction between beers consumed and gender (\\(t_{11} =-1.67\\), \\(p = 0.12\\)). We could interpret this to mean that the relationship between beers consumed and BAC is the same for men and women, or to mean that the difference in BAC between men and women doesnt depend on the number of beers consumed. 2.6 Polynomial regression The machinery of MLR can be used to model a non-linear relationship between a predictor and a response. This is called polynomial regression. A \\(k^{th}\\) order polynomial regression model is \\[ y=\\beta_0 +\\beta_1 x+\\beta_2 x^2 +\\beta_3 x^{3} +\\ldots +\\beta_k x^{k} +\\epsilon \\] where the error term is subject to the standard regression assumptions. In practice, the most commonly used models are quadratic (\\(k=2\\)) and cubic (\\(k=3\\)) polynomials. Before proceeding, a historical note is worthwhile. It used to be that polynomial regression was the only way to accommodate non-linear relationships in regression models. In the present day, non-linear regression (see the later section in these notes) allows us to fit a much richer set of non-linear models to data. However, in complex models (especially complex ANOVA models for designed experiments), there are still cases where it is easier to add a quadratic term to accommodate a non-linear association than it is to adopt the machinery of non-linear regression. Thus, it is still worthwhile to know a little bit about polynomial regression, but dont shoehorn every non-linear association into a polynomial regression if an alternative non-linear model is more suitable. Example. In the cars data, the relationship between highway mpg and vehicle weight is clearly non-linear: cars &lt;- read.table(&quot;data/cars.txt&quot;, head = T, stringsAsFactors = T) with(cars, plot(mpghw ~ weight, xlab = &quot;Vehicle weight (lbs)&quot;, ylab = &quot;Highway mpg&quot;)) To fit a quadratic model, we could manually create a predictor equal to weight-squared. Or, in R, we could create the weight-squared predictor within the call to lm by using the following syntax: quad &lt;- lm(mpghw ~ weight + I(weight^2), data = cars) summary(quad) ## ## Call: ## lm(formula = mpghw ~ weight + I(weight^2), data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.4386 -1.8216 0.1789 2.3617 7.5031 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.189e+01 6.332e+00 14.511 &lt; 2e-16 *** ## weight -2.293e-02 3.119e-03 -7.353 1.64e-11 *** ## I(weight^2) 1.848e-06 3.739e-07 4.942 2.24e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.454 on 136 degrees of freedom ## Multiple R-squared: 0.7634, Adjusted R-squared: 0.7599 ## F-statistic: 219.4 on 2 and 136 DF, p-value: &lt; 2.2e-16 In the quadratic regression \\(y=\\beta_0 +\\beta_1 x+\\beta_2 x^2 +\\epsilon\\), the test of \\(H_0\\): \\(\\beta_2=0\\) vs. \\(H_a\\): \\(\\beta_2 \\ne 0\\) is tantamount to a test of whether the quadratic model provides a significantly better fit than the linear model. I this case, we can conclusively reject \\(H_0\\): \\(\\beta_2=0\\) in favor of \\(H_a\\): \\(\\beta_2 \\ne 0\\) , and thus conclude that the quadratic model provides a significantly better fit than the linear model. However, in the context of the quadratic model, the test of \\(H_0\\): \\(\\beta_1=0\\) vs. \\(H_a\\): \\(\\beta_1 \\ne 0\\) doesnt give us much useful information. In the context of the quadratic model, the null hypothesis \\(H_0\\): \\(\\beta_1=0\\) is equivalent to the model \\(y=\\beta_0 +\\beta_2 x^2 +\\epsilon\\). This is a strange model, and there is no reason why we should consider it. Thus, we disregard the inference for \\(\\beta_1\\), and (by similar logic) we disregard the inference for \\(\\beta_0\\) as well. If a quadratic model is good, will the cubic model \\(y=\\beta_0 +\\beta_1 x+\\beta_2 x^2 +\\beta_3 x^{3} +\\epsilon\\) be even better? Lets see: cubic &lt;- lm(mpghw ~ weight + I(weight^2) + I(weight^3), data = cars) summary(cubic) ## ## Call: ## lm(formula = mpghw ~ weight + I(weight^2) + I(weight^3), data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.247 -1.759 0.281 2.411 7.225 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.164e+02 2.697e+01 4.318 3.03e-05 *** ## weight -4.175e-02 2.033e-02 -2.054 0.042 * ## I(weight^2) 6.504e-06 4.984e-06 1.305 0.194 ## I(weight^3) -3.715e-10 3.966e-10 -0.937 0.351 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.456 on 135 degrees of freedom ## Multiple R-squared: 0.7649, Adjusted R-squared: 0.7597 ## F-statistic: 146.4 on 3 and 135 DF, p-value: &lt; 2.2e-16 In the cubic model, the test of \\(H_0\\): \\(\\beta_3=0\\) vs. \\(H_a\\): \\(\\beta_3 \\ne 0\\) is tantamount to a test of whether the cubic model provides a significantly better fit than the quadratic model. The \\(p\\)-value associated with the cubic term suggests that the cubic model does not provide a statistically significant improvement in fit compared to the quadratic model. At this point, you might wonder if we are limited only to comparing models of adjacent orders, that is, quadratic vs. linear, cubic vs. quadratic, etc. The answer is no  we can, for example, test whether a cubic model provides a significantly better fit than a linear model. To do so, we would have to test \\(H_0\\): \\(\\beta_2 = \\beta_3 = 0\\) in the cubic model. We can test this null hypothesis with an \\(F\\)-test, which we will learn how to do later. Note that, even though as cubic model does not offer a significantly better fit than a quadratic model, we have not necessarily ruled out the possibility that a higher-order polynomial model might provide a significantly better fit. However, higher-order polynomials (beyond a cubic) are typically difficult to justify on scientific grounds, and offend our sense of parsimony. Plus, a plot of the quadratic model and the associated residuals suggest that a quadratic model captures the trend in the data well: with(cars, plot(mpghw ~ weight, xlab = &quot;Vehicle weight (lbs)&quot;, ylab = &quot;Highway mpg&quot;)) quad &lt;- with(cars, lm(mpghw ~ weight + I(weight^2))) quad.coef &lt;- as.vector(coefficients(quad)) quad.fit &lt;- function(x) quad.coef[1] + quad.coef[2] * x + quad.coef[3] * x^2 curve(quad.fit, from = min(cars$weight), to = max(cars$weight), add = TRUE, col = &quot;red&quot;) plot(x = fitted(quad), y = resid(quad), xlab = &quot;Fitted values&quot;, ylab = &quot;Residuals&quot;) abline(h = 0, lty = &quot;dashed&quot;) Therefore, the quadratic model clearly provides the best low-order polynomial fit to these data. Finally, it doesnt make sense to consider models that include higher-order terms without lower-order terms. For example, we wouldnt usually consider a cubic model without an intercept, or a quadratic model without a linear term. Geometrically, these models are constrained in particular ways. If such a constraint makes sense scientifically, entertaining the model may be warranted, but this situation arises only rarely. Thus, our strategy for fitting polynomial models is to choose the lowest-order model that provides a reasonable fit to the data, and whose highest-order term is statistically significant. 2.7 Testing multiple regression coefficients at once Consider a general multiple regression model with \\(k\\) predictors: \\[ y=\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +...+\\beta_k x_k +\\epsilon \\] So far, weve seen how to test whether any one individual partial regression coefficient is equal to zero, i.e., \\(H_0 :\\beta_j =0\\) vs. \\(H_a :\\beta_j \\ne 0\\). We will now discuss how to generalize this idea to test if multiple partial regression coefficients are simultaneously equal to zero. This is particularly useful when we have a categorical variable with more than 2 levels. Example. D. L. Sackett investigated mercury accumulation in the tissues of large-mouth bass (a species of fish) in several lakes in the Raleigh area. The data that we will examine are samples of fish from three lakes: Adger, Bennetts Millpond, and Waterville. From each lake, several fish were sampled, and the mercury (Hg) content of their tissues was measured. Because fish are known to accumulate mercury in their tissues as they age, the age of each fish (in years) was also determined. The plot below shows the mercury content (in mg / kg) for each fish plotted vs. age, with different plotting symbols used for the three lakes. To stabilize the variance, we will take the log of mercury content as the response variable (shown in the right panel). There are \\(n=23\\) data points in this data set. fish &lt;- read.table(&quot;data/fish-mercury.txt&quot;, head = T, stringsAsFactors = T) with(fish, plot(log(hg) ~ age, xlab = &quot;age (years)&quot;, ylab = &quot;log(tissue Hg)&quot;, type = &quot;n&quot;)) with(subset(fish, site == &quot;Adger&quot;), points(log(hg) ~ age, pch = &quot;A&quot;)) with(subset(fish, site == &quot;Bennett&quot;), points(log(hg) ~ age, pch = &quot;B&quot;)) with(subset(fish, site == &quot;Waterville&quot;), points(log(hg) ~ age, pch = &quot;W&quot;)) With these data, we would like to ask: after accounting for the effect of age, is there evidence that tissue mercury content in fish differs among the three lakes? To do so, we will consider the model \\[ y=\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +\\beta_3 x_3 +\\epsilon \\] where \\(y\\) is the log of tissue mercury content, \\(x_1\\) is age (in years) and \\(x_2\\) and \\(x_3\\) are indicator variables to code for the differences among the three lakes. To learn about how R codes the indicator variables, we can use contrasts(fish$site) ## Bennett Waterville ## Adger 0 0 ## Bennett 1 0 ## Waterville 0 1 To test for a difference among the lakes, we want to test the hypotheses \\(H_0 :\\beta_2 =\\beta_3 =0\\) vs. \\(H_a :\\beta_2 \\ne 0{\\rm \\; or\\; }\\beta_3 \\ne 0\\). Here, the null hypothesis embodies the idea that there is no difference among the lakes, after accounting for the effect of age. Here is the recipe for testing a general null hypothesis about the parameters in a regression model, illustrated with the fish data. Fit the full model. The full model is the one provides the context for the hypothesis test. Record the SSE and the df for error. full.model &lt;- lm(log(hg) ~ age + site, data = fish) summary(full.model) ## ## Call: ## lm(formula = log(hg) ~ age + site, data = fish) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.47117 -0.08896 0.03796 0.13910 0.31327 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.80618 0.15398 -11.730 3.80e-10 *** ## age 0.14309 0.01967 7.276 6.66e-07 *** ## siteBennett 0.07107 0.12910 0.550 0.5884 ## siteWaterville -0.26105 0.10817 -2.413 0.0261 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2084 on 19 degrees of freedom ## Multiple R-squared: 0.7971, Adjusted R-squared: 0.765 ## F-statistic: 24.88 on 3 and 19 DF, p-value: 8.58e-07 (sse.full &lt;- sum(resid(full.model)^2)) ## [1] 0.8252884 Fit a reduced model. The reduced model is found by taking the full model and imposing the null hypothesis on it. In the fish example, the reduced model is \\(y=\\beta_0 +\\beta_1 x_1 +\\epsilon\\). Record the SSE and the df for error. reduced.model &lt;- lm(log(hg) ~ age, data = fish) summary(reduced.model) ## ## Call: ## lm(formula = log(hg) ~ age, data = fish) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.57944 -0.11907 -0.02226 0.16972 0.44691 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.73344 0.10668 -16.25 2.28e-13 *** ## age 0.12054 0.01674 7.20 4.27e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2363 on 21 degrees of freedom ## Multiple R-squared: 0.7117, Adjusted R-squared: 0.698 ## F-statistic: 51.84 on 1 and 21 DF, p-value: 4.271e-07 (sse.reduced &lt;- sum(resid(reduced.model)^2)) ## [1] 1.172507 Calculate an \\(F\\)-statistic. The formula for the \\(F\\)-statistic is: \\[ F=\\frac{\\left[SSE_{reduced} -SSE_{full} \\right]/\\left[df_{reduced} -df_{full} \\right]}{{SSE_{full} / df_{full} }} \\] For the fish data, this test statistic evaluates to: \\[ F=\\frac{\\left[1.1725 - 0.8253 \\right]/\\left[21 - 19 \\right]}{{1.1725 / 21 }} = 3.997 \\] If the null hypothesis is true, then the \\(F\\)-statistic calculated in step 3 will have an \\(F\\) distribution with numerator df equal to \\(df_{reduced} -df_{full}\\), and denominator df equal to \\(df_{full}\\). Evidence against the null and in favor of the alternative comes from large values of the \\(F\\)-statistic. The \\(p\\)-value associated with the test is the probability of observing an \\(F\\)-statistic at least as large as the one observed if the null hypothesis is true. In R, obtain these \\(p\\)-values can be found with the command pf, which works much like pt. pf(3.997, df1 = 2, df2 = 19, lower = FALSE) ## [1] 0.03557299 Thus, our \\(p\\)-value is \\(p = 0.036\\). At the customary \\(\\alpha = 0.05\\) level of significance, we would reject the null hypothesis that there is no difference among the lakes, after accounting for the effect of age. In other words, we have found a statistically significant difference in the mercury content of fish across the three lakes. We can conduct this \\(F\\)-test in R using the anova function: anova(reduced.model, full.model) ## Analysis of Variance Table ## ## Model 1: log(hg) ~ age ## Model 2: log(hg) ~ age + site ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 21 1.17251 ## 2 19 0.82529 2 0.34722 3.9969 0.03558 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 An extended discussion of \\(t\\)-tests and \\(F\\)-tests. In ST512, we will encounter predominantly two types of tests, \\(t\\)-tests and \\(F\\)-tests. We have already seen how \\(t\\)-tests work. \\(t\\)-tests are used to test null hypotheses that have a single equality. In general notation, if we have a generic parameter \\(\\theta\\), a \\(t\\)-test tests \\(H_0\\): \\(\\theta = \\theta_0\\) vs. either the two-sided alternative \\(H_a\\): \\(\\theta \\ne \\theta_0\\) or either of the one-sided alternatives \\(H_a\\): \\(\\theta &lt; \\theta_0\\) or \\(H_a\\): \\(\\theta &gt; \\theta_0\\). One calculates a one- or two-tailed \\(p\\)-value based on whether the alternative hypothesis is one- or two-sided. \\(F\\)-tests are used to test null hypotheses that either multiple parameters or multiple combinations of parameters are simultaneously equal to 0. For example, in the fish data, we have just tested hypotheses \\(H_0 :\\beta_2 =\\beta_3 =0\\) vs. \\(H_a :\\beta_2 \\ne 0{\\rm \\; or\\; }\\beta_3 \\ne 0\\). The mechanics of an \\(F\\)-test are such that the only viable alternative is a two-sided alternative. (This is actually not entirely true, but we can take it as true for our purposes in ST512.) \\(P\\)-values are found by comparing \\(F\\)-statistics to \\(F\\)-distributions. \\(F\\)-distributions are specified by two separate degrees of freedom, which we call the numerator degrees of freedom (ndf) and the denominator degrees of freedom (ddf). The ndf will typically be the number of equalities needed to specify the null hypothesis, and the denominator df will be the number of df associated with the SSE for the full model. Note that \\(F\\)-statistics take only positive values. The larger the value of the \\(F\\)-statistic, the more evidence the data provide against the null. Thus, with an \\(F\\)-test, the \\(p\\)-value is always the area to the right of the observed statistic, or the probability of observing a test statistic at least as large as the value observed. Consequently, the terminology associated with an \\(F\\)-test can be mildly confusing: the \\(p\\)-value is always a one-tailed \\(p\\)-value, but it is used to test a two-sided alternative hypothesis. Finally, \\(F\\)-tests can also be used to test null hypotheses with single equalities. That is, we can use an \\(F\\)-test to test \\(H_0\\): \\(\\theta = \\theta_0\\), but with an \\(F\\)-test we can only consider the two-sided alternative \\(H_a\\): \\(\\theta \\ne \\theta_0\\). (To put this more in the form of an \\(F\\)-test, we might re-write the hypotheses as \\(H_0\\): \\(\\theta - \\theta_0 = 0\\) and \\(H_a\\): \\(\\theta - \\theta_0 \\ne 0\\).) Thus, in some sense, an \\(F\\)-test is a generalization of a \\(t\\)-test. However, only a \\(t\\)-test can be used to test \\(H_0\\): \\(\\theta =\\theta_0\\) vs. the one-sided alternatives \\(H_a\\): \\(\\theta &lt; \\theta_0\\) or \\(H_a\\): \\(\\theta &gt; \\theta_0\\). 2.7.1 Model utility tests We have just seen how \\(F\\)-tests can be used in the multiple regression model to test whether any subset of the partial regression coefficients are simultaneously equal to zero. Theres nothing stopping us from using the same idea to test whether all of the partial regression coefficients in a multiple regression model are simultaneously equal to zero. That is, in the general regression model \\[ y=\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +...+\\beta_k x_k +\\epsilon \\] we can test \\(H_0 :\\beta_1 =\\beta_2 =...=\\beta_k =0\\) vs. the alternative that at least one of the partial regression coefficients is not equal to zero. In this case, the reduced model is just the model $y=_0 +$, that is, one in which the response has no linear relationship with any of the predictors. This particular test is called the model utility test. Most statistical software packages, including R, calculate this test automatically. In R, the model utility test appears in the last line of output provided by a summary of a linear model. For example, consider again the full model from the fish example: full.model &lt;- lm(log(hg) ~ age + site, data = fish) summary(full.model) ## ## Call: ## lm(formula = log(hg) ~ age + site, data = fish) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.47117 -0.08896 0.03796 0.13910 0.31327 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.80618 0.15398 -11.730 3.80e-10 *** ## age 0.14309 0.01967 7.276 6.66e-07 *** ## siteBennett 0.07107 0.12910 0.550 0.5884 ## siteWaterville -0.26105 0.10817 -2.413 0.0261 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2084 on 19 degrees of freedom ## Multiple R-squared: 0.7971, Adjusted R-squared: 0.765 ## F-statistic: 24.88 on 3 and 19 DF, p-value: 8.58e-07 Here, the last line of output tells us that the \\(F\\)-statistic for the model utility test is \\(F_{3, 19} = 24.88\\). If we compare this to an \\(F\\)-distribution with 3 and 19 df, the \\(p\\)-value is infinitesimal, suggesting that there is overwhelming evidence against the null hypothesis that neither age nor lake are associated with mercury content in fish. Although the model utility test is commonly computed by statistical software and has a grandiose name, it is rarely an interesting test. Rejecting the null in the model utility test is usually not an impressive conclusion. You may have also noticed that in SLR the model utility test always provided a \\(p\\)-value that was exactly equal to the \\(p\\)-value generated for the test of \\(H_0\\): \\(\\beta_1 =0\\) vs. \\(H_a\\): \\(\\beta_1 \\ne 0\\). Can you figure out why this is so? 2.8 (Multi-)Collinearity What is collinearity? To quote the Quinn &amp; Keough text (p. 127), One important issue in multiple linear regression analysis, and one that seems to be ignored by many biologists who fit multiple regression models to their data, is the impact of correlated predictor variables on the estimates of parameters and hypothesis tests. If the predictors are correlated, then the data are said to be effected by (multi)collinearity.  Lack of collinearity is also very difficult to meet with real biological data Perfect collinearity occurs when there is a linear dependency in the design matrix. That is to say, one of the predictors is exactly equal to a linear combination of the other predictors. Assuming that you are on your toes, you should be able to detect and avoid perfect collinearity. However, if predictors are strongly (but nor perfectly) correlated, trouble still lurks. Even worse, when there are many predictors relative to the number of data points, collinearity is nearly inevitable. Mathematically, calculating \\(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\) with strong (but not perfect) collinearity is numerically unstable, and tends to magnify rounding errors (think of dividing by a number very close to, but not equal to zero). Geometrically, the plane that we are trying to fit to the cloud of data points is not well anchored. It is unstable, in the sense that small changes in the data can have large impacts on the estimated regression coefficients. As Quinn &amp; Keough say in their text (p. 127): Small changes in data or adding or deleting one of the predictor variables can change the estimated regression coefficients considerably, even changing their sign (Bowerman &amp; OConnell 1990). This instability is not the hallmark of a trustworthy model. As a consequence of this instability, the standard errors of the partial regression coefficients can be large. This makes it difficult, if not impossible, to have confidence in our inferences about the estimated partial regression coefficients. Collinearity is not a problem for prediction, however. Quoting Quinn &amp; Keough once more (p. 127) as long as we are not extrapolating beyond the range of our predictor variables and we are making predictions from data with a similar pattern of collinearity as the data to which we fitted our model, collinearity doesnt necessarily prevent us from estimating a regression model that fits the data well and has good predictive power (Rawlings et al. 1998). It does, however, mean that we are not confident in our estimates of the model parameters. A different sample from the same population of observations, even using the same values of the predictor variables, might produce very different parameter estimates. Collinearity is usually assessed by a variance inflation factor (VIF). A separate VIF is calculated for each predictor in the model. To calculate the VIF for predictor \\(x_j\\), do the following: Regress \\(x_j\\) against all other predictors. That is, fit a new regression model in which \\(x_j\\) is the response. Note that the actual response \\(y\\) is not included in this model. Calculate \\(R^2\\). The VIF associated with predictor \\(x_j\\) is \\[ 1/\\left(1-R^2 \\right) \\] The usual rule of thumb is that a VIF \\(\\geq\\) 10 indicates strong enough collinearity that additional measures should be taken. As always, dont take the bright-line aspect of this rule too seriously; a VIF of 9.9 is not meaningfully different from a VIF of 10.1.8 There is no magic solution to collinearity. If two predictors and a response vary in concert, then it is difficult (if not impossible) to tease apart the effect of one predictor on the response from the effect of the other predictor with a statistical model. Each of the following techniques tries to stabilize the estimated partial regression coefficients at the expense of accepting a (hopefully) small bias.9 Omit predictors that are highly correlated with other predictors in the model. The justification usually offered here is that highly correlated predictors may just be redundant measures of the same thing. As an example, if we are studying lakes, the amount of sediment in the water and the clarity of the water may be two different variables that are measuring the same characteristic. If this is true, there is little to be gained by including both variables as predictors in a regression model. Use principal components analysis (PCA) to reduce the number of predictors, and use principal components as predictors. PCA is a multivariate statistical method that takes several variables and produces a smaller number of new variables (the principal components) that contain the majority of the information in the original variables. The advantage of using a principal component as a predictor is that different principal components are guaranteed to be independent of one another, by virtue of how they are calculated. The major disadvantage of using principal components is that the newly created predictors (the principal components) are amalgams of the original variables, and thus it is more difficult to assign a scientific interpretation to the partial regression coefficients associated with each. So, using PCA to find new predictors yields a more robust statistical model, albeit at the expense of reduced interpretability. 2.9 Variable selection: Choosing the best model So far, weve learned how to construct and fit regression models that can potentially include many different types of terms, including multiple predictors, transformations of the predictors, indicator variables for categorical predictors, interactions, and polynomial terms. Even a small set of possible predictors can produce a large array of possible models. How do we go about choosing the best model? First, we have to define what we mean by a best model. What are the qualities of a good model? Parsimony. We seek a model that adequately captures the signal in the data, and no more. There are both philosophical and statistical reasons for seeking a parsimonious model. The statistical motivation is that a model with too many unnecessary predictors is too flexible, and prone to detect spurious patterns that would not appear in repeated samples from the same population. We call this phenomenon overfitting, or fitting the noise. In technical terms, fitting a model with too many unnecessary predictors increases the standard errors of the parameter estimates. Interpretability. We often want to use regression models to understand associations between predictors and the response, and to shed light on the data-generating process. This argues for keeping models simple. There are occasions where the sole goal of regression modeling is prediction, and in this case interpretability is less important. This is often the case in so-called big data applications, where prediction is the primary goal, and understanding is only secondary. 3.Statistical inference. As scientists, we are not interested merely in describing patterns in our data set. Instead, we want to use the data to draw inferences about the population from which the sample was drawn. Therefore, we want a model that meets the assumptions of regression, so that we can use regression theory to draw statistical inferences. In statistical jargon, the process of choosing which predictors to include in a regression model and which to leave out is called variable selection. More generally, beyond a regression context, the problem of identifying the best statistical model is called model selection. We will look at several automated routines for choosing the best model. Helpful as these routines are, they are no substitute for intelligent analysis. Feel free to use an automated variable selection route to get started, but dont throw your brain out the window in the process. Also, remember that there is a hierarchy among model terms in regression. Most automated variable selection routines do not incorporate this hierarchy, so we must impose it ourselves. In most cases, the following rules should be followed: Models that include an interaction between predictors should also include the predictors individually. Models that include polynomial powers of a predictor should include all lower-order terms of that predictor. Automated variable selection routines can be grouped into two types: ranking methods and sequential methods. 2.9.1 Ranking methods Ranking methods work best when it is computationally feasible to fit every possible candidate model. In these situations, we calculate a metric that quantifies the models adequacy, and select the model that scores the best with respect to that metric. There are several possible metrics to choose from, and they dont always point to the same best model. We will look at three different metric that enjoy wide use. Throughout this section, we will refer to the number of predictors in a model, and denote this quantity by \\(k\\). To remove ambiguity, what we mean in this case is the number of partial regression coefficients to be estimated. So, for example, we would say that the model \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1^2 + \\beta_4 x_1 x_2 + \\epsilon\\) has \\(k = 4\\) predictors. Before beginning, we should note that \\(R^2\\) is {} a good choice for a ranking metric. This is because adding a predictor will never decrease \\(R^2\\). Therefore, $R^2 $ can only be used to compare models that have the same number of predictors. Adjusted \\(R^2\\). Adjusted \\(R^2\\) is a penalized version of \\(R^2\\) that imposes a penalty for each additional parameter added to the model. The (rather opaque) formula for adjusted \\(R^2\\) is \\[ {\\rm Adj-}R^2 =1-\\frac{n-1}{n-\\left(k+1\\right)} \\left(1-R^2 \\right) \\] The model with the largest Adj-$R^2 $ is considered best. PRESS statistic (PRESS = PRedicted Sum of Squares) Step 1.Remove the first data point. Step 2: Fit the model to the remaining data. Step 3. Use the model from step 2 to predict the removed data point, $_1^* $. Step 4. Add the first data point back to the data set. Step 5. Repeat steps 1-4 for each data point in turn. The PRESS statistic is $_{i=1}^n(y_i -_i* )2 $. The model with the smallest PRESS statistic is considered best. The PRESS statistic is interesting because it is an example of a more general idea called cross-validation. The idea of cross-validation is to remove one or more points from a data set, fit the model to the remaining data, and use the fitted model to predict the data point(s) that were removed. A good model should accurately predict the removed data point(s). There are many variations on the idea of cross-validation, including schemes that remove a subset of the data instead of just a single data point. The removed data are often called the testing data, and the data to which the model are fit (i.e., the data that are not removed) are called the training data. AIC (Akaikes Information Criterion) AIC is also a penalized goodness-of-fit measure, like adjusted \\(R^2\\). AIC enjoys a bit more theoretical support than adjusted \\(R^2\\), and is more versatile, although its derivation is a bit more opaque. (As the name suggests, AIC has its roots in information theory.) The general form of AIC involves math that is beyond the scope of ST 512, but we can write down the specific formula for regression models, which is \\[ AIC=n\\ln \\left[\\frac{SSE}{n} \\right]+2\\left(k+1\\right) \\] The smallest value of AIC is best. (Smaller here means algebraically smaller  that is, further to the left on the number line  not closer to zero.) Despite its theoretical support, AIC tends to favor models with too many predictors. 2.9.2 Sequential methods Sequential methods work best for problems where the set of candidate models is so vast that fitting all the candidate models is not feasible. Because computers are faster today than they were years ago, it is now feasible to fit a large number of candidate models quickly, and thus sequential methods are less necessary today than they were years ago. Nevertheless, the ideas are straightforward. There are three different types of sequential methods, based on the direction in which the model is built. In forward selection, we begin with the simplest possible model (namely, \\(y = \\beta_0 + \\epsilon\\)), and grow the model by adding predictors to it. In backwards elimination, we start with the most expansive possible model, and shrink it by removing unnecessary predictors. In stepwise selection, we start with the simplest possible model (namely, \\(y = \\beta_0 + \\epsilon\\)), and then merge forwards and backwards steps, either growing and shrinking the model until converging on one that cannot be improved. Stepwise selection is used more often than the other two variants. Each of the three procedures could possibly lead to a different best model. Here is the algorithm for forward selection: Initiation step: Start with the model \\(y=\\beta_0 +\\epsilon\\). This is the initial working model. Iteration step: Fit a set of candidate models, each of which differs from the working model by the inclusion of a single additional model term. Be aware that the set of candidate models must abide our rules of thumb about hierarchy. (That is, we wouldnt consider a model like \\(y = \\beta_0 + \\beta_1 x_1 x_2 + \\epsilon\\).) Ask: Do any of the candidate models improve upon the working model? If the answer to this question is yes, then choose the model that improves upon the working model the most. Making this model the working model, and begin a new iteration (return to step 2). (Termination step): If the answer to this question is no, then stop. The current working model is the final model. The algorithm for backwards elimination is similar: Initiation step: Start with the most expansive possible model. Usually, this will be the model with all possible predictors, and potentially with all possible first- (and conceivably second-)order interactions. Note that we can consider a quadratic term to be an interaction of a predictor with itself in this context. This is the initial working model. Iteration step: Fit a set of candidate models, each of which differs from the working model by the elimination of a single model term. Again, be aware that the set of candidate models must abide our rules of thumb about hierarchy, so (for example) we wouldnt consider a model that removes \\(x_1\\) if the interaction \\(x_1 x_2\\) is still in the model. (Same as forward selection.) Ask: Do any of the candidate models improve upon the working model? If the answer to this question is yes, then choose the model that improves upon the working model the most. Making this model the working model, and begin a new iteration (return to step 2). (Termination step): If the answer to this question is no, then stop. The current working model is the final model. The algorithm for stepwise selection initiates the algorithm with \\(y=\\beta_0 +\\epsilon\\), and then forms a set of candidate models that differ from the working model by either the addition or elimination of a single model term. The algorithm proceeds until the working model cannot be improved either by a single addition or elimination. So far, we have been silent about how we determine whether a candidate model improves on the working model, and if so, how to find the candidate model that offers the most improvement. We can use any of our ranking methods at this step. Historically, \\(p\\)-values have often been used to determine whether a candidate model improves on the working model, though this practice has largely been discontinued. The argument against it is that using \\(p\\)-values for variable selection destroys their interpretation in the context of hypothesis testing. This being said, any statistical tests can only be regarded as descriptive if the tests occur in the context of a model that has been identified by model selection. Statistical tests only have their advertised properties if decisions about which predictors to include are made before looking at the data. The `step routine in R uses AIC as its default criterion for adding or dropping terms in stepwise selection. Here is an example of stepwise selection with the cheese data, considering only models without interactions or polynomial terms. fm0 &lt;- lm(taste ~ 1, data = cheese) # the initial model y = b0 + eps step(fm0, taste ~ Acetic + H2S + Lactic, data = cheese) ## Start: AIC=168.29 ## taste ~ 1 ## ## Df Sum of Sq RSS AIC ## + H2S 1 4376.7 3286.1 144.89 ## + Lactic 1 3800.4 3862.5 149.74 ## + Acetic 1 2314.1 5348.7 159.50 ## &lt;none&gt; 7662.9 168.29 ## ## Step: AIC=144.89 ## taste ~ H2S ## ## Df Sum of Sq RSS AIC ## + Lactic 1 617.2 2669.0 140.65 ## &lt;none&gt; 3286.1 144.89 ## + Acetic 1 84.4 3201.7 146.11 ## - H2S 1 4376.7 7662.9 168.29 ## ## Step: AIC=140.65 ## taste ~ H2S + Lactic ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 2669.0 140.65 ## + Acetic 1 0.55 2668.4 142.64 ## - Lactic 1 617.18 3286.1 144.89 ## - H2S 1 1193.52 3862.5 149.74 ## ## Call: ## lm(formula = taste ~ H2S + Lactic, data = cheese) ## ## Coefficients: ## (Intercept) H2S Lactic ## -27.592 3.946 19.887 There is no prescription for building models automatically. (If there were, someone would have written a computer package implementing the procedure and would be filthy rich.) Here is one cycle of steps for building a regression model, courtesy of Dr. Roger Woodard, formerly of NCSU: Examine univariate (ST 511) summaries of the data (summary statistics, boxplots, etc.). Identify unusual values or possible problems. (Dont take it on faith that your data are all correct!) Examine scatterplots with all variables. Find variables that are closely correlated with the response and with each other. Candidate model selection: Identify a model that includes relevant variables. Use automated selection procedures if you wish. 4: Assumption checking: Check (standardized) residuals. Determine if polynomial terms or transformations may be needed. 5: Examine collinearity diagnostics. Inspect VIFs and pairwise correlations between variables. Decide if some variables may be removed or added. 6: Revision. Add or remove terms based on steps 4-5. Return to step 3. 7: Prediction and testing: Consider validating the model with a sample that was not included in building the model. 2.10 Non-linear regression We have shown how transformations and polynomial regression can be used to fit non-linear relationships between two variables using the machinery of linear regression. However, machinery now exists to fit fundamentally non-linear models to data using the same least-squares criterion that we use to estimate parameters in the linear model. The computation involved in fitting a non-linear model is fundamentally different from the computation involved in a linear model. A primary difference is that there is no all-purpose formula like \\(\\hat{\\mathbf{\\beta}}=\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39;\\mathbf{Y}\\) available for the non-linear model. Therefore, parameter estimates (and their standard errors) have to be found using a numerical algorithm. (Well see more about what this means in a moment.) However, these algorithms are sufficiently well developed that they now appear in most common statistical software packages, such as R, SAS, or others. In R, the command that we use to fit a non-linear model is `nls, for [n]on-linear [l]east [s]quares. In SAS, non-linear models can be fit using PROC NLIN. Ex. Puromycin. This example is taken directly from the text , by D.M. Bates and D.G. Watts Bates and Watts (1988). The data themselves are from Treloar (1974, MS Thesis, Univ of Toronto), who studied the relationship between the velocity of an enzymatic reaction (the response, measured in counts / minute\\(^2\\)) vs. the concentration of a particular substrate (the predictor, measured in parts per million). The experiment was conducted in the presence of the puromycin Puromycin. The data are shown below. puromycin &lt;- read.table(&quot;data/puromycin.txt&quot;, head = T, stringsAsFactors = T) with(puromycin, plot(velocity ~ conc, xlab = &quot;concentration&quot;, ylab = &quot;velocity&quot;)) It is hypothesized that these data can be described by the Michaelis-Menten model for puromycin kinetics. The Michaelis-Menten model is: \\[ y=\\frac{\\theta_1 x}{\\theta_2 +x} +\\epsilon \\] We continue to assume that the errors are iid normal with mean 0 and unknown but constant variance, i.e., \\(\\epsilon_i \\sim \\mathcal{N}\\left(0,\\sigma_{\\epsilon}^2 \\right)\\). With non-linear models, it is helpful if one can associate each of the parameters with a particular feature of the best-fitting curve. With these data, it seems that the best fitting curve is one that will increase at a decelerating rate until it approaches an asymptote. A little algebra shows that we can interpret \\(\\theta_1\\) directly as the asymptote (that is, the limiting value of the curve as \\(x\\) gets large), and \\(\\theta_2\\) as the value of the predictor at which the fitted curve reaches one-half of its asymptotic value. To estimate parameters, we can define a least-squares criterion just as before. That is to say, the least-squares estimates of \\(\\theta_1\\) and \\(\\theta_2\\) will be the values that minimize \\[ SSE=\\sum_{i=1}^ne_i^2 = \\sum_{i=1}^n\\left(y_i -\\hat{y}_i \\right)^2 =\\sum_{i=1}^n\\left(y_i -\\left[\\frac{\\hat{\\theta }_1 x_i }{\\hat{\\theta }_{2} +x_i } \\right]\\right)^2 \\] However, unlike with the linear model, there is no formula that can be solved directly to find the least-squares estimates. Instead, the least-squares estimates (and their standard errors) must be found using a numerical minimization algorithm. That is, the computer will use a routine to iteratively try different parameter values (in an intelligent manner) and proceed until it thinks it has found a set of parameter values that minimize the SSE (within a certain tolerance). While we can trust that the numerical minimization routine implemented by R or SAS is a reasonably good one, all numerical minimization routines rely critically on finding a good set of starting values for the parameters. That is, unlike in a linear model, we must initiate the algorithm with a reasonable guess of the parameter values that is in the ballpark of the least-squares estimates. Here is where it is especially beneficial to have direct interpretations of the model parameters. Based on our previous analysis, we might choose a starting values of (say) \\(\\theta_1 = 200\\) and \\(\\theta_2 = 0.1\\). (Note that R will try to find starting values if they arent provided. However, the documentation to nls says that these starting values are a very cheap guess.) Equipped with our choice of starting values, we are ready to find the least-squares estimates using nls: fm1 &lt;- nls(velocity ~ theta1 * conc / (theta2 + conc), data = puromycin, start = list(theta1 = 200, theta2 = 0.1)) summary(fm1) ## ## Formula: velocity ~ theta1 * conc/(theta2 + conc) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## theta1 2.127e+02 6.947e+00 30.615 3.24e-11 *** ## theta2 6.412e-02 8.281e-03 7.743 1.57e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.93 on 10 degrees of freedom ## ## Number of iterations to convergence: 6 ## Achieved convergence tolerance: 6.085e-06 In the call to nls, the first argument is a formula where we specify the non-linear model that we wish to fit. In this data set, velocity is the response and conc is the predictor. The last argument to nls is a list of starting values. The list contains one starting value for each parameter in the model. (R note: In R, `lists are like vectors, except that lists can contain things other than numbers.) The output shows that the least squares estimates are \\(\\hat{\\theta}_1 =212.7\\) and \\(\\hat{\\theta}_2 =0.064\\). We also get estimated standard errors for each of the parameters, as well as \\(t\\)-tests of \\(H_0\\): \\(\\theta =0\\) vs. \\(H_a\\): \\(\\theta \\ne 0\\). Note that the \\(t\\)-tests are not particularly useful in this case  theres no reason why we would entertain the possibility that either \\(\\theta_1\\) or \\(\\theta_2\\) are equal to 0. The last portion of the output from nls tells us about the performance of the numerical algorithm that was used to find the least-squares estimates. We wont delve into this information here, but if you need to use non-linear least squares for something important, be sure to acquaint yourself with what this output means. Like linear least-squares, there are cases where non-linear least squares will not work (or will not work well), and it is this portion of the output that will give you a clue when youve encountered one of these cases. We can examine the model fit by overlaying a fitted curve: xvals &lt;- seq(from = min(puromycin$conc), to = max(puromycin$conc), length = 100) with(puromycin, plot(velocity ~ conc, xlab = &quot;concentration&quot;, ylab = &quot;velocity&quot;)) lines(xvals, predict(fm1, newdata = data.frame(conc = xvals)), col = &quot;red&quot;) It is instructive to compare the fit of this non-linear model with the fit from a few polynomial regressions. Neither the quadratic nor the cubic models fits very well in this case. Polynomial models often have a difficult time handling a data set with an asymptote. In this case, the Michaelis-Menten model clearly seems preferable. quad &lt;- lm(velocity ~ conc + I(conc^2), data = puromycin) cubic &lt;- lm(velocity ~ conc + I(conc^2) + I(conc^3), data = puromycin) quad.coef &lt;- as.vector(coefficients(quad)) quad.fit &lt;- function(x) quad.coef[1] + quad.coef[2] * x + quad.coef[3] * x^2 cubic.coef &lt;- as.vector(coefficients(cubic)) cubic.fit &lt;- function(x) cubic.coef[1] + cubic.coef[2] * x + cubic.coef[3] * x^2 + cubic.coef[4] * x^3 with(puromycin, plot(velocity ~ conc, xlab = &quot;concentration&quot;, ylab = &quot;velocity&quot;, ylim = c(min(velocity), 230))) curve(quad.fit, from = min(puromycin$conc), to = max(puromycin$conc), add = TRUE, col = &quot;blue&quot;) curve(cubic.fit, from = min(puromycin$conc), to = max(puromycin$conc), add = TRUE, col = &quot;forestgreen&quot;) legend(x = 0.6, y = 100, legend = c(&quot;quadratic&quot;, &quot;cubic&quot;), col = c(&quot;blue&quot;, &quot;darkgreen&quot;), lty = &quot;solid&quot;, bty = &quot;n&quot;) 2.11 Leverage, influential points, and standardized residuals Recall that in SLR, a data point can have undue influence on the regression model if the value of the predictor, \\(x\\), is for away from \\(\\bar{x}\\). The same notion applies in MLR: data points can be unduly influential if their combination of predictors lies far away from the center of mass of the other predictors. However, with multiple predictors, it is harder to detect influential points visually. The leverage of a data point is a measure of its distance from the center of mass of the predictors, and hence its influence. The formula for calculating leverages is complicated, so well use a computer to calculate them. Leverages are usually denoted with the letter \\(h\\), so the leverage for the \\(i\\)th data point is \\(h_i\\). If we looked at the equation for a leverage, however, we would discover that leverages are strictly functions of the predictors, and do not depend on th response. To calculate leverages in R, first pass the regression model object to the function influence.lm. This function returns several diagnostic measures; the leverages are contained in the component called hat. To extract the leverages, use code like the following: fm1 &lt;- lm(BAC ~ weight + beers, data = beer) fm1.diagnostics &lt;- lm.influence(fm1) lev &lt;- fm1.diagnostics$hat Here is a look at some of the leverage values for the BAC data: head(cbind(beer, lev)) ## BAC weight gender beers beers.c weight.c lev ## 1 0.100 132 female 5 0.1875 -39.5625 0.1118535 ## 2 0.030 128 female 2 -2.8125 -43.5625 0.1948842 ## 3 0.190 110 female 9 4.1875 -61.5625 0.5176556 ## 4 0.120 192 male 8 3.1875 20.4375 0.2029871 ## 5 0.040 172 male 3 -1.8125 0.4375 0.1111125 ## 6 0.095 250 female 7 2.1875 78.4375 0.2588899 Not surprisingly, the point with the greatest leverage is the 110-lb person who drank 9 beers. What qualifies as a large leverage? Quinn &amp; Keoughs text, p. 95, gives one possible rule of thumb: a useful criterion is that any observation with a leverage value greater than \\(2\\left(k+1\\right)/n\\) should be checked (Hoaglin &amp; Welsch 1978). {} Data points associated with large leverages tend to have smaller (raw) residuals. A better choice than analyzing the raw residuals is to analyze the standardized residuals. The formula for a standardized residual is: \\[ e_i^{(s)} =\\frac{e_i}{s_\\epsilon \\sqrt{1-h_i} } \\] The benefit of standardized residuals is that if our model assumptions are appropriate, then they should behave like an iid sample from a normal distribution with mean 0 and variance 1. That is to say, most standardized residuals should be between -2 and +2, and only a few should be \\(&lt; -3\\) or \\(&gt; +3\\). Some texts call these studentized residuals instead of standardized residuals. R does not have a built-in function for calculating standardized residuals. However, there is a library of functions called the MASS library that contains the function stdres. (MASS is an acronym for , which is one of the standard more advanced texts for using R. It is written by W.N. Venables and B.D. Ripley.) library(MASS) stdres(fm1) ## 1 2 3 4 5 6 7 ## 0.8307561 -0.3611695 1.4198337 -1.0767727 0.2664003 0.6708175 1.6189097 ## 8 9 10 11 12 13 14 ## -1.6125272 -1.6623992 1.2179975 -0.2650284 0.1241508 -0.8509379 -0.0485307 ## 15 16 ## 1.0854287 -0.6259662 Cooks distance. Leverages and standardized residuals can be combined into various quantities that measure the influence each observation has on the fitted regression line. One of the most popular of these are Cooks distance, \\(D_i\\). In R, if you pass a regression model to the command plot, it will produce four (rather sophisticated) diagnostic plots. The last of these shows Cooks distance. plot(fm1) Observations with large values of Cooks distance merit greater scrutiny. Bibliography "],["bibliography.html", "Bibliography", " Bibliography "]]
