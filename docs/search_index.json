[["index.html", "Statistical analysis of designed experiments: yesterday, today, and tomorrow Preface", " Statistical analysis of designed experiments: yesterday, today, and tomorrow Kevin Gross 2025-11-04 Preface This is a set of course notes to accompany the second semester of a traditional graduate-level sequence in statistical methods. These notes aim both to introduce the linear model (e.g., regression and ANOVA) and adjacent methods and to serve as a reference for later consultation. Philosophy These notes take the following perspectives. Statistics is nonintuitive. When it comes to statistics, researchers cannot necessarily rely on their common sense to lead them towards correct answers. Statistical reasoning is non-intuitive (Kahneman (2011)), and the foundational ideas of statistics are elusive. Therefore statistical literacy must be learned. The primary goal of this course is to sharpen students’ statistical literacy so that they may become more effective researchers. The route to conceptual understanding is the detailed study of basic methods. However, one does not develop a deep conceptual understanding merely by discussing concepts. Instead, conceptual understanding is honed in part by studying the details of particular methods to understand why those details matter. When we study the details of a method, the expectation is not that the student will remember those details in perpetuity. Indeed, practicing scientists are unlikely anyway to remember details about statistical methods that they do not use routinely. (This is not a knock on practicing scientists, but is instead simply a statement about the limitations of human memory.) Instead, the point of studying statistical methods in detail is to strengthen conceptual understanding by exploring statistical methods at a reasonably deep level. Examining details now will also make those details more recognizable if and when one faces a future data-analysis task that requires re-engaging those details. That said, the ultimate emphasis of this course is not on the details of the methods, but is instead on the ideas, concepts, and reasoning that underlie statistical thinking. I hope that these notes will deepen readers’ conceptual understanding of statistics and by doing so strengthen their effectiveness as scientists. Except when it comes to sums-of-squares decompositions in ANOVA. The exception to the statement above is ANOVA and the associated sums-of-squares decompositions. Sums-of-squares decompositions and the associated ANOVA tables make sense only when engaging the underlying linear algebra. Without engaging the linear-algebra foundations, sums-of-squares tables fill texts with inscrutable tables that are poor vehicles for developing a conceptual understanding.1 ANOVA is still worth learning — especially for the analysis of designed experiments — but unless the underlying linear algebra is engaged, ANOVA is more usefully approached as a special case of a regression model. For this reason, these notes introduce regression modeling first and ANOVA second, reversing the path taken by most texts. Confidence intervals deserve more emphasis, and hypothesis tests less. Hypothesis tests have become the primary route to inference in contemporary science, likely because they are the de facto evidentiary standard for announcing results in the scientific literature. This is unfortunate, because statistical significance provides only the thinnest of summaries of pattern in data. Confidence intervals, on the other hand (or even standard errors), are often relegated to a secondary role, even though they provide a richer basis for characterizing pattern and uncertainty. In the fullness of time, these notes will seek to promote the reporting of confidence intervals or standard errors as opposed to hypothesis tests as the primary vehicle for inference. Simplicity in statistical analysis is a virtue. Contemporary statistical software allows anyone to fit complex statistical models. However, just because one can fit a complex model does not mean that one should. For a statistical analysis to have scientific value, it must be understood by both the analyst and the analyst’s audience. Unfortunately, many contemporary statistical methods produce opaque analyses that are impossible for the informed reader to understand without a substantial and unusual investment of time and energy. (It doesn’t help that, in the scientific literature, the details of contemporary analyses are buried in supplementary material that escapes the scrutiny of most reviewers, but that’s another matter.) As a result, informed and well-intentioned readers of the scientific literature have little choice but to accept the analysis at face value without understanding the genesis of the announced results. This state of affairs does not serve science well. It is high time for the scientific community to ask whether complex but opaque statistical analyses are in the best interest of science. For most scientific studies, a simple and transparent analysis provides a more trustworthy vehicle to understanding for both the analyst and the analyst’s audience. These notes will emphasize methods that, when studied, are transparent enough to promote such an understanding. Visualization is everything. No one is convinced by a \\(p\\)-value in isolation, or at least no one should be, given the ease of making errors in statistical analysis. When an analysis suggests a pattern in data, the best way to understand the pattern is to visualize it with a thoughtfully constructed graphic. Unfortunately, these notes in their current state are not as richly illustrated as they should be. Eventually, I hope the notes will contain a full set of graphics that exemplify how to visualize patterns revealed by statistical analysis.2 Scope and coverage These notes form the basis for an intermediate course in statistical analysis. These notes do not start from the very beginning, and they presume a basic knowledge of the fundamentals of (frequentist) statistical inference, similar to what one might see in an introductory statistics course. Breiman (2001) wrote a provocative paper some twenty years ago describing two statistical cultures: one of data modeling and a second of algorithmic modeling, thus anticipating the era of machine learning and artificial intelligence in which we increasingly seem to reside. These notes fall squarely within (and even celebrate!) the former culture of data modeling as a path to scientific understanding. Moreover, it seems to me that the data-modeling culture contains at least two different subcultures that map to disciplines that either learn from designed experiments vs. disciplines that rely primarily on so-called observational data.3 The experimental-data culture tends to involve disciplines in the life sciences, favors ANOVA modeling, uses frequentist inference, and codes its models in SAS. The observational-data culture tends to involve disciplines in the social and environmental sciences, favors regression modeling, increasingly embraces Bayesian analysis, and codes its models in R or Python. These notes aim to serve both the experimental and observational subcultures. The saving grace that allows us to do so is that the core statistical model in both subcultures is the linear statistical model, which encompasses both regression and ANOVA. Of course, most graduate students will need to learn specialized statistical methods that are popular in their own field of study. These notes are not meant to cover these specialized methods, and thus they are not meant to embody the whole of statistics. However, study of regression and ANOVA provides an opportunity to master core tools and provides a springboard to the study of more specialized and possibly discipline-specific methods. These notes also deal exclusively with so-called “frequentist” statistical inference. We do not engage Bayesian methods yet, although some Bayesian coverage is eventually forthcoming. This class is also firmly situated in the study of low-dimensional statistical models. We value parsimony, and we take the view that well constructed models are worthy objects of study in their own right. More concretely, we seek to construct statistical models with parameters that correspond to natural phenomena of interest. Algorithmic modeling (that is, machine learning) is outside the scope of these notes. Mathematical level These notes do not assume knowledge of or use any math beyond arithmetic and basic probability. This basic probability includes an understanding of random variables, standard distributions — primarily Gaussian (normal) distributions, but also binomial and Poisson — and basic properties of means and variances. Students who are willing to engage the math a bit more deeply will find that doing so provides a more satisfying path through the material and leads to a more durable understanding. Without knowing the math underneath, one can only learn statistical methods as different recipes in a vast cookbook, a tedious task that taxes the memory and gives statistics courses their reputation for drudgery. For those who are so inclined, learning a bit of the mathematical theory reveals how the methods we study connect with one another, and thus provides a scaffolding to organize the methods sensibly and coherently. Moreover, the underpinning mathematics can be understood with a minimum of calculus. Linear algebra, however, is more essential. Indeed, the linear models that we study are, ultimately, exercises in linear algebra. These notes assume no previous familiarity with linear algebra, and so we will not emphasize the linear algebra underpinnings of the methods. In the fullness of time, I hope that these notes will eventually include sidebars that present the linear algebra underneath the methods, for interested readers. In this day and age, one might ask why it’s necessary to understand the math at all. Indeed, the internet makes it easy to quickly find code for any standard analysis.4 In such a world, the primary task facing an analyst is not so much to get the computer to give you an answer, but instead to confirm that the answer is in fact the one you want. Towards this end, knowing a bit about the math behind the methods makes it possible to determine whether the computer output you’ve obtained is indeed the analysis you hoped for. Throughout, we will try to emphasize simple, quick calculations that can be used to verify that computer output is correct, or indicate that something needs to be fixed. Computing The first portion of these notes (focused on regression) presents analyses in R, while the latter portion (focused on designed experiments) presents analyses in SAS. In the fullness of time, I hope that these notes will include complete code for conducting analyses in both R and SAS, but that is a work in progress. While the notes examine the R and SAS implementation of the methods that it presents, these notes are not intended as a complete guide for learning either R or SAS from square one. The internet abounds with resources for learning the basics of R, and I would not be able to improve on those resources here. In many cases I provide R code for the sake of illustration, but—especially when it comes to data wrangling and to graphics—the code is not meant to be authoritative. ST 512 students will receive instruction in R and SAS coding in the laboratory component fo the course. Readers interested in using R professionally would be well served by consulting Hadley Wickham’s tidyverse style guide. The ideas therein have helped me write substantially cleaner code, even if I haven’t had the discipline to adhere to those ideas in all the code in these notes. That said, the R code in these notes does not fully embrace the piping style of the tidyverse ecosystem and the associated graphical facilities of ggplot. I take this approach because the focus of these notes is on fitting and visualizing traditional statistical models, and it seems to me that the conventional style of R coding is still best suited for this purpose. The piping style of the tidyverse seems better suited to data-science tasks such as wrangling with and visualizing large data sets. As for ggplot, I prefer the style of coding in R’s native graphical facilities, although ggplot can certainly produce high-quality graphics with relatively few lines of code. As a practical matter, these notes are prepared in bookdown (Xie (2022)). While it is possible to compile both R and SAS code on the fly in bookdown, the extensive output produced by SAS does not serve these notes well. As a consequence, SAS output is condensed to show only the most salient portions of the output. LLMs in statistical analysis LLMs (e.g., ChatGPT, Claude, etc.) can be a useful complement to contemporary data analysis. In particular, LLMs can reduce the tedium of coding routine tasks and suggest solutions to coding riddles that extend beyond the routine. That said, as of this writing, LLMs are not reliable for data analyses of moderate or substantial complexity. In my experience, LLMs routinely make conceptual errors—which are often fatal to the analysis—when presented with non-trivial analysis tasks. Unfortunately, LLMs seem unable to recognize their own mistakes, and they present flawed solutions with brazen confidence. To use LLMs safely in such settings, human analysts must be able to recognize when LLMs have erred. To develop that understanding, though, learners must first study and master the basics of statistical analysis. Therefore, students still need to understand basic statistical methods, not because LLMs can’t be trusted with basic analyses, but because mastery of the basics is mandatory for safely using LLMs to assist the more complex analyses that most students will face during their graduate careers. Format of the notes Advanced sections are indicated by section titles that begin with stars (\\(^\\star\\)). Shorter sidebars for enrichment appear in gray text and are offset by horizontal rules (like the one following the acknowledgments). This material may be skipped without loss. Sections that are in an early and rougher stage of development are indicated with section titles shown in italics. A word on the title As statistical practice evolves, new methods come and go, but the linear model remains a cornerstone of applied data analysis. The primary objective of these notes is to equip researchers to analyze their own data using contemporary methods. That said, an exclusive focus on contemporary methods risks making the historical literature inaccessible. These notes also aim to familiarize readers with past statistical practice so that they can understand the experiments of yesteryear. I think this is something of a unique aspect to these notes, and I have chosen the title accordingly.5 Acknowledgments and license I am deeply indebted to the R community (R Core Team (2021)) for their project which has revolutionized data analysis in our times. I also thank the developers of bookdown for providing the platform for these notes (Xie (2022)). These notes are provided under version 3 of the GNU General Public License. Bibliography Breiman, Leo. 2001. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” Statistical Science 16 (3): 199–231. Kahneman, Daniel. 2011. Thinking, Fast and Slow. Macmillan. R Core Team. 2021. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Xie, Yihui. 2022. Bookdown: Authoring Books and Technical Documents with r Markdown. https://github.com/rstudio/bookdown. Again, to be clear, the formulas lack a compelling underlying logic because we are not engaging the linear algebra. If we embraced the linear algebra foundations, then the underlying logic would be clear indeed.↩︎ I’ve long grappled with, and continue to grapple with, whether an emphasis on data visualization creates a burden for learners with impaired vision. The goal of these notes is to create a resource that learners can use to master statistical concepts. While data visualization is one aid to understanding, alternate, non-visual paths to deep understanding exist are available to learners of all types. The ultimate learning objective is statistical reasoning, not visual acuity. I aspire to provide materials that provide a path towards that comprehension for every learner.↩︎ In this context, the term “observational data” is used to refer to data collected outside the context of a designed experiment. One might bicker that all data, even those data from designed experiments, must be “observed”, but this is the terminology that we have.↩︎ Indeed, we are probably not too far away from the rise of artificial intelligence-based statistical consulting, where anyone can upload a data set and answer a few questions about it in return for an AI’s analysis.↩︎ Sticklers would be right to note that the “yesterday” and “today” components of the title are better justified than the “tomorrow” part. I claim no special ability to predict the future of statistics, and these notes make no attempt to do so. I’ve added “tomorrow” to the title because the linear model is unlikely to be replaced anytime soon, and because leaving things at “yesterday and today” seems a bit flat.↩︎ "],["simple-linear-regression.html", "Chapter 1 Simple linear regression 1.1 The basics of SLR 1.2 Least-squares estimation 1.3 Inference for the slope 1.4 Sums of squares decomposition and \\(R^2\\) 1.5 Diagnostic plots 1.6 Consequences of violating model assumptions, and possible fixes 1.7 Prediction with regression models 1.8 Regression design 1.9 \\(^\\star\\)Centering the predictor Appendix A: Fitting the SLR model in R Appendix B: Regression models in SAS PROC REG", " Chapter 1 Simple linear regression In statistics, regression models relate the distribution of an output variable to the value(s) of one or several input variables. Characterizing the relationships among input and output variables is central to much of science, and regression methods form the foundation for much of data analysis. We’ll have a lot to say about regression, but we’ll begin with so-called simple linear regression (SLR). SLR models are “simple” in the sense that they contain only one predictor. Because these notes are meant for the intermediate analyst, I’ll assume that you’ve seen SLR before. The purpose of our study here is twofold. First, we’ll use SLR as a familiar venue to review many of the foundational concepts of (frequentist) statistical inference. These ideas are often elusive, so it’s worth reviewing them again to solidify our understanding. Along the way, we’ll encounter some new (!) ideas for interpreting the outcomes of statistical hypothesis test that may improve upon the traditional convoluted definitions. Second, we’ll also introduce some more advanced regression ideas. These ideas will carry over into our study of regression models with several predictors, so it is helpful to study them here in a less complicated setting. For such a fundamental technique, the name “regression” seems a bit odd. Why do we give this most central of tasks such an obscure moniker? The name traces back to Francis Galton’s late 19th century study of the relationship between the heights of individuals and their parents’ heights (Galton (1886)). Galton observed that while the children of taller-than-average or shorter-than-average parents also tend to be taller than average or shorter than average, respectively, the children’s heights tend to be closer to the average height than those of their parents. Galton termed this phenomenon “regression to mediocrity”; today, we don’t like to equate average-ness with mediocrity, so we now refer to the phenomenon as “regression to the mean”. In any case, to characterize this relationship statistically, Galton wrote down equations that were the precursors of the statistical model that we now know as regression. So, the statistical model of regression was first used to describe the empirical phenomenon of regression to the mean, even though statistical regression models are now used in a much broader variety of contexts. All that said, regression to the mean doesn’t just apply to people’s heights; instead, it appears in all sorts of everyday contexts. For example, it helps explains why students who do particularly well on a first exam tend not to do quite so well on a subsequent exam, or why repeating as a league champion is so rare in sports. For more coverage, see e.g. Ch. 17 of Kahneman (2011). 1.1 The basics of SLR Simple linear regression characterizes the relationship between two variables: a predictor variable and a response variable. We will begin with a simple example for context. Example: Individuals in this study consumed a certain number of beers, and their blood alcohol content (BAC) was measured. Data were obtained for \\(n=16\\) individuals.6 Here is a scatter plot of the data: Figure 1.1: BAC vs. beers consumed. To begin, let’s observe that the two variables that a regression model associates are not on equal footing. One variable is designated as the “predictor” and the other variable is designated as the “response”. The predictor variable is denoted by the symbol \\(x\\), and the response variable is denoted by \\(y\\). In plotting, we almost always show the predictor on the horizontal axis and the response on the vertical axis.7 The predictor is also called the “independent” variable because, in a designed experiment, its values are determined by the investigator. The response is also called the “dependent” variable because its distribution depends on the value of the predictor variable, in a way that is determined by nature. For the BAC data, we will identify the number of beers consumed as the predictor and BAC as the response. The regression model associates each value of the predictor variable with a distribution for the response variable. Indeed, the fact that the output of the model is a distribution is what makes this a statistical model, as opposed to some other flavor of mathematical model. A simple linear regression (SLR) is a simple statistical model in which the association between the value of the predictor and the distribution of the response takes a specific form. In particular, in a SLR, the distribution of the response variable is Gaussian (or normal) with a mean that depends linearly on the value of the predictor and a variance that is independent of the value of the predictor. When we plot the fit of a regression model, we typically only plot the regression line. However, the line merely shows how the average of the distribution of the response depends on the predictor. The model has more structure than a plot of the regression line suggests. In terms of an equation, we can write the model using the regression equation \\[\\begin{equation} y_i =\\beta_0 +\\beta_1 x_i +\\varepsilon_i \\tag{1.1}. \\end{equation}\\] In words, we might re-write the equation as \\[ \\mbox{response = intercept + slope} \\times \\mbox{predictor + error}. \\] In the mathematical equation above, the i subscript distinguishes individual data points. For example, \\(y_1\\) is the value of the response associated with the first observation in the data set. Usually, we use the notation \\(n\\) for the total number of data points, and so to be precise we might also write \\(i = 1, \\ldots, n\\). In words, we say that “\\(i\\) varies from 1 to \\(n\\)” or “\\(i\\) ranges from 1 to \\(n\\)”. We’ll suppress the \\(i\\) subscript when we don’t need it. In the SLR model, the equation \\(\\beta_0 + \\beta_1 x\\) shows how the average of the response depends on the predictor value. The parameter \\(\\beta_0\\) is called the intercept, and it gives the value of the regression line when the predictor \\(x = 0\\). As we will see, the value of the regression line at \\(x=0\\) often isn’t a scientifically meaningful quantity, even though we need to know the value to specify the model fully. The parameter \\(\\beta_1\\) is the slope. In SLR, the slope is a parameter tells us by how much regression line rises or falls as the predictor changes. Positive values of the slope indicate that the regression line increases as the predictor increases, and negative values of the slope indicate that the regression line decreases as the predictor increases. The regression line alone is not sufficient to fully specify the entire regression model. To the regression line we add a normally distributed error, denoted by \\(\\varepsilon\\). The error term is a catch-all that subsumes all the other factors that might influence the response that are not included in the predictors. In the context of the BAC example, these might include body weight, metabolism, and/or alcohol content of the beer (if it differed among subjects). Although they look similar, it is important to realize that \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\varepsilon\\) are different beasts. The quantities \\(\\beta_0\\) and \\(\\beta_1\\) are parameters. Recall that in statistics, parameters are quantities that characterize a population. We assume that true values of \\(\\beta_0\\) and \\(\\beta_1\\) exist; those values are just unknown to us. We will estimate these parameters and draw inferences about their values on the basis of data. In contrast, the error term \\(\\varepsilon\\) is a random variable. It does not have one single value, but instead takes a different value for every member of a population. We describe the distribution of the errors across the members of the population using a probability distribution. In simple linear regression, we assume that the random errors have a Gaussian (or normal, or bell-shaped) distribution with mean 0 and variance \\(\\sigma_{\\varepsilon}^{2}\\). We also assume that the random errors are independent among individuals in our sample. A succinct way of stating this is to state that the errors are Gaussian and “independent and identically distributed” (abbreviated “iid”). In notation, we write \\(\\varepsilon_{i} \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}\\left(0, \\sigma_{\\varepsilon }^2 \\right)\\), a statement which we would read as “the errors have a normal (or Gaussian) distribution with mean 0 and variance \\(\\sigma^2_\\varepsilon\\)”. The error variance \\(\\sigma_{\\varepsilon }^2\\) is a parameter, and it measure of the variability in the response that is not explained by the predictor. We will also discuss how to estimate \\(\\sigma_{\\varepsilon }^2\\). (It is also possible to draw statistical inferences for \\(\\sigma_{\\varepsilon }^2\\), although we will not discuss how to do so in these notes.) Before moving on to discussing how to estimate the model parameters, let’s reflect a bit on the slope, \\(\\beta_1\\), because this is the parameter that captures the linear association between the two variables. A particularly nice way to interpret the slope is due to Gelman, Hill, and Vehtari (2020). Their interpretation works like this. Consider two values of the response \\(y_1\\) and \\(y_2\\), associated respectively with two values of the predictor \\(x_1\\) and \\(x_2\\). The regression model says that, on average, the difference \\(y_1 - y_2\\) will equal \\(\\beta_1 \\times (x_1 - x_2)\\). The “on average” part of this interpretation is important because we realize that any two actual observations will also include their respective errors, and so we don’t expect these two observations to differ by exactly \\(\\beta_1 \\times (x_1 - x_2)\\). Second, this interpretation also makes it clear that the regression model predicts that the average difference between two responses will increase or decrease linearly as the difference between their two associated predictor values grows or shrinks. Thus, if the SLR model is appropriate for the BAC data (something we have yet to verify), then the model suggests that the average BAC difference between two individuals who have consumed 1 vs. 2 beers is the same as the average BAC difference between two individuals who have consumed 4 vs. 5 beers, and that both of these differences are one-half as big as the average BAC difference between two individuals who have drank 2.5 vs. 4.5 beers. Our assumption of normally distributed errors has a deeper justification than may meet the eye. If you’ve studied probability, you may have encountered an important result called the Central Limit Theorem. For our purposes, the Central Limit Theorem tells us that if the error results from the combined effect of many small factors added together, then the error’s distribution will be approximately normal. (We will see that regression models are not sensitive to moderate departures from normality, so approximately normal errors are good enough.) This result provides a strong justification for expecting normally distributed errors in many cases. The normality assumption begins to break down when the errors are dominated by only a few factors, or when the factors that contribute to the error combine multiplicitavely. This latter scenario — errors that result from the product of many small influences as opposed to their sum — frequently arises in biology when the response measures some form of population size. Populations grow or shrink multiplicitavely, and so population sizes tend to have right-skewed distributions. It’s also worth noting that we can write the regression model as the sum of the regression line (\\(\\beta_0 + \\beta_1 x\\)) and an error term with mean zero (\\(\\varepsilon\\)) because we have assumed that the errors have a normal distribution. A normal distribution has the special property that we can take a normally distributed quantity, add a constant to it, and the sum will still have a normal distribution. Most statistical distributions do not have this property; for example, a Poisson random variate plus a non-zero constant does not yield a Poisson distributed sum. Some authors find it more natural to write the SLR model as \\(y \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x, \\sigma^2)\\), to emphasize that the response has a Gaussian distribution and that the predictor only affects the mean of this distribution. We will use the style of eq. (1.1), because this style lends itself more readily to mixed-effects models with multiple variance terms. However, the two styles of notation denote the same model. Feel free to use whichever style makes the most sense to you. 1.2 Least-squares estimation The parameters of an SLR model are estimated by the method of least-squares. That is, we find the values of the parameters that minimize the sum of the squared differences between the data points themselves and the line. The estimates are denoted by “hats”, i.e., \\(\\hat{\\beta}_0\\) is the estimate of \\(\\beta_0\\). Other authors use \\(b\\)’s instead of \\(\\hat{\\beta}\\)’s for parameter estimates in regression. Both types of notation commonly appear in the scientific literature. If we were inventing SLR from scratch, we might imagine many possible criteria that we could use to determine the parameter values that provide the best fit of the SLR model. For example, we might contemplate fitting the line that minimized the average absolute difference between the data points and the line. The reason why we favor the least-squares criterion is a direct consequence of the assumption that the errors take a Gaussian distribution.8 In an introductory statistics course, you may have derived formulas for calculating the least-squares estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) by hand. Here, we will rely on software for the necessary computations, although one might note that one could derive the formulas for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) by using basic calculus tools to minimize the error sum-of squares. Using R, we obtain the least-squares fit of the regression model to the BAC data below.9 fm1 &lt;- with(beer, lm(BAC ~ Beers)) summary(fm1) ## ## Call: ## lm(formula = BAC ~ Beers) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.027118 -0.017350 0.001773 0.008623 0.041027 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.012701 0.012638 -1.005 0.332 ## Beers 0.017964 0.002402 7.480 2.97e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02044 on 14 degrees of freedom ## Multiple R-squared: 0.7998, Adjusted R-squared: 0.7855 ## F-statistic: 55.94 on 1 and 14 DF, p-value: 2.969e-06 The least-squares estimates of the intercept and slope for the BAC data are \\(\\hat{\\beta}_0 = -0.013\\) and \\(\\hat{\\beta}_1 = 0.018\\), respectively. Here’s a picture of the scatter-plot with the least-squares line: with(beer, plot(BAC ~ Beers, xlab = &quot;beers consumed&quot;)) abline(fm1) Figure 1.2: SLR fit of BAC vs. beers consumed. The best fitting line shows a positive relationship between BAC and beers consumed. Using the interpretation that we introduced in the previous section, we would say that if we measure the BAC of two people, one of whom has consumed one more beer than the other, on average the BAC of the person who drank more beers will be 0.018 higher than the BAC of the persion who drank fewer beers. Similarly, if we compare the BAC of two people, one of whom drank four more beers than the other, on average the BAC of the person who drank more beers will be \\(0.4 \\times 0.018 = 0.072\\) higher than the person who drank fewer beers, and so on. In a perfect world, we would always include units along with our parameter estimates. In the BAC data, the units of the predictor are perfectly clear (the units are the number of beers), but the units of the response are a bit trickier. The units of BAC are percent by volume, which is often just shortened to percent. So, in the BAC regression model, the units of the intercept are \\(\\hat{\\beta}_0 = -0.013\\%\\), and the units of the slope are \\(\\hat{\\beta}_1 = 0.018\\) percent per beer consumed. Quoting units can get a bit repetitive, so we’ll omit them on occasion, but identifying the units of parameters in your own analysis is a good way to deepen your understanding of what the numbers in the analysis mean. Evaluating the fitted regression line for a given value of the predictor generates a fitted value for each data point. Fitted values are denoted \\(\\hat{y}_i\\). In notation, \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\). (What are the units of fitted values? And why did the error term vanish in the equation for \\(\\hat{y}_i\\)?) The residual for observation \\(i\\), denoted \\(e_i\\), is the difference between the actual observation and the fitted value. In notation, we write \\(e_i = y_i -\\hat{y}_i\\). (What are the units of residuals?) In terms of the data plot, the residuals can be thought of as the vertical differences between the actual data points and the fitted line. In the figure below, the vertical line represents the residual for the individual who consumed 9 beers. Example: The first individual in the data set drank \\(x_1 = 5\\) beers and had a BAC of \\(y_1 = 0.1\\%\\). Find the fitted value and residual for this data point. Answer: \\(\\hat{y}_1 = 0.077\\%\\), \\(e_1 = 0.023\\%\\). The error sum of squares (SSE) is the sum of the squared residuals. Written as a formula, we would write \\[ SSE = \\sum_{i=1}^{n} e_i^{2} = \\sum_{i=1}^{n} \\left(y_{i} - \\hat{y}_i \\right)^{2}. \\] The SSE is a measure of the unexplained variability in the response. The least squares estimates, \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), are called the least squares estimates because they minimize the SSE. We can use the SSE to find an estimate of the error variance parameter by using the formula \\[ s_\\varepsilon^2 = \\dfrac{SSE}{n-2} = MSE \\] We divide by \\(n - 2\\) because there are \\(n - 2\\) degrees of freedom (df) associated with the SSE. When we divide an error sum-of-squares by its degrees of freedom, the resulting quotient is called the “mean-squared error” (MSE). For the BAC data, the SSE is 0.0058, yielding a MSE of \\(0.0058/(16-2) \\approx 0.0004\\). See the gray text at the end of this section for an explanation of why the number of degrees of freedom is \\(n-2\\). Variances are difficult to understand because they are on a squared scale. Thus, the units of the error variance are the units of the response, squared. To place this estimate on a more meaningful scale, we take the square root to obtain the estimate of the residual standard deviation \\(s_{\\varepsilon}\\): \\[ s_{\\varepsilon} =\\sqrt{\\dfrac{SSE}{n-2}} = \\sqrt{MSE} \\] For the BAC data, \\(s_{\\varepsilon} = \\sqrt{0.0004} = 0.020\\). This is a more useful number, as it suggests that a typical deviation between an observed BAC and the corresponding fitted value is 0.020%. (Take a look again at the magnitude of the residuals in the scatterplot of the BAC data, and convince yourself that 0.020% is a reasonable guide to the magnitude of a typical residual.) In the R summary of our model fit, the value of \\(s_{\\varepsilon}\\) is given by the portion of the output labeled “Residual standard error”.10 Degrees of freedom appear frequently in statistical modeling. We will spend quite a bit of effort in these notes keeping track of degrees of freedom, so it’s helpful to understand this concept well. We’ll look carefully at df in the simple case of SLR to build intuition that will carry over into more complicated models. Most error terms, like the SLR error variance \\(\\sigma_\\varepsilon^2\\), are estimated by sums of squares. The concept of degrees of freedom quantifies how many “free differences” are available to compute a sum of squares. Consider the following thought experiment. Suppose that, bizarrely, we knew the values of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in an SLR, and only needed to estimate the error variance \\(\\sigma_\\varepsilon^2\\). We could do so using the sum of squares \\(\\sum_{i=1}^{n}\\left(y_{i} -\\left[\\beta_0 +\\beta_1 x_i \\right]\\right)^2\\). In this case, each of our \\(n\\) data points would contribute a “free difference” to the summation above, and so there would be \\(n\\) free differences with which we could estimate the error variance \\(\\sigma_\\varepsilon^2\\). However, we never know the values of \\(\\beta_0\\) and \\(\\beta_1\\) in advance. Instead, we have to use the data to estimate both \\(\\beta_0\\) and \\(\\beta_1\\). Now, because we have to estimate both \\(\\beta_0\\) and \\(\\beta_1\\), there are only \\(n - 2\\) free differences in the sum of squares \\(\\sum_{i=1}^{n}\\left(y_{i} -\\left[\\hat{\\beta}_0 +\\hat{\\beta}_1 x_i \\right]\\right)^{2}\\). One way to visualize this is to imagine fitting a line to a data set with only \\(n = 2\\) data points (with different \\(x\\) values). The line would be guaranteed to pass through both points, and consequently both residuals would equal 0. Because both residuals equal 0, the SSE would also equal 0. However, the SSE doesn’t equal 0 because the actual value of \\(\\sigma_\\varepsilon^2\\) equals 0. Instead, the SSE equals 0 because there is no information remaining to estimate the residual variance. In general, when we have to use the same data set to estimate the parameters that determine the average value of the response and to estimate the residual variance, then each parameter that we have to estimate in the mean component of the model eliminates a free difference from the sum of squares \\(\\sum_{i=1}^{n}\\left(y_{i} -\\hat{y}_{i} \\right)^{2}\\). To convert the sum of squares into an estimate of an error variance, we need to count the number of free differences (or degrees of freedom) correctly, and divide the sum of squares by the appropriate number of df to make sure we get a good estimate of the variance. 1.3 Inference for the slope To draw statistical inferences about the slope parameter \\(\\beta_1\\), we make the following assumptions: Linearity: The average value of the response is a linear function of the predictor. Equal variance (“homoscedasticity”): The variance of the error terms (the \\(\\varepsilon_i\\)’s) is the same for all observations. Independence: The error terms are independent of one another. Normality. The errors have a normal (i.e., bell-shaped, or Gaussian) distribution. Note that assumption number 1 deals with the mean component of the model, while assumptions 2–4 deal with the error component of the model. 1.3.1 Standard errors As intelligent scientists, we realize that estimates are not exactly equal to the parameters that they seek to estimate. We can characterize the uncertainty in parameter estimates in different ways. One tool that we have for quantifying uncertainty in parameter estimates is to calculate a standard error. In general, a standard error quantifies the variability in an estimate that is attributable to random sampling. Most parameter estimates that we will encounter have known formulas for their standard errors. In most cases, these formulas are complicated, and we will rely on computers to calculate standard errors for us. However, the formula for the standard error of the slope parameter in SLR is interesting to examine because it contains a valuable insight that we can use when collecting data for a regression study. The standard error of \\(\\hat{\\beta}_1\\), denoted \\(s_{\\hat{\\beta}_1}\\), is given by the formula \\[\\begin{equation} s_{\\hat{\\beta}_1} = \\dfrac{s_{\\varepsilon}}{\\sqrt{S_{xx} } } \\tag{1.2} \\end{equation}\\] where \\(S_{xx} =\\sum_{i}\\left(x_{i} -\\bar{x}\\right)^2\\) quantifies the dispersion in the predictor variables. Although this formula looks a bit daunting, there’s some intuition to be gained here, and a lesson for experimental design. Suppose we had designed a regression experiment in which all of the individuals were assigned similar values of the predictor. In this case, \\(S_{xx}\\) would be small, and consequently the standard error \\(s_{\\hat{\\beta}_1}\\) would be large. Conversely, if the values of the predictor were very different among individuals in the study, then \\(S_{xx}\\) would be large and the standard error \\(s_{\\hat{\\beta}_1}\\) would be small. Thus, if we want a precise estimate of the slope, we should choose predictor values that span the range over which we want to learn. Thought question: Following this line of reasoning, is it a good idea to design a study so that half the individuals are assigned a very large value of the predictor, and the other half are assigned a very small value? Why or why not? For the BAC example, \\(s_{\\hat{\\beta}_1} = 0.0024\\). (The units are the same units as the slope, or percent per beer consumed for the BAC data.) This tells us that, over many hypothetical repetitions of this same experiment, a typical difference between our estimate of the slope and its true value is 0.0024. This information sharpens our understanding of the precision of the estimate.11 1.3.2 Confidence intervals A second way in which we can measure the uncertainty in a parameter estimate is to calculate a confidence interval (CI). Recall that the general formula for a confidence interval associated with a statistic is: \\[ \\mathrm{estimate} \\pm \\mathrm{critical\\ value} \\times \\mathrm{standard\\ error} \\] Critical values are found either by consulting a table (and re-living the good old days) or using the internet or a computer program. Critical values depend on the confidence level that you want to associate with the CI. Although it seems a bit backwards, we typically denote the confidence level of a CI as \\(100 \\times \\left(1-\\alpha \\right)\\%\\). Thus, for a 95% confidence interval (a common choice), \\(\\alpha = 0.05\\). Alternatively, we might seek a 99% CI, in which case \\(\\alpha = 0.01\\). To construct a CI for \\(\\beta_1\\) , we find the appropriate critical values from a \\(t\\)-distribution with \\(n - 2\\) df. For a \\(100\\times \\left(1-\\alpha \\right)\\%\\) CI, the critical value is the value that “cuts-off” an upper tail of \\(\\alpha / 2\\) %. For example, to calculate a 99% CI for \\(\\beta_{1}\\), we need to find the critical value of a \\(t\\)-distribution with 14 df that cuts-off an upper 0.5%-tail. Using an online calculator, or another tool, we find that this critical value is 2.977. Thus, a 99% CI is 0.018 \\(\\pm\\) 2.977 \\(\\times\\) 0.0024 = (0.011, 0.025). Recall that the appropriate interpretation of the confidence level a CI is fairly tricky. A proper interpretation is that, if we were to repeat this experiment a large number of times, and calculate a 99% CI for each experiment, in the long run 99% of those CIs would contain the true value of \\(\\beta_1\\). Of course, in real life, we’ll only do the experiment once, and we don’t know if our experiment is one of the 99% in which the CI contains the true parameter value or not. It is often tempting to abbreviate this interpretation by saying that ``there is a 99% chance that \\(\\beta_1\\) is in the CI’’, although technically this interpretation is incorrect (because any single CI either contains the parameter or it doesn’t).12 Note also that there is a trade-off between the confidence level and the width of the interval. If we wanted greater confidence that our interval contained the true parameter value, we could increase the confidence level. However, increasing the confidence level increases the width of the interval, and thus provides less information about the true parameter value in some sense. If we follow this argument to its (il)logical extreme, a 100% CI for \\(\\beta_1\\) covers the entire number line. Now we are fully confident that our interval contains \\(\\beta_1\\), but at the cost of having no information whatsoever about the actual value of \\(\\beta_1\\). 1.3.3 Statistical hypothesis tests Finally, a third way to characterize the statistical uncertainty in \\(\\hat{\\beta}_1\\) is to conduct a statistical hypothesis test. Recall that statistical hypotheses are statements about the values of unknown parameters, and a statistical hypothesis test is a way to measure the strength of evidence against a “null hypothesis”. In the context of SLR, we are almost always interested in testing the null hypothesis that the true value of the slope parameter is equal to zero. In notation, we write this as \\(H_0: \\beta_1 = 0\\). Evidence against this null hypothesis is taken as evidence that the predictor is linearly related to the response. Recall that in statistical hypothesis testing, we must also specify an alternative hypothesis. In SLR, we are almost always interested in testing \\(H_0: \\beta_1 = 0\\) vs. the two-sided alternative \\(H_a: \\beta_1 \\ne 0\\). We conduct a statistical hypothesis test by first calculating a test statistic. In general, formulas for test statistics take the form: \\[ \\mbox{test statistic} = \\dfrac{\\mbox{parameter estimate} - \\mbox{value of parameter under }H_0} {\\mbox{standard error}} \\] Test statistics have the property that if the null hypothesis is true, then the test statistic has a known sampling distribution. In the case of testing \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\) in SLR, if the null hypothesis is true, then the test statistic will have a \\(t\\)-distribution with \\(n-2\\) df. In notation, the test statistic is \\[ t=\\frac{\\hat{\\beta}_{1} -0}{s_{\\hat{\\beta }_{1} } } =\\frac{\\hat{\\beta }_{1} }{s_{\\hat{\\beta }_{1} } } \\] In SLR, this test is so common that the value of the \\(t\\)-statistic is provided automatically by most statistical software packages, including R. For the BAC data, the \\(t\\)-statistic associated with the test of \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\) is \\(t = 7.48\\). Values of the test statistic by themselves are not terribly enlightening. Instead, we use the test statistic to find a \\(p\\)-value. \\(P\\)-values are famously difficult to interpret, and those difficulties in interpretation have impeded their proper use. In 2016, a blue-ribbon panel of experts were convened by the American Statistical Association (the leading professional organization for statisticians in the US) to take the remarkable step of issuing a policy statement regarding the use of \\(p\\)-values. That statement (Wasserstein and Lazar (2016)) defines a \\(p\\)-value as follows: Informally, a \\(p\\)-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value. (Bear in mind that this definition is the work of two dozen of the world’s leading statisticians.) In the context of the test of \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\) in SLR, this means finding the probability that a \\(t\\)-statistic with \\(n-2\\) df is at least as different from zero as the value observed. For a two-sided alternative hypothesis, we say “different from zero’ because the sign (positive vs. negative) of the \\(t\\)-statistic is irrelevant. Be careful, though: for a one-sided alternative hypothesis, the sign of the observed \\(t\\)-statistic is critical! For the BAC data, we find the area under the tail of a \\(t\\)-distribution with 14 df that is greater than 7.48, and then (because the \\(t\\)-distribution is symmetric) multiply by 2. That is, \\[\\begin{align*} p &amp; = \\mathrm{Pr}\\!\\left\\{ t_{14} &lt; -7.48\\right\\} +\\mathrm{Pr}\\!\\left\\{ t_{14} &gt; 7.48\\right\\} \\\\ &amp; = 2 \\times \\mathrm{Pr}\\!\\left\\{ t_{14} &gt;7.48 \\right\\} \\\\ &amp; = 3\\times 10^{-6} \\end{align*}\\] Thus, there is exceedingly strong evidence that BAC is related to the number of beers consumed. My NCSU colleague Ryan Martin suggests that we interpret the \\(p\\)-value as the plausibility of the null hypothesis (Martin (2017)). Thus, small \\(p\\) values correspond to null hypotheses that are not plausible in light of the data, and large \\(p\\) values (those nearer to 1) indicate that null hypothesis is plausible in light of the data. The good news here is that “plausibility” in this context has a rigorous mathematical meaning, and that meaning is more or less exactly what the everyday definition of “plausibility” suggests. The less good news is that understanding this meaning exactly requires the mathematics imprecise probability, which is beyond the scope of this and most statistics courses today. Nevertheless, it strikes me as the best available option for interpreting \\(p\\)-values. Continuing in this vein, we can plot the \\(p\\)-value associated with the test of \\(H_0: \\beta_1 = \\beta_{1,0}\\) vs. \\(H_a: \\beta_1 \\ne \\beta_{1,0}\\) for any parameter value \\(\\beta_{1,0}\\) that we might assume under the null. This plot shows the \\(p\\)-value function, or, following along the lines of the interpretation above, what we might call the plausibility function. Here is a look at the plausibility function for \\(\\beta_1\\) for the BAC data: b1.hat &lt;- 0.017964 b1.se &lt;- 0.002402 p_val &lt;- function(b1) { t.stat &lt;- (b1.hat - b1) / b1.se pt(-abs(t.stat), df = 14, lower.tail = TRUE) + pt(abs(t.stat), df = 14, lower.tail = FALSE) } curve(p_val, from = 0, to = 0.03, xlab = expression(beta[1]), ylab = &quot;plausibility&quot;, yaxt = &quot;n&quot;) axis(2, at = c(0, 0.5, 1), las = 1) abline(v = b1.hat, lty = &quot;dotted&quot;) axis(3, at = b1.hat, lab = expression(hat(beta)[1])) Another nice feature of the \\(p\\)-value function is that we can find a \\(100 \\times (1 - \\alpha)\\%\\) confidence interval (or, more generally, a confidence region) by taking all those parameter values that are individually at least \\(\\alpha\\%\\) plausible. So, for example, a 90% confidence interval consists of all those parameter values that are at least 10% plausible. For the BAC data, we can show this confidence interval as: curve(p_val, from = 0, to = 0.03, xlab = expression(beta[1]), ylab = &quot;plausibility&quot;, yaxt = &quot;n&quot;) axis(2, at = c(0, 0.5, 1), las = 1) axis(2, at = 0.1, col = &quot;red&quot;, las = 1) abline(h = 0.1, col = &quot;red&quot;, lty = &quot;dashed&quot;) (conf.limits &lt;- confint(fm1, level = 0.9)) ## 5 % 95 % ## (Intercept) -0.03495916 0.009557957 ## Beers 0.01373362 0.022193906 abline(v = conf.limits[2, ], col = &quot;red&quot;, lty = &quot;dotted&quot;) This graph nicely illustrates the tight connection between confidence intervals and hypothesis tests. For example, a 90% confidence interval consists of all those parameter values that we would fail to reject at the 10% significance level. While \\(p\\)-value curves have been around a long time, statisticians have never agreed on what to do with them. For that reason, they don’t appear commonly in statistics texts. The values above could be found by consulting a table, or by using statistical software such as R. Because the test of \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\) is sufficiently common in SLR, most computer packages will do this calculation for us. We’ll sweep a lot of acrimonious debate about statistical hypothesis testing under the rug and simply say that some scientists like to make a decision about whether or not to “reject” the null hypothesis. In contemporary practice, most scientists make these “reject” or “do not reject” decisions by comparing the \\(p\\)-value to the test’s significance level, which is usually denoted by \\(\\alpha\\). The significance level (or size) of a test is the frequency with which one would erroneously reject a true null hypothesis; you might also think of it as the allowable Type I (or false-positive) error rate. Consequently, tests with more exacting thresholds for statistical signficance require more evidence against the null to reject it. Most scientists conventionally make reject / do not reject decisions with a significance level of \\(\\alpha = .05\\), but you are free to use whatever significance level you deem appropriate. If \\(p \\le \\alpha\\), we reject the null hypothesis; otherwise, we fail to reject it. (Remember that we never `accept’ the null hypothesis. We only fail to reject it.) Of course, the Type I (false positive) error rate is only one side of the coin. In a perfect world, we want to balance the Type I error rate against the so-called Type II error rate, which is the probability of rejecting a false null hypothesis. More informally, we might think of the Type II error rate as the ``false negative’’ error rate, if the null hypothesis corresponds to a statement of no effect as it usually does. In practice, it is usually easiest to strike this balance by specifying the Type I error rate and letting the Type II error rate fall where it may. This is because specifying the TyPe I error rate is as easy as choosing it. Determining the Type II error rate, on the other hand, entails a more involved calculation that usually depends on quantities that we don’t even know. For more on the Type II error rate, see the subsequent discussion on statistical power. Although it is rare, we can also entertain so-called ‘one-sided’ alternative hypotheses. For example, suppose that we were uninterested in the (somewhat nonsensical) possibility that the numbers of beers consumed decreased BAC, and only were interested in measuring the evidence that the numbers of beers consumed increases BAC. To do so, we might test the same null hypothesis \\(H_0: \\beta_1 \\leq 0\\) vs. the one-sided alternative \\(H_a: \\beta_1 &gt; 0\\). To conduct this test, the test statistic is still \\[ t=\\dfrac{\\hat{\\beta }_{1} -0}{s_{\\hat{\\beta }_{1} } } =\\dfrac{0.0180}{0.0024} =7.48. \\] However, because the alternative hypothesis is one-sided, to calculate a \\(p\\)-value, we interpret “equal to or more extreme than its observed value” as the probability of observing a test statistic greater than 7.48, i.e., \\[ p=\\mathrm{Pr}\\!\\left\\{ t_{14} &gt;7.48\\right\\} =1.5\\times 10^{-6} \\] We would then reject \\(H_0: \\beta_1 \\leq 0\\) in favor of the one-sided alternative \\(H_a: \\beta_1 &gt; 0\\) at the \\(\\alpha = .05\\) significance level. Finally, although it doesn’t make much sense in terms of what we know about alcohol, we could consider testing \\(H_0: \\beta_1 \\geq 0\\) vs. the one-sided alternative \\(H_a: \\beta_1 &lt; 0\\). Again, the test statistic is the same (\\(t\\) = 7.48), but now evidence against the null and in favor of the alternative is provided by negative values of the test statistic, so the p-value is the probability of observing a test statistic less than 7.48, i.e., \\[ p=\\mathrm{Pr}\\!\\left\\{ t_{14} &lt; 7.48\\right\\} = 1 - \\mathrm{Pr}\\!\\left\\{ t_{14} &gt; 7.48\\right\\} \\approx 0.9999. \\] Thus, there is no evidence that would allow us to reject \\(H_0: \\beta_1 \\geq 0\\) in favor of the one-sided alternative \\(H_a: \\beta_1 &lt; 0\\). One final note: Although it is rarely done, there is no reason why we must restrict ourselves to testing \\(H_0: \\beta_1 = 0\\). We could in fact test any null hypothesis. For example, suppose conventional wisdom held that each additional beer consumed increased BAC by 0.02, and we were interested in asking if these data contain evidence that the conventional wisdom is false. Then we could test \\(H_0: \\beta_1 = 0.02\\) vs. \\(H_a: \\beta_1 \\ne 0.02\\), although we have to calculate the test statistic and \\(p\\)-value manually instead of relying on computer output: \\[\\begin{align*} t &amp; = \\dfrac{\\hat{\\beta}_1 -0.02}{s_{\\hat{\\beta}_1 }} \\\\ &amp; = \\dfrac{0.0180-0.02}{0.0024} \\\\ &amp; =-0.83 \\\\ \\\\ p &amp; = \\mathrm{Pr}\\!\\left\\{t_{14} &lt;-0.83\\right\\} +\\mathrm{Pr}\\!\\left\\{t_{14} &gt;0.83\\right\\} \\\\ &amp; = 2 \\times \\mathrm{Pr}\\!\\left\\{ t_{14} &gt;0.83\\right\\}\\\\ &amp; = 0.421. \\end{align*}\\] Thus, \\(H_0: \\beta_1 = 0.02\\) is reasonably plausible in light of these data. Do be mindful of the distinction between a statistical hypothesis and a scientific hypothesis. The following excerpt from an article by B. Dennis and M.L. Taper (Dennis and Taper (1994)) puts it nicely: A statistical hypothesis is an assumption about the form of a probability model, and a statistical hypothesis test is the use of data to make a decision between two probability models. A scientific hypothesis, on the other hand, is an explanatory assertion about some aspect of nature. Thus, while a statistical hypothesis can often embody a scientific hypothesis, a scientific hypothesis does not always boil down to a statistical hypothesis. When the ideas of hypothesis testing were first being developed, there was stark disagreement about what the output of a hypothesis test should be. R.A. Fisher argued that a hypothesis test should quantify how compatible the data are with the null hypothesis, relative to the universe of alternatives contained in the alternative hypothesis. Fisher argued that the appropriate tool for this purpose was the \\(p\\)-value. In contrast, Jerzy Neyman and Egon Pearson argued that the result of a hypothesis test should be a decision about whether or not to reject the null.13 Of course, the two approaches can often be combined by specifying the rejection region (the set of outcomes that would cause the analyst to “reject” the null) in terms of the \\(p\\)-value. While Fisher, Neyman, and Pearson argued vehemently, contemporary practice typically reports both a \\(p\\)-value and a reject / fail-to-reject decision, even if it may be difficult to articulate an entirely coherent rationale for doing so. 1.3.4 Inference for the intercept Most statistical packages automatically provide the standard errors for the intercept, \\(s_{\\hat{\\beta}_0}\\), as well as a test of \\(H_0: \\beta_0 = 0\\) vs. \\(H_a: \\beta_0 \\ne 0\\). Sometimes this is a meaningful test, but usually it isn’t. The scientific context of the problem will determine whether or not it makes sense to pay attention to this test. There is a special type of regression called ``regression through the origin’’ that is appropriate when we can assume \\(\\beta_0 = 0\\) automatically. Should we use regression through the origin for the BAC example? 1.4 Sums of squares decomposition and \\(R^2\\) We have already seen that the SSE measures the unexplained variability in the response. \\[ {\\rm SSE}=\\sum _{i=1}^{n}e_{i}^{2} = \\sum _{i=1}^{n}\\left(y_{i} -\\hat{y}_{i} \\right)^{2} \\] We can also define the total sum of squares, SS(Total): \\[ {\\rm SS(Total)}=\\sum _{i=1}^{n}\\left(y_{i} -\\bar{y}\\right)^{2} \\] SS(Total) is a measure of the total variability in the response. Finally, we can define the regression sum of squares, SS(Regression), as \\[ {\\rm SS(Regression)}=\\sum _{i=1}^{n}\\left(\\hat{y}_{i} -\\bar{y}\\right)^{2} \\] SS(Regression) measures the variability in the response that is explained by the regression. The regression sum of squares is also called the model sum of squares, or SS(Model). By a small miracle (actually, by the Pythagorean Theorem), it happens to be true that: \\[ {\\rm SS(Total)=SS(Regression)+SSE} \\] The coefficient of determination, or \\(R^2\\), is the proportion of the variability in the response explained by the regression model. The formula for \\(R^2\\) is \\[ R^2 = \\dfrac{{\\rm SS(Regression)}}{{\\rm SS(Total)}} = 1-\\frac{{\\rm SSE}}{{\\rm SS(Total)}} . \\] \\(R^2\\) is a nice metric because it quantifies how much of the variability in the response is explained by the predictor. Values of \\(R^2\\) close to 1 indicate that the regression model explains much of the variability in the response, while values of \\(R^2\\) close to 0 suggest the regression model explains little of the variability in the response. We’ll also see that \\(R^2\\) is not limited to SLR and in fact has the same interpretation for more complicated regression models that we will examine later. For the BAC example, \\(R^2\\) = 0.80, suggesting that variation in beers consumed explains roughly 80% of the variation in BAC. Mathematically, \\(R^2\\) can also be computed as square of the (sample) correlation coefficient between the fitted values and the response. In SLR, the fitted values and the predictor are perfectly correlated with one another, so \\(R^2\\) is also the square of the sample correlation coefficient between the predictor and the response. 1.5 Diagnostic plots We have seen that, in order to draw statistical inferences from a simple linear regression, we need to make several assumptions. Although in everyday life assumptions can get a bad rap, assumptions in statistics are necessary and appropriate. The statistician Don Rubin puts it nicely (Rubin (2005)): Nothing is wrong with making assumptions … they are the strands that link statistics to science. It is the scientific quality of the assumptions, not their existence, that is critical. In regression, we can use diagnostic plots to investigate the scientific quality of our assumptions. The main idea of diagnostic plots is that if the assumptions are appropriate, then residuals should be independent draws from a normal distribution with constant variance (what some might more colorfully describe as “white noise”). Any structure in the residuals indicates a violation of at least one assumption. We list commonly used diagnostic plots below. Although some types of plots are more useful for examining some assumptions than others, there isn’t a strict correspondence between plot types and assumptions. Any plot can reveal a departure from any one of our assumptions. Examples of each for the BAC data and the R code used to generate the plots are provided as examples. 1.5.1 Residuals vs. fitted values Check for non-constant variance (trumpeting). The BAC data shown here don’t show an obvious increase or decrease in variance as the fitted values increase, although the fact that the largest residual is associated with the largest fitted value is notable. We might want to go back and check that data point out. plot(resid(fm1) ~ fitted(fm1), xlab = &quot;Fitted Values&quot;, ylab = &quot;Residuals&quot;) abline(h = 0, lty = &quot;dotted&quot;) Perhaps the most common violation of the regression assumption occurs when the variance of a response increases as the fitted values increase. The tell-tale signature of this violation is a “trumpeting” pattern in the plot of the residuals vs. the fitted values. Indeed, an increasing variance is perhaps more the rule than the exception in some sciences, especially the life sciences. To illustrate, here is a data set that we will study more closely when we study ANCOVA. For now, it suffices to say that this is a data set in which the response is the lifespan of a fruitfly, and there are several predictors. Here is a residual plot of the ANCOVA model: Fruitflies that live longer clearly have more variable lifespans. Recall that the Central Limit Theorem gives us a good reason to expect normally distributed residuals when the many small influences that comprise the residual error add together. There’s a related explanation for why increasing variance is so common. When the many small influences that comprise the residual error multiply together instead of adding together, then we tend to observe more variance in the response when the fitted value is larger. Indeed, this is the usual explanation offered for why increasing variance is common in the life sciences, where many processes involve some form of multiplicative growth or decay. This explanation also helps us understand why a log transformation is usually helpful as a remedy for increasing variance, because when we take a log of the response we are converting a multiplicative process into an additive one. 1.5.2 Residuals vs. predictor(s) We can use this plot to check for non-linear trends. If we see a non-linear trend, like a hump-shaped pattern, it might suggest that the true relationship between predictor and response is actually non-linear. For the BAC data, you’ll note that the plot below looks exactly like the plot of residuals vs. fitted values above. This isn’t just coincidence; in fact, residuals vs. fitted values and residuals vs. predictor will always generate exactly the same patterns in SLR. (The reason is because in SLR the fitted value is just a linear function of the predictor.) We want to get in the habit of checking both types of plots, however, because when we start entertaining multiple predictor variables in multiple regression, the plots will no longer be identical. plot(resid(fm1) ~ beer$Beers, xlab = &quot;Beers&quot;, ylab = &quot;Residuals&quot;) abline(h = 0, lty = &quot;dotted&quot;) 1.5.3 Residuals vs. other variables Plot the residuals against variables that are not in the model, e.g., other predictors, observer, order of observation, spatial coordinates. In the BAC data, the only other variable we have (for now at least) is the order in which the observations appear in the data set. Without knowing how the data were collected or recorded, it’s impossible to say whether this variable is meaningful. However, the plot suggests a distinctive downward trend – data points that appear early in the data set are associated with positive residuals, and data points that appear later in the data set are associated with negative residuals. What do you think might have caused this trend? plot(resid(fm1), xlab = &quot;Order&quot;, ylab = &quot;Residuals&quot;) abline(h = 0, lty = &quot;dotted&quot;) 1.5.4 Normal probability plot An obvious way to check the normality assumption is to plot a histogram of the residuals. While this is a straightforward idea, it suffers from the fact that the shape of the histogram depends strongly on how the residuals are grouped into bins. Note how the two histograms below of the BAC residuals provide different impressions about the suitability of the normality assumption. hist(resid(fm1), main = &quot;Bin width = 0.01&quot;, xlab = &quot;Residuals&quot;) hist(resid(fm1), main = &quot;Bin width = 0.02&quot;, xlab = &quot;Residuals&quot;, breaks = 4) An alternative to histograms is a normal probability plot of residuals, also known as a quantile-quantile, or Q-Q, plot. Q-Q plots calculate the empirical quantile of each residual and compare this to the theoretical quantile from a normal distribution. If the normality assumption is appropriate, the empirical and theoretical quantiles will change at the same rate, so they’ll fall on a line when plotted against one another. If the normality assumption is not appropriate, the plot of empirical vs. theoretical quantiles will bend. As we’ll see below, the normality assumption is the critical of the assumptions in regression. Thus, unless the Q-Q plot shows big and dramatic bends, we won’t concern ourselves with small bumps and wiggles. The Q-Q plot for the BAC data below doesn’t seem terribly problematic. qqnorm(resid(fm1)) qqline(resid(fm1)) Here is an example of some Q-Q plots that do show strong departures from normality.14 1.6 Consequences of violating model assumptions, and possible fixes 1.6.1 Linearity When the linearity assumption is violated, the model has little worth. What’s the point of fitting a linear model to data when the relationship between predictor and response is clearly not linear? The best fix is to fit a non-linear model using non-linear regression. (We will discuss non-linear regression later.) A second-best option is to transform the predictor and / or the response to make the relationship linear. 1.6.2 Independence Inference about regression parameters using naive standard errors is not trustworthy when errors are correlated (there is more uncertainty in the estimates than the naive standard errors suggest). The most common sources of non-independence is either temporal or spatial structure in the data, or if the data are grouped in some way that has been accounted for in the analysis. Arguably, we have seen this with the BAC data, where one way to think about the downward trend of residuals vs. the order of observation is that residuals close together in time tend to be positively correlated. The best, and easiest, way to accommodate this type of dependence is to include (an)other predictor(s) in the model for time or space, or to account for a group structure. A second-best solution is to use specific methods for time-series data or spatial data, which doable, but is fairly involved, and will require considerable additional study. 1.6.3 Constant variance Like violations of the independence assumption, violations of the constant-variance assumption cause inference about regression parameters is not trustworthy. Non-constant variance causes there to be more uncertainty in the parameters estimates than the default CIs or \\(t\\)-tests suggest. There are two possible fixes for non-constant variance. If the non-constant variance arises because the response variable has a known, non-normal distribution, then one can use generalized linear models (such as logistic regression for binary data, or Poisson regression for count data). We will touch on generalized linear models briefly at the end of ST 512. Alternatively, if there is no obvious alternative distribution for the response, the usual approach is to transform the response variable to “stabilize” the variance. For better or worse, there used to be a bit of a cottage industry in statistics in developing variance-stabilizing transformations. Remember that transformations come with a cost of diminished interpretability, and be wary of exotic transformations. It is not uncommon to observe data where the variance increases as the mean response increases. Good transformations for this situation are either a log transformation or a square-root transformation.15 Another common non-constant variance problem arises when the response is a percentage or a proportion. In this case, the standard and appropriate transformation is the arcsin-square root transformation, i.e., if the observed response is 10%, the transformed response is \\(\\sin^{-1}(\\sqrt{.1})=0.322\\). 1.6.4 Normality Perhaps surprisingly, the consequences of violating the normality assumption are minimal, unless departures from normality are severe (e.g., binary data).16 When one encounters decidedly non-normal data, the usual remedy is to entertain a so-called generalized linear models, i.e., logistic regression for binary data; Poisson regression for count data. Here’s an example of another data set with residuals that are a bit more problematic. These data give the box office take (in millions of US$) vs. a composite rating score from critics’ reviews: movie &lt;- read.table(&quot;data/movie.txt&quot;, head = T, stringsAsFactors = T) with(movie, plot(BoxOffice ~ Score, xlab = &quot;Average rating&quot;, ylab = &quot;Box office take&quot;)) fm1 &lt;- lm(BoxOffice ~ Score, data = movie) abline(fm1) The plots of residuals vs. fitted value show clear evidence of non-constant variance. The Q-Q plot indicates right-skew. Taking a square-root transformation of the response stabilizes the variance nicely: plot(resid(fm1) ~ fitted(fm1), xlab = &quot;Fitted value&quot;, ylab = &quot;Residual&quot;) abline(h = 0,lty = &quot;dashed&quot;) qqnorm(resid(fm1),main = &quot;QQ plot, movie data&quot;) qqline(resid(fm1)) Let’s try a square-root transformation of the response: fm2 &lt;- lm(sqrt(BoxOffice) ~ Score, data = movie) summary(fm2) ## ## Call: ## lm(formula = sqrt(BoxOffice) ~ Score, data = movie) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.60533 -0.17889 -0.07339 0.17983 0.92065 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.114102 0.106000 1.076 0.284 ## Score 0.010497 0.001834 5.722 6.27e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3109 on 138 degrees of freedom ## Multiple R-squared: 0.1918, Adjusted R-squared: 0.1859 ## F-statistic: 32.74 on 1 and 138 DF, p-value: 6.272e-08 plot(fm2) Another commonly used transformation for right-skewed data is the log transformation. Here are residual plots and model output for log-transformed data: fm3 &lt;- lm(log(BoxOffice) ~ Score, data = movie) summary(fm3) ## ## Call: ## lm(formula = log(BoxOffice) ~ Score, data = movie) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.99268 -0.43135 0.00783 0.67263 1.81413 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.634451 0.323390 -8.146 2.01e-13 *** ## Score 0.029984 0.005596 5.358 3.44e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9484 on 138 degrees of freedom ## Multiple R-squared: 0.1722, Adjusted R-squared: 0.1662 ## F-statistic: 28.71 on 1 and 138 DF, p-value: 3.438e-07 plot(fm3) Which transformation do you think is more appropriate? Do the different transformations lead to different qualitative conclusions regarding the statistical significance of the relationship between reviewer rating and box office take? Here’s a second example using a data set that gives the highway fuel efficiency (in mpg) and vehicle weight of 1999 model cars: cars &lt;- read.table(&quot;data/cars.txt&quot;, head = T) with(cars, plot(mpghw ~ weight, xlab = &quot;Vehicle weight (lbs)&quot;, ylab = &quot;Highway mpg&quot;)) fm1 &lt;- lm(mpghw ~ weight, data = cars) abline(fm1) plot(resid(fm1) ~ fitted(fm1), xlab = &quot;Fitted value&quot;, ylab = &quot;Residual&quot;) abline(h = 0,lty = &quot;dashed&quot;) The relationship between highway mpg and vehicle weight is clearly non-linear, although that is seen most clearly from the plot of residuals vs. fitted values. We will discuss modeling non-linear relationships later. Here are some additional comments: What about outliers? The famous statistician George Box was fond of saying that outliers can be the most informative points in the data set. If you have an outlier, try to figure out why that point is an outlier. Discard outliers only if a good reason exists for doing so – resist the temptation to ``scrub’’ your data. Doing so is tantamount to cheating. If you absolutely must remove an outlier, at least report the model fits both with and without the outliers included. Be particularly wary of data points associated with extreme \\(x\\)-values. These points can be unduly influential. (See discussion in the multiple-regression installment of the notes on leverage, standardized residuals, and Cook’s distance.) What about transforming the \\(x\\)-variable? Remember that there are no assumptions about the distribution of the \\(x\\)-variable. However, transformations of the \\(x\\)-variable can also make non-linear relationships into linear ones. Remember though that transformations tend to lessen interpretability. Don’t extrapolate the regression line beyond the range of the \\(x\\)-variable observed in the data. Remember that statistical models are only valid to the extent that data exist to support them. Although it’s often overlooked, remember that the standard regression model also assumes that the predictor is measured without error. If there’s error in the predictor as well as the response, then the estimated slope will be biased towards 0. If the error in the predictor is comparable to the error in the response, then consider a regression model that allows for variability in the predictor. These models go by multiple names, but they are most often called ``major-axis regression’’. 1.7 Prediction with regression models Regression models are regularly used for prediction. Consider a new value of the predictor \\(x^\\star\\). There are two different types of predictions we could make: What is the average response of the population at \\(x^\\star\\)? What is the value of a single future observation at \\(x^\\star\\)? Point estimates (i.e., single best guesses) are the same for both predictions. They are found by simply plugging \\(x^\\star\\) into the fitted regression equation. Example. Suppose every grad student at NCSU drinks 2.5 beers. What do we predict the average BAC of this population to be? \\[\\begin{align*} \\hat{y}^\\star &amp; = \\hat{\\beta }_{0} +\\hat{\\beta }_{1} x^\\star \\\\ &amp; = -0.013 + 0.018 \\times 2.5\\\\ &amp; = 0.032 \\end{align*}\\] Suppose Danny drinks 2.5 beers. What do we predict Danny’s BAC to be? \\[ \\hat{y}^\\star = 0.032 \\] However, the uncertainty in these two predictions is different. Predictions of single future observations are more uncertain than predictions of population averages (why?). We quantify the uncertainty in prediction 1 with a confidence interval. We quantify the uncertainty in prediction 2 with a prediction interval. A prediction interval (PI) is just like a confidence interval in the sense that you get to choose the coverage level. i.e., a 95% prediction interval will contain a single new prediction 95% of the time, while a 99% prediction interval will contain a single new prediction 99% of the time. All else being equal, a 99% prediction interval will be wider than a 95% prediction interval. Both confidence intervals and prediction intervals follow the same general prescription of \\[ \\mbox{estimate} \\pm \\mbox{critical value} \\times \\mbox{standard error} \\] Both also use the same point estimate, \\(\\hat{y}^\\star\\), and the same critical value (taken from a \\(t\\)-distribution with \\(n-2\\) df). However, the standard errors differ depending on whether we are predicting an average response or a single future observation. If you find formulas helpful, you might derive some insight from the formulas for these two standard errors. For an average population response, the standard error is \\[ s_{\\varepsilon} \\sqrt{\\frac{1}{n} +\\frac{\\left(x^\\star -\\bar{x}\\right)^{2} }{S_{xx} } } \\] while for a single future observation, the standard error is \\[ s_{\\varepsilon} \\sqrt{1+\\frac{1}{n} +\\frac{\\left(x^\\star -\\bar{x}\\right)^{2} }{S_{xx} } } \\] Thus, the width of a CI or PI depends on the following: The type of interval (all else being equal, a PI is wider than a CI; note the extra ‘1’ in the formula for the standard error of a single future observation). The coverage level (all else being equal, higher coverage requires a wider interval). The unexplained variability in the data (all else being equal, larger MSEs yield wider intervals). The distance between \\(x^\\star\\) and the average predictor value, \\(\\bar{x}\\) (all else being equal, predictions are more uncertain further away from \\(\\bar{x}\\)). The function predict can be used to calculate these intervals in R: fm1 &lt;- lm(BAC ~ Beers, data = beer) new.data &lt;- data.frame(Beers = 2.5) predict(fm1, interval = &quot;confidence&quot;, newdata = new.data) ## fit lwr upr ## 1 0.0322088 0.01602159 0.04839601 predict(fm1, interval = &quot;prediction&quot;, newdata = new.data) ## fit lwr upr ## 1 0.0322088 -0.01452557 0.07894317 predict(fm1, interval = &quot;prediction&quot;, newdata = new.data, level = 0.90) ## fit lwr upr ## 1 0.0322088 -0.006169709 0.07058731 Regression (solid line), 95% confidence intervals (dashed lines), and 95% prediction intervals (dotted lines) for the beer data. Note that both confidence and prediction intervals widen near the edges of the range of the predictor. 1.8 Regression design 1.8.1 Choice of predictor values Regression models can be used both for observational and experimental data. In some experiments, the experimenter has control over the values of the predictor included in the experiment. Gotelli, Ellison, et al. (2004) (pp. 167-9) give the following guidelines for a regression design with a single predictor: Ensure that the range of values sampled for the predictor variable is large enough to capture the full range of responses by the response variable. Ensure that the distribution of predictor values is approximately uniform within the sampled range. Once the values of the predictor to be included in the experiment have been chosen, these values should be randomly assigned to the experimental units. Note that randomization does not require randomly choosing the values of the predictor to be included in the experiment! 1.8.2 Power In statistical hypothesis testing, power refers to the probability of rejecting a false null hypothesis. It is equal to one minus the Type II (false negative) error rate. For example, if the type II error rate is 40%, then the power is \\(100\\% - 40\\% = 60\\%\\). All else equal, we favor designs with more power as opposed to less. Power calculations are typically used to provide a rough sense of the appropriate sample size for a regression study. Unfortunately, power depends on several factors, many of which are unknown prior to conducting an experiment. Thus, when calculating power, usually the best that one can do is to make educated guesses about the values of the inputs on which the power depends. For this reason, power calculations are better regarded as rough guides to sample size. For the usual SLR test of \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\), power depends on all of the following: the sample size, \\(n\\). Larger sample sizes give greater power. the size of the true slope \\(\\beta_1\\). Values of \\(\\beta_1\\) further away from zero (that is, larger values of \\(|\\beta_1|\\)) give greater power. the magnitude of the residual variance, \\(\\sigma^2_\\varepsilon\\). Larger values of \\(\\sigma^2_\\varepsilon\\) give less power. the acceptable Type I (false positive) error rate, \\(\\alpha\\). Greater tolerance for Type I errors gives greater power (why?). Typically, a power calculation for SLR involves specifying best guesses for \\(\\beta_1\\) and \\(\\sigma^2_\\varepsilon\\), determining the allowable type I error rate \\(\\alpha\\), and then finding the sample size needed to achieve a minimal acceptable power. The internet contains a variety of power calculators for simple regression. Here’s an example using the pwrss.f.reg function found in the pwrss package in R. The code here uses the fact that in SLR the usual test of \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\) is equivalent to a test of \\(H_0: R^2 = 0\\) vs. \\(H_a: R^2 &gt; 0\\). Conveniently, when we specify the test in terms of \\(R^2\\), we don’t need to specify both \\(\\beta_1\\) and \\(\\sigma^2_\\varepsilon\\); instead, we only need to specify out best guess for \\(R^2\\). The calculation below shows the sample size needed to achieve 80% power in a SLR when \\(R^2=0.3\\) and \\(\\alpha=0.05\\). Note that the function pwrss.f.reg is built for multiple regression models that can include several predictor variables. The argument k in the function call indicates the number of predictor variables; in SLR, \\(k=1\\) because there is a single predictor.17 pwrss::pwrss.f.reg(r2 = 0.3, k = 1, power = 0.8, alpha = 0.05) ## Linear Regression (F test) ## R-squared Deviation from 0 (zero) ## H0: r2 = 0 ## HA: r2 &gt; 0 ## ------------------------------ ## Statistical power = 0.8 ## n = 21 ## ------------------------------ ## Numerator degrees of freedom = 1 ## Denominator degrees of freedom = 18.425 ## Non-centrality parameter = 8.754 ## Type I error rate = 0.05 ## Type II error rate = 0.2 We see that we would need a sample size of \\(n=21\\) to achieve the desired power. The graph below shows how the power is related to sample size when \\(R^2 = 30\\%\\) and when \\(R^2 = 60\\%\\). The plot below shows the power as a function of \\(R^2\\) for several sample sizes. Why do all three curves intersect at the same point when \\(R^2 = 0\\)? What is this value? 1.9 \\(^\\star\\)Centering the predictor While it isn’t essential, it can be useful to redefine the predictor in a regression as the difference between the observed value and the average value of the predictor. For example, in the BAC data, we can define a centered version of the number of beers consumed by \\[ x^{ctr} =x -\\bar{x} \\] Let’s try regressing the response against the centered version of the predictor: beer$beers.c &lt;- beer$Beers - mean(beer$Beers) head(beer) ## Beers BAC beers.c ## 1 5 0.100 0.1875 ## 2 2 0.030 -2.8125 ## 3 9 0.190 4.1875 ## 4 8 0.120 3.1875 ## 5 3 0.040 -1.8125 ## 6 7 0.095 2.1875 beer_slr_ctr &lt;- lm(BAC ~ beers.c, data = beer) summary(beer_slr_ctr) ## ## Call: ## lm(formula = BAC ~ beers.c, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.027118 -0.017350 0.001773 0.008623 0.041027 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.073750 0.005110 14.43 8.47e-10 *** ## beers.c 0.017964 0.002402 7.48 2.97e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02044 on 14 degrees of freedom ## Multiple R-squared: 0.7998, Adjusted R-squared: 0.7855 ## F-statistic: 55.94 on 1 and 14 DF, p-value: 2.969e-06 The main advantage of centering the predictors is that the intercept now has a nice interpretation. Namely, the intercept is now the value of the regression line when \\(x = x^{ctr}\\), which happens to equal the average value of \\(y\\) in the data set. Importantly, we have accomplished this without changing anything about the linear association between the predictor and the response, so our inference for the slope remains unchanged. This is perhaps only a small victory, but it’s a nice touch. Centering the predictor also eases the interpretation of regression parameters in more complicated models with interactions, as we will see later. Appendix A: Fitting the SLR model in R The basic command in R for fitting a regression model is the function lm, short for [l]inear [m]odel. (As the name suggests, the `lm’ function can be used for more than just SLR.) The basic syntax is &gt; lm(response ~ predictor) where “response” and “predictor” would be replaced by the appropriate variable names. The &gt; is the R prompt, and is meant to show what you could type at the command line. Although the above command would work, it would fit the SLR and then forget the model fit. We want to keep the model fit around to analyze it, so we’ll store it in memory under a name of our choosing. Here, we’ll choose the name fm1, although any name would work. Anything proceeded by a pound sign (#) is a comment in R. We’ll assume that the BAC data have already been read into R and reside in memory, and that the variables in the BAC data are named BAC and Beers. Here is code for fitting a SLR model to these data: fm1 &lt;- lm(BAC ~ Beers, data = beer) # The &#39;&lt;-&#39; is the assignment operator. # Here, the output produced by the call to &#39;lm&#39; is stored in memory under # the name &#39;fm1&#39;. We can learn about &#39;fm1&#39; by asking for a summary. summary(fm1) ## ## Call: ## lm(formula = BAC ~ Beers, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.027118 -0.017350 0.001773 0.008623 0.041027 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.012701 0.012638 -1.005 0.332 ## Beers 0.017964 0.002402 7.480 2.97e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02044 on 14 degrees of freedom ## Multiple R-squared: 0.7998, Adjusted R-squared: 0.7855 ## F-statistic: 55.94 on 1 and 14 DF, p-value: 2.969e-06 Let’s examine each portion of the R output above. The portion labeled Call simply tells us what command was used to generate the model. The portion labeled Residuals tells us a five-number summary (minimum, first quartile, median, third quartile, and maximum) of the residuals. The portion labeled Coefficients gives us a table of parameter estimates and standard errors. Each row of the table corresponds to a single parameter. The row labeled (Intercept) obviously corresponds to the intercept. The row labeled with the name of the predictor gives information about the slope parameter. In addition to parameter estimates and standard errors, R (like many computer packages) also automatically generates hypothesis tests of \\(H_0: \\beta_0 = 0\\) vs. \\(H_a: \\beta_0 \\ne 0\\) and \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\). It is up to you, the user, to determine whether or not these tests are informative. Finally, the last block of output provides a variety of additional information. The “residual standard error” (perhaps not the best term) is the estimate of the residual standard deviation, \\(s_{\\varepsilon}\\). R also provides two different \\(R^2\\) values; the \\(R^2\\) that we discussed above is labeled as the “Multiple R-squared”. We will discuss adjusted R-squared later. Finally, the \\(F\\)-statistic corresponds to a `model utility test’, which we will discuss in the context of multiple regression. For now, you might notice that in SLR the p-value of the model-utility test is always equal to the p-value for the test of \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\). We will explain why this is so later. The SS decomposition for a regression model is also referred to as the analysis of variance for the regression model. We can use the `anova’ command in R to obtain the SS decomposition: anova(fm1) ## Analysis of Variance Table ## ## Response: BAC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Beers 1 0.0233753 0.0233753 55.944 2.969e-06 *** ## Residuals 14 0.0058497 0.0004178 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The \\(F\\)-statistic is the model utility test, which we will examine in more detail when we study multiple regression. Appendix B: Regression models in SAS PROC REG There are two main procedures (‘PROCs’ for short) that can be used to fit regression models: PROC REG (for REGression) and PROC GLM (for General Linear Model). As the names suggest, GLM is more versatile, but both can be used for regression. Let’s assume the BAC data have already been loaded into memory in a data set called ‘beer’, and the pertinent variables reside under the variable names ‘bac’ and ‘beers’. Here is sample code for fitting an SLR using PROC REG, and some edited output: proc reg data = beer; model bac = beers; run; The SAS System The REG Procedure Root MSE 0.02044 R-Square 0.7998 Dependent Mean 0.07375 Adj R-Sq 0.7855 Coeff Var 27.71654 Parameter Estimates Parameter Standard Variable DF Estimate Error t Value Pr&gt;|t| Intercept 1 -0.01270 0.01264 -1.00 0.3320 beers 1 0.01796 0.00240 7.48 &lt;.0001 Note that even though the output is arranged differently, the parameter estimates and inference provided are exactly the same, regardless of whether one uses PROC REG, PROC GLM, or R. Bibliography Dennis, Brian, and Mark L Taper. 1994. “Density Dependence in Time Series Observations of Natural Populations: Estimation and Testing.” Ecological Monographs 64 (2): 205–24. Galton, Francis. 1886. “Regression Towards Mediocrity in Hereditary Stature.” The Journal of the Anthropological Institute of Great Britain and Ireland 15: 246–63. Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press. Gotelli, Nicholas J, Aaron M Ellison, et al. 2004. A Primer of Ecological Statistics. Vol. 1. Sinauer Associates Sunderland. Kahneman, Daniel. 2011. Thinking, Fast and Slow. Macmillan. Lehmann, Erich L. 1993. “The Fisher, Neyman-Pearson Theories of Testing Hypotheses: One Theory or Two?” Journal of the American Statistical Association 88 (424): 1242–49. Martin, Ryan. 2017. “A Statistical Inference Course Based on p-Values.” The American Statistician 71 (2): 128–36. Oehlert, Gary W. 2010. A First Course in Design and Analysis of Experiments. Pearson, Egon S. 1955. “Statistical Concepts in the Relation to Reality.” Journal of the Royal Statistical Society. Series B (Methodological), 204–7. Rubin, Donald B. 2005. “Causal Inference Using Potential Outcomes: Design, Modeling, Decisions.” Journal of the American Statistical Association 100 (469): 322–31. Wasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA Statement on p-Values: Context, Process, and Purpose.” The American Statistician 70 (2): 129–33. Regrettably, I’ve lost track of the original source of these data.↩︎ This convention is so common that one often hears the horizontal axis referred to as the \\(x\\)-axis and the vertical axis referred to as the \\(y\\)-axis. If we wanted to be exceedingly careful we should only refer to the axes in this way when the variables that they show are in fact \\(x\\) and \\(y\\), but few outside of mathematics find such care necessary.↩︎ More precisely, the least-squares estimates of the intercept and slope are the maximum likelihood estimates, when we assume that the errors take a Gaussian distribution. Maximum likelihood will be discussed in later versions of these notes.↩︎ A note on terminology: It is conventional to refer to a regression model as a regression of the response on, versus, or against the predictor. Thus, the BAC model could be described as a regression of BAC on the number of beers consumed, or alternatively as a regression of BAC against the number of beers consumed.↩︎ Perhaps this reveals my own ignorance, but I can’t figure out why the R summary of the linear model refers to \\(s_{\\varepsilon}\\) as the “Residual standard error”. It seems to me that \\(s_{\\varepsilon}\\) is the standard deviation of the residuals, and thus it would be better to call it the “Residual standard deviation”.↩︎ If we wanted to be more precise, we should note that the value given by eq. (1.2) is actually an estimate of the standard error. The true standard error of the slope is a parameter denoted by \\(\\sigma_{\\hat{\\beta}_1}\\) and given by the formula \\(\\sigma_{\\hat{\\beta}_1} = \\frac{\\sigma_{\\varepsilon}}{\\sqrt{S_{xx} } }\\), where \\(\\sigma_{\\varepsilon}\\) is the (true) residual standard deviation. Of course, we can never compute \\(\\sigma_{\\hat{\\beta}_1}\\), because we can never know \\(\\sigma_{\\varepsilon}\\). So, we do the sensible thing and substitute our estimate of the residual standard deviation \\(s_{\\varepsilon}\\) for its unknown counterpart \\(\\sigma_{\\varepsilon}\\). The resulting expression in eq. (1.2) gives us an estimate of the standard error of the slope. It would be cumbersome to call \\(s_{\\hat{\\beta}_1}\\) (and every estimated standard error) an “estimated standard error”, so we usually just call it a “standard error”. That said, if we were to repeat the experiment with a different random sample of individuals, we would expect to obtain a different value for \\(s_{\\hat{\\beta}_1}\\), because the value is an estimate. (And, yes, because \\(s_{\\hat{\\beta}_1}\\) is an estimate, it has its own standard error \\(\\sigma_{s_{\\hat{\\beta}_1}}\\) which we could estimate as \\(s_{s_{\\hat{\\beta}_1}}\\), and so on to infinity.))↩︎ Here’s another way to see why it is incorrect to interpret a the coverage level of a confidence interval as a statement about the probability that a random parameter falls in a fixed interval. Suppose we repeated the experiment with a new random sample drawn from the same population and calculated a new 99% confidence interval for \\(\\beta_1\\) based on this second experiment. Under the incorrect interpretation, we could immediately use the mathematics of probability to determine that the intersection (or overlap) of the two 99% confidence intervals must itself be a confidence interval with at least 98% coverage. But this can’t be true. Indeed, there is some chance that the two intervals won’t overlap at all! Thus our original premise of treating the coverage level as a statement about the probability that a random parameter is contained in a fixed interval must be wrong. The only way to interpret the situation correctly is to realize that confidence is a statement about the probability that a random interval contains a fixed parameter value.↩︎ Pearson later (quoting Lehmann (1993)) “admitted that the terms”acceptance” and “rejection” were perhaps unfortunately chosen” (Pearson (1955)). If we could re-invent statistics from scratch, can you think of better alternatives terms?↩︎ This plot is based on Figure 6.1 of Oehlert (2010).↩︎ Note that a log transformation will not work if the data contain response values equal to 0. The usual approach in this case is either to take the \\(\\ln(y + 1)\\), or to take \\(\\ln(y + c)\\), where \\(c\\) is one-half of the smallest non-zero response value in the data set. Note also that the base of the logarithm doesn’t matter when taking a log transformation. Natural log is the most common choice, but one can also use \\(\\log_2\\) or \\(\\log_{10}\\).↩︎ One of my instructors used to refer to normality of the residuals in a linear model as a “self-fulfilling prophecy”.↩︎ Actually, pwrss::pwrss.f.reg computes the power of the model-utility test. Later, we will see that in the context of SLR this test is equivalent to the test of \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\).↩︎ "],["multiple-regression.html", "Chapter 2 Multiple regression 2.1 Multiple regression basics 2.2 \\(F\\)-tests for several regression coefficients 2.3 Categorical predictors 2.4 Interactions between predictors 2.5 (Multi-)Collinearity 2.6 Variable selection: Choosing the best model 2.7 Leverage, influential points, and standardized residuals Appendix: Regression as a linear algebra problem", " Chapter 2 Multiple regression In this chapter, we examine regression models that contain several predictor variables. Thankfully, many of the ideas from the simple linear regression also apply to regression models with several predictors. After an overview of the basics, this chapter will focus on the new aspects of regression modeling that arise when considering several predictors. 2.1 Multiple regression basics Just as SLR was used to characterize the relationship between a single predictor and a response, multiple regression can be used to characterize the relationship between several predictors and a response. Example. In the BAC data, we also know each individual’s weight: beer &lt;- read.csv(&quot;data/beer2.csv&quot;, head = T, stringsAsFactors = T) head(beer) ## BAC weight beers ## 1 0.100 132 5 ## 2 0.030 128 2 ## 3 0.190 110 9 ## 4 0.120 192 8 ## 5 0.040 172 3 ## 6 0.095 250 7 A plot of the residuals from the BAC vs. beers consumed model against weight strongly suggests that some of the variation in BAC is attributable to differences in weight: fm1 &lt;- with(beer, lm(BAC ~ beers)) plot(x = beer$weight, y = resid(fm1), xlab = &quot;weight&quot;, ylab = &quot;residual&quot;) abline(h = 0, lty = &quot;dotted&quot;) To simultaneously characterize the effect that the variables “beers” and “weight” have on BAC, we might want to entertain a model with both predictors. In words, the model is \\[ \\mbox{BAC} = \\mbox{beers} + \\mbox{weight} + \\mbox{error}. \\] As in SLR, the error term is a catch-all term that includes all the variation not accounted for by the linear associations between the response and the predictors “beers” and “weight”. In mathematical notation, we can write the model as \\[\\begin{equation} y = \\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +\\varepsilon \\tag{2.1} \\end{equation}\\] We use subscripts to distinguish different predictors. In this case, \\(x_1\\) is the number of beers consumed and \\(x_2\\) is the individual’s weight. Of course, the order in which we designate the predictors is arbitrary. For the moment, we shall be vague about how we interpret the \\(\\beta\\) parameters that multiply each of the predictors, but we will provide a precise interpretation soon. As in SLR, we place the standard assumptions of independence, constant variance, and normality on the error. We can also think about this model geometrically. Recall that in SLR, we could interpret the SLR model as a line passing through a cloud of data points. With 2 predictors, the model describes a plane that passes through data points that “exist” in a three- dimensional data cloud. In general, the equation for an MLR model with any number of predictors can be written: \\[ y =\\beta_0 +\\beta_1 x_{1} +\\beta_2 x_{2} +\\ldots +\\beta_k x_{k} +\\varepsilon \\] 2.1.1 Ideas that carry over from SLR to multiple regression We can apply much of our understanding from simple regression directly to multiple regression. Here are some of the ideas that carry over directly from SLR to multiple regression. 2.1.1.1 Least-squares estimation As in SLR, we use the least squares criteria to find the best-fitting parameter estimates. That is to say, we will agree that the best estimates of the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\beta_2\\) are the values that minimize \\[\\begin{eqnarray*} SSE &amp; = &amp; \\sum_{i=1}^n e_i^2 \\\\ &amp; = &amp; \\sum_{i=1}^n \\left(y_i -\\hat{y}_i \\right)^2 \\\\ &amp; = &amp; \\sum_{i=1}^n\\left(y_i -\\left[\\hat{\\beta}_0 +\\hat{\\beta}_1 x_{i1} +\\hat{\\beta}_{2} x_{i2} + \\ldots + \\hat{\\beta}_{k} x_{ik} \\right]\\right)^2 \\end{eqnarray*}\\] In the formula for the fitted values, we require a double subscripting of the \\(x\\)’s, with the first subscript is used to distinguish the individual observations and the second subscript is used to distinguish different predictors. For example, \\(x_{i2}\\) is the value of the second predictor for the \\(i\\)th data point. Recall that the least-squares criterion is attached to our assumption of normally distributed errors. Because we are still assuming that the errors are normally distributed, we continue to use the least-squares criterion here. In R, we can find the LSEs for the BAC data by adding a new term to the right-hand side of the model formula in the call to the function ‘lm’: fm2 &lt;- lm(BAC ~ beers + weight, data = beer) summary(fm2) ## ## Call: ## lm(formula = BAC ~ beers + weight, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.0162968 -0.0067796 0.0003985 0.0085287 0.0155621 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.986e-02 1.043e-02 3.821 0.00212 ** ## beers 1.998e-02 1.263e-03 15.817 7.16e-10 *** ## weight -3.628e-04 5.668e-05 -6.401 2.34e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.01041 on 13 degrees of freedom ## Multiple R-squared: 0.9518, Adjusted R-squared: 0.9444 ## F-statistic: 128.3 on 2 and 13 DF, p-value: 2.756e-09 Thus, we see that the LSEs are \\(\\hat{\\beta}_0 = 0.040\\%\\), \\(\\hat{\\beta}_1 = 0.020\\%\\) per beer consumed, and \\(\\hat{\\beta}_{2} = -0.00036\\%\\) per pound of body weight. 2.1.1.2 Fitted values and residuals As in SLR, we can define the fitted value associated with the \\(i\\)th data point as \\[ \\hat{y}_i =\\hat{\\beta}_0 +\\hat{\\beta}_1 x_{i1} +\\hat{\\beta}_{2} x_{i2} + \\ldots + \\hat{\\beta}_{k} x_{ik} \\] and the residual associated with the \\(i\\)th data point as \\[ e_i =y_i -\\hat{y}_i. \\] Example. Find the fitted value and residual for the first observation in the data set, a \\(x_2=132\\) lb person who drank \\(x_1=5\\) beers and had a BAC of \\(y=0.1\\). Answer: \\(\\hat{y}_1 =0.092\\) and \\(e_1 =0.008\\). 2.1.1.3 MSE and the estimate of the residual variance We can define the error sum of squares as \\(SSE=\\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n \\left(y_i -\\hat{y}_i \\right)^2\\). To count the df associated with the SSE, we will use the notation that \\(k\\) is the number of parameters that need to be estimated in the formula for the fitted values, excluding the intercept. (When counting parameters, some texts include the intercept, while others do not. If you consult a text, check to make sure you know what definition is being used.) The SSE will be associated with \\(n - (k + 1)\\) df. Thus, the estimate of \\(\\sigma_{\\varepsilon}^2\\) will be \\[ s_\\varepsilon^2 = MSE = \\frac{SSE}{n-(k+1)}. \\] For the BAC data, there are \\(16-3=13\\) df associated with the SSE, because 3 parameters are needed to determine the mean component of the model. From the R output above, we can see that \\(s_\\varepsilon = 0.010\\%\\). 2.1.1.4 Sums of squares decomposition and \\(R^2\\) The sums-of-squares decomposition also carries over from SLR. In fact, the formulas for \\({\\rm SS(Total)}\\), \\({\\rm SS(Regression)}\\), and \\(SSE\\) are exactly the same as in SLR. It is also still true that \\({\\rm SS(Total) = SS(Regression) + SSE}\\). Thus, we can define \\(R^2\\) using the same formula: \\[ R^2 = \\frac{{\\rm SS(Regression)}}{{\\rm SS(Total)}} = 1- \\frac{{\\rm SSE}}{{\\rm SS(Total)}} \\] We still interpret \\(R^2\\) as a measure of the proportion of variability in the response that is explained by the regression model. In the BAC example, \\(R^2=0.952\\). 2.1.2 Interpreting partial regression coefficients. The \\(\\hat{\\beta}\\)’s in a MLR model are called partial regression coefficients (or partial regression slopes). Their interpretation is subtly different from SLR regression slopes. Misinterpretation of partial regression coefficients is one of the most common sources of statistical confusion in the scientific literature. We can interpret the partial regression coefficients geometrically. In this interpretation, \\(\\beta_j\\) is the slope of the regression plane in the direction of the predictor \\(x_j\\). Imagine taking a “slice” of the regression plane. In the terminology of calculus, \\(\\beta_j\\) is also the partial derivative of the regression plane with respect to the predictor \\(x_j\\) (hence the term “partial regression coefficient”). Another ways to express this same idea in everyday language is that \\(\\beta_j\\) quantifies the linear association between predictor \\(j\\) and the response when the other predictors are held constant, or while controlling for the effects of the other predictors. This is different from an SLR slope, which we can interpret as the slope of the linear association between \\(y\\) and \\(x_j\\) while ignoring all other predictors. Thus, the interpretation of a partial regression coefficient depends on the model in which the parameter is found. Compare the estimated regression coefficients for the number of beers consumed in the SLR model and the MLR model that includes weight. Estimated SLR coefficient for no. of beers consumed: 0.018 Estimated MLR coefficient for no. of beers consumed: 0.020 The coefficients differ because they estimate different parameters that mean different things. The SLR coefficient estimates a slope that does not account for the effect of weight, while the MLR coefficient estimates a slope that does account for the effect of weight. Gelman, Hill, and Vehtari (2020)‘s interpretation of regression coefficients extends nicely here. Recall that we interpreted the SLR slope as saying that if we compare two individuals who consume different numbers of beers (call those values \\(x_1\\) and \\(x_2\\)), then the average difference in the individuals’ BAC equals \\(0.018 \\times (x_1 - x_2)\\). In a multiple regression context, we would now say that if we compare two individuals who weigh the same but who consume different numbers of beers, then the average difference in the individuals’ BAC equals \\(0.020 \\times (x_1 - x_2)\\). Thus, the partial regression coefficient tells us something different than the simple regression coefficient. Therefore, we are not surprised that the values differ. As a final point, note that our interpretation of the partial regression coefficient 0.020 above does not depend on the particular weight of the two individuals that we are comparing; all that matters is that the two individuals compared weigh the same. Thus, if we are comparing two 100-pound individuals, one of whom has comsumed 1 beer and the other who has consumed 3 beers, then we estimate that the person who drank 3 beers would have a BAC 0.040 larger than the person who drank 1 beer. Under the current model, this would also be true if we compared two 250-point individuals, one of whom had comsumed 1 beer and the other who had consumed 3 beers. All that matters, so far, is that the individuals to be compared weigh the same. That said, knowing what we do about human physiology, we might that this value should differ depending on whether we are comparing two 100-pound individuals or two 250-pound individuals. This idea — that the association between one predictor and the response depends on the value of another predictor — is the idea of a statistical interaction. We will encounter interactions soon. Here’s another example from the world of food science. As cheese ages, various chemical processes take place that determine the taste of the final product. These data are from the Moore and McCabe (1989). The response variable is the taste scores averaged from several tasters. There are three predictors that describe the chemical content of the cheese. They are: acetic: the natural log of acetic acid concentration h2s: the natural log of hydrogen sulfide concentration lactic: the concentration of lactic acid Here is a “pairs plot” of the data. In this plot, each panel is a scatterplot showing the relationship between two of the four variables in the model. Pairs plots are useful ways to gain a quick grasp of the structure in the data and how the constituent variables are related to one another. cheese &lt;- read.table(&quot;data/cheese.txt&quot;, head = T, stringsAsFactors = T) pairs(cheese) Let’s entertain a model that uses all three predictors. cheese_regression &lt;- lm(taste ~ Acetic + H2S + Lactic, data = cheese) summary(cheese_regression) ## ## Call: ## lm(formula = taste ~ Acetic + H2S + Lactic, data = cheese) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.390 -6.612 -1.009 4.908 25.449 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -28.8768 19.7354 -1.463 0.15540 ## Acetic 0.3277 4.4598 0.073 0.94198 ## H2S 3.9118 1.2484 3.133 0.00425 ** ## Lactic 19.6705 8.6291 2.280 0.03108 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.13 on 26 degrees of freedom ## Multiple R-squared: 0.6518, Adjusted R-squared: 0.6116 ## F-statistic: 16.22 on 3 and 26 DF, p-value: 3.81e-06 Compare this MLR model with each of the three possible SLR models: slr1 &lt;- lm(taste ~ Acetic, data = cheese) summary(slr1) ## ## Call: ## lm(formula = taste ~ Acetic, data = cheese) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.642 -7.443 2.082 6.597 26.581 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -61.499 24.846 -2.475 0.01964 * ## Acetic 15.648 4.496 3.481 0.00166 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.82 on 28 degrees of freedom ## Multiple R-squared: 0.302, Adjusted R-squared: 0.2771 ## F-statistic: 12.11 on 1 and 28 DF, p-value: 0.001658 slr2 &lt;- lm(taste ~ H2S, data = cheese) summary(slr2) ## ## Call: ## lm(formula = taste ~ H2S, data = cheese) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.426 -7.611 -3.491 6.420 25.687 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.7868 5.9579 -1.643 0.112 ## H2S 5.7761 0.9458 6.107 1.37e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.83 on 28 degrees of freedom ## Multiple R-squared: 0.5712, Adjusted R-squared: 0.5558 ## F-statistic: 37.29 on 1 and 28 DF, p-value: 1.374e-06 slr3 &lt;- lm(taste ~ Lactic, data = cheese) summary(slr3) ## ## Call: ## lm(formula = taste ~ Lactic, data = cheese) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.9439 -8.6839 -0.1095 8.9998 27.4245 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -29.859 10.582 -2.822 0.00869 ** ## Lactic 37.720 7.186 5.249 1.41e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.75 on 28 degrees of freedom ## Multiple R-squared: 0.4959, Adjusted R-squared: 0.4779 ## F-statistic: 27.55 on 1 and 28 DF, p-value: 1.405e-05 What do you make of the fact that an SLR analysis suggests that there is a (statistically significant) positive relationship between acetic acid concentration and taste, yet the partial regression coefficient associated with acetic acid concentration is not statistically significant in the MLR model? The fact that the interpretation of regression parameters depends on the context of the model in which they are found cannot be overemphasized. I speculate that much of the confusion surrounding this point flows from the arguably deficient notation that we use to write regression models. Consider the beer data. When we compare the simple regression model \\(y = \\beta_0 + \\beta_1 x_1 + \\varepsilon\\) and the multiple regression model \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon\\), it seems natural to conclude that the parameter \\(\\beta_1\\) means the same thing in both models. But the parameters differ, as we have seen. In a different world, we might imagine notation that makes this context-dependence explicit, by writing something like \\(\\beta_1(x_1)\\) for the regression coefficient associated with \\(x_1\\) in a model that includes only \\(x_1\\), and \\(\\beta_1(x_1, x_2)\\) for the regression coefficient associated with \\(x_1\\) in a model that includes both \\(x_1\\) and \\(x_2\\). But such notation would quickly become unwieldy. So we are stuck with the notation that we have, and must avoid the confusion that it can create. 2.1.3 Visualizing a multiple regression model How do you visualize a multiple regression model? With two predictors (as in our current BAC model), the regression fit is a (rigid) plane in three-dimensional space. So, we have a fighting chance of making a picture of the model fit if we have good graphics software handy. However, once we enocunter models with more than two predictors, the regression fits become increasingly high-dimensional objects that we humans won’t be able to visualize in full. So the best we can do in general is to think about the bivariate relationship implied between each predictor and the response. We’ll continue to use the BAC model as an example. For starters, we might consider plotting the implied relationship between the predicted response and a predictor when the other predictors are set at their average value. In the BAC model, that means plotting the relationship between the predicted BAC and beers consumed for an individual of average weight, where by average weight we mean equal to the average weight of the individuals in the data set. We can couple this with a plot of predicted BAC vs weight for an individual who has consumed an average number of beers, where again we determine this average based on the average value that appears in the data set. Here are those plots for the BAC model: par(mfrow = c(1, 2)) b.hat &lt;- coefficients(fm2) # extract LSEs of regression coefficients avg.beers &lt;- mean(beer$beers) avg.weight &lt;- mean(beer$weight) plot(BAC ~ beers, type = &quot;n&quot;, xlab = &quot;beers consumed&quot;, data = beer) # set up axes abline(a = b.hat[1] + b.hat[3] * avg.weight, b = b.hat[2]) # predicted BAC for average weight individual legend(&quot;topleft&quot;, leg = paste(&quot;weight = &quot;, round(avg.weight, 0), &quot;lbs&quot;), bty = &quot;n&quot;) plot(BAC ~ weight, type = &quot;n&quot;, data = beer) # set up axes abline(a = b.hat[1] + b.hat[2] * avg.beers, b = b.hat[3]) # predicted BAC for average weight individual legend(&quot;topleft&quot;, leg = paste(&quot;beers consumed = &quot;, round(avg.beers, 1)), bty = &quot;n&quot;) It’s not clear whether we should overlay the data on these plots. Data are helpful, but showing the data points would suggest that the fitted line is a simple regression fit. We want to be clear that it isn’t. 2.1.4 Statistical inference for partial regression coefficients Statistical inference for partial regression coefficients proceeds in the same way as statistical inference for SLR slopes. Standard errors for both partial regression coefficients are provided in the R output: \\(s_{\\hat{\\beta}_1 }\\)= 0.0013, \\(s_{\\hat{\\beta}_{2} }\\)= 0.000057. Under the standard regression assumptions, the quantity \\(t=(\\hat{\\beta}_i -\\beta _i )/s_{\\hat{\\beta}_i }\\) has a \\(t\\)-distribution. The number of degrees of freedom is the number of df associated with the SSE. This fact can be used to construct confidence intervals and hypothesis tests. Most software will do the needed math for us. For example, to find 99% CIs for the partial regression coefficients in the BAC model, we can use confint(fm2, level = 0.99) ## 0.5 % 99.5 % ## (Intercept) 0.0084354279 0.0712912780 ## beers 0.0161715108 0.0237799132 ## weight -0.0005335564 -0.0001920855 Thus a 99% CI for the partial regression coefficient associated with weight is given by the interval from -0.00053 to -0.00019. Most statistical software will automatically provide tests of the null \\(H_0\\): \\(\\beta_i =0\\) vs. the alternative \\(H_a\\): \\(\\beta_i \\ne 0\\). For example, the R output above tells us that the \\(p\\)-value associated with this test for the partial regression coefficient associated with weight is \\(p = 0.000023\\). If we were reporting this analysis in scientific writing, we might say that when comparing people who have consumed the same number of beers, every 1-lb increase in weight is associated with an average BAC decrease of 3.6% \\(\\times\\) 10\\(^{-4}\\) (s.e. 5.7% \\(\\times\\) 10\\(^{-5}\\)). This association is statistically significant (\\(t_{13} =-6.40\\), \\(p &lt; .001\\)). 2.1.5 Prediction As in SLR, we distinguish between predictions of the average response of the population at a new value of the predictors vs. the value of a single future observation. The point estimates of the two predictions are identical, but the value of a single future observation is more uncertain. Therefore, we use a prediction interval for a single observation and a confidence interval for a population average. The width of a PI or CI is affected by the same factors as in SLR. In MLR, the width of the PI or CI depends on the distance between the new observation and the “center of mass” of the predictors in the data set. For example, if we now use the BAC model to predict the BAC of a 170-lb individual who consumes 4 beers: new.data &lt;- data.frame(weight = 170, beers = 4) predict(fm2, newdata = new.data, interval = &quot;prediction&quot;) ## fit lwr upr ## 1 0.05808664 0.03480226 0.08137103 predict(fm2, newdata = new.data, interval = &quot;confidence&quot;) ## fit lwr upr ## 1 0.05808664 0.05205732 0.06411596 2.2 \\(F\\)-tests for several regression coefficients 2.2.1 Basic machinery Consider a general multiple regression model with \\(k\\) predictors: \\[ y=\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +...+\\beta_k x_k +\\varepsilon \\] So far, we’ve seen how to test whether any one individual partial regression coefficient is equal to zero, i.e., \\(H_0 :\\beta_j =0\\) vs. \\(H_a :\\beta_j \\ne 0\\). We will now discuss how to generalize this idea to test if multiple partial regression coefficients are simultaneously equal to zero. The statistical method that we will use is an \\(F\\)-test. Unfortunately, we don’t yet have a great context for motivating \\(F\\)-tests. We will see more compelling motivations for \\(F\\)-tests soon when we consider categorical predictors. For now, we’ll consider our multiple regression model for the BAC data with the two predictors “beers consumed” and “weight”. Recall that the equation for this model is given in eq. (2.1). Let’s suppose that we were interested in testing the null hypothesis that neither of the predictors have a linear association with the response, versus the alternative hypothesis that at least one predictor (and perhaps both) has a linear association with the response. In symbols, we would write the null hypothesis as \\(H_0 :\\beta_1 =\\beta_2 =0\\) and the alternative as \\(H_a :\\beta_1 \\ne 0{\\rm \\; or\\; }\\beta_2 \\ne 0\\). \\(F\\)-tests are essentially comparisons between models. The model that provides the context for the null hypothesis is the “full” model, in the sense that it will prove to be the more flexible of our two models to be compared. In the context of our present example, the full model is given by (2.1). The full model is then compared to a “reduced” model, which is the full model constrained by the null hypothesis. In the present example, when we constrain the full model by the null hypothesis \\(\\beta_1 =\\beta_2 =0\\), we are left with a reduced model that includes only the intercept: \\[\\begin{equation} y = \\beta_0 + \\varepsilon. \\end{equation}\\] Formally, it is important to note that the reduced model is a special case of the full model. The statistical jargon for this observation is that the reduced model is “nested” in the full model, or (to say it in the active voice) the full model “nests” the reduced model. Because the full model nests the reduced model, we know that the full model is guaranteed to explain at least as much of the variation in the response as the reduced model. The operative question is whether the improvement obtained by the full model is statistically significant, that is, whether it is more of an improvement than we would expect by random chance. An \\(F\\)-test proceeds by fitting both the full and reduced model, and for each model recording the SSE and the df associated with the SSE. An \\(F\\)-statistic is then calculated as \\[ F=\\frac{\\left[SSE_{reduced} -SSE_{full} \\right]/\\left[df_{reduced} -df_{full} \\right]}{{SSE_{full} / df_{full} }} \\] where by \\(df_{full}\\) and \\(df_{reduced}\\) we mean the df associated with the SSE of the full and reduced models, respectively. Roughly, the numerator of the \\(F\\)-statistic quantifies the improvement in fit that the full model provides relative to the reduced model. The denominator quantifies the variation in the response left unexplained by the full model. Both numerator and denominator are standardized by their associated df to create an apples-to-apples comparison. The larger the numerator is relative to the denominator, the less likely it is that the improvement in fit was strictly due to random chance. For the BAC data, we could compute the \\(F\\)-statistic “manually” by first fitting both models and extracting the SSE: full.model &lt;- lm(BAC ~ beers + weight, data = beer) (sse.full &lt;- sum(resid(full.model)^2)) ## [1] 0.001408883 reduced.model &lt;- lm(BAC ~ 1, data = beer) (sse.full &lt;- sum(resid(reduced.model)^2)) ## [1] 0.029225 Then our \\(F\\)-statistic evaluates to: \\[ F=\\frac{\\left[0.0292 - 0.0014 \\right]/\\left[15 - 13 \\right]}{{0.0014 / 13 }} = 128.3 \\] If the null hypothesis is true, then the \\(F\\)-statistic will be drawn from an \\(F\\) distribution with numerator df equal to \\(df_{reduced} -df_{full}\\), and denominator df equal to \\(df_{full}\\). Evidence against the null and in favor of the alternative comes from large values of the \\(F\\)-statistic. The \\(p\\)-value associated with the test is the probability of observing an \\(F\\)-statistic at least as large as the one observed if the null hypothesis is true. In R, this \\(p\\)-values can be found with the command pf. pf(128.3, df1 = 2, df2 = 13, lower = FALSE) ## [1] 2.760276e-09 Thus, our \\(p\\)-value is infinitesimal. In light of these data, it is almost completely implausible that there is no linear association between BAC and both the number of beers consumed and the individual’s weight. We’ve taken the above calculations slowly so that we can understand their logic. In R, we can use the anova command to execute the \\(F\\)-test in one fell swoop. anova(reduced.model, full.model) ## Analysis of Variance Table ## ## Model 1: BAC ~ 1 ## Model 2: BAC ~ beers + weight ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 15 0.0292250 ## 2 13 0.0014089 2 0.027816 128.33 2.756e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 2.2.2 Model utility test The test that we have just conducted for the BAC data turns out to be a type of \\(F\\)-test called the model utility test. In general, with the general regression model \\[ y=\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +...+\\beta_k x_k +\\varepsilon \\] the model utility test is a test of the null hypothesis that all the regression coefficients equal zero, that is, \\(H_0 :\\beta_1 =\\beta_2 =...=\\beta_k =0\\) vs. the alternative that at least one of the partial regression coefficients is not equal to zero. In other words, it is a test of whether the regression model provides a significant improvement in fit compared a simpler model that assumes the \\(y\\)’s are a simple random sample from a Gaussian distirbution. The model utility test is easy for computer programmers to automate, so it is usually included as part of the standard regression output. In the R summary of a regression model, we can find it at the very end of the output. full.model &lt;- lm(BAC ~ beers + weight, data = beer) summary(full.model) ## ## Call: ## lm(formula = BAC ~ beers + weight, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.0162968 -0.0067796 0.0003985 0.0085287 0.0155621 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.986e-02 1.043e-02 3.821 0.00212 ** ## beers 1.998e-02 1.263e-03 15.817 7.16e-10 *** ## weight -3.628e-04 5.668e-05 -6.401 2.34e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.01041 on 13 degrees of freedom ## Multiple R-squared: 0.9518, Adjusted R-squared: 0.9444 ## F-statistic: 128.3 on 2 and 13 DF, p-value: 2.756e-09 Although the model utility test has a grandiose name, it is rarely interesting. Rejecting the null in the model utility test is usually not an impressive conclusion (Quinn and Keough (2002)). You may have also noticed that in SLR the model utility test always provided a \\(p\\)-value that was exactly equal to the \\(p\\)-value generated for the test of \\(H_0\\): \\(\\beta_1 =0\\) vs. \\(H_a\\): \\(\\beta_1 \\ne 0\\). Can you figure out why this is so? 2.3 Categorical predictors So far, we have dealt exclusively with quantitative predictors. Although we haven’t given it much thought, a key feature of quantitative predictors is that their values can be ordered, and that the distance between ordered values is meaningful. For example, in the BAC data, a predictor value of \\(x=3\\) beers consumed is greater than \\(x=2\\) beers consumed. Moreover, the difference between \\(x=2\\) and \\(x=3\\) beers consumed is exactly one-half of the distance between \\(x=2\\) and \\(x=4\\) beers consumed. Another way to think about quantitative predictors is that we could sensibly place all of their values on a number line. Categorical variables are variables whose values cannot be sensibly placed on a number line. Examples include ethnicity or brand of manufacture. We use indicator variables as devices to include categorical predictors in regression models.18 Example. D. K. Sackett investigated mercury accumulation in the tissues of large-mouth bass (a species of fish) in several lakes in the Raleigh, NC area (Sackett et al. (2013)). We will examine data from three lakes: Adger, Bennett’s Millpond, and Waterville. From each lake, several fish were sampled, and the mercury (Hg) content of their tissues was measured. Because fish are known to accumulate mercury in their tissues as they age, the age of each fish (in years) was also determined. The plot below shows the mercury content (in mg / kg) for each fish plotted vs. age, with different plotting symbols used for the three lakes. To stabilize the variance, we will use the log of mercury content as the response variable. There are \\(n=23\\) data points in this data set. fish &lt;- read.table(&quot;data/fish-mercury.txt&quot;, head = T, stringsAsFactors = T) with(fish, plot(log(hg) ~ age, xlab = &quot;age (years)&quot;, ylab = &quot;log(tissue Hg)&quot;, type = &quot;n&quot;)) with(subset(fish, site == &quot;Adger&quot;), points(log(hg) ~ age, pch = &quot;A&quot;)) with(subset(fish, site == &quot;Bennett&quot;), points(log(hg) ~ age, pch = &quot;B&quot;)) with(subset(fish, site == &quot;Waterville&quot;), points(log(hg) ~ age, pch = &quot;W&quot;)) With these data, we would like to ask: when comparing fish of the same age, is there evidence that tissue mercury content in fish differs among the three lakes? To do so, we need to develop a set of \\(indicator\\) variables to capture the differences among the lakes. As is often the case, one has options here. If you wish, you can construct the indicator variables manually, perhaps using a spreadsheet program. Alternatively, most computer programs, including R, will create indicator variables automatically. We’ll sketch out the ideas behind indicator variables first and then see how R creates them. To create a set of indicator variables, we first need to choose one level of the variable as the reference level or baseline. While the scientific context of a problem will sometimes make it more natural to designate one level as a reference, the choice is often arbitrary. In all cases, the choice of a reference level will not affect the ensuing analysis. For every level other than the reference, we create a separate indicator variable that is equal to 1 for that level and is equal to 0 for all levels. Thus, to include a categorical variable with \\(c\\) different levels, we need \\(c−1\\) indicator variables. To see how R constructs indicator variables, we can use the contrast command19 contrasts(fish$site) ## Bennett Waterville ## Adger 0 0 ## Bennett 1 0 ## Waterville 0 1 In the R output above, each column represents an indicator variable, the rows give the levels of the categorical variable, and the numbers give the coding of each indicator variable. We see that R has created two indicator variables (as we’d expect), and labeled them “Bennett” and “Waterville”. The names of the indicator variables provide strong hints about their coding, but to be completely sure we can inspect the numerical coding given below. The indicator labeled “Bennett” takes the value of 1 when site equals Bennett and takes the value of zero otherwise. In other words, it is the indicator for Bennett. The indicator labeled “Waterville” takes the value of 1 when site equals Waterville and takes the value of zero for the other two sites. In other words, it is the indicator for Waterville. Adger, therefore, is the reference site.20 Now let’s fit a regression model with both age and site as predictors. fish_model &lt;- lm(log(hg) ~ age + site, data = fish) summary(fish_model) ## ## Call: ## lm(formula = log(hg) ~ age + site, data = fish) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.47117 -0.08896 0.03796 0.13910 0.31327 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.80618 0.15398 -11.730 3.80e-10 *** ## age 0.14309 0.01967 7.276 6.66e-07 *** ## siteBennett 0.07107 0.12910 0.550 0.5884 ## siteWaterville -0.26105 0.10817 -2.413 0.0261 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2084 on 19 degrees of freedom ## Multiple R-squared: 0.7971, Adjusted R-squared: 0.765 ## F-statistic: 24.88 on 3 and 19 DF, p-value: 8.58e-07 The R output gives us information about the partial regression coefficients associated with age and with each of the indicator variables for site. In an equation, we could write this model as \\[ y=\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +\\beta_3 x_3 +\\varepsilon \\] where \\(x_1\\) gives the age of the fish and \\(x_2\\) and \\(x_3\\) are the indicator variables for Bennett and Waterville, respectively. While the R output gives us tests of the significance of the individual regression coefficients, we want to determine whether there are significant differences among the three sites when comparing fish of the same age. To answer this question, we need to test the hypothesis \\(H_0: \\beta_2 = \\beta_3 = 0\\). We can do this with an \\(F\\)-test. fish_model_full &lt;- lm(log(hg) ~ age + site, data = fish) fish_model_reduced &lt;- lm(log(hg) ~ age, data = fish) anova(fish_model_reduced, fish_model_full) ## Analysis of Variance Table ## ## Model 1: log(hg) ~ age ## Model 2: log(hg) ~ age + site ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 21 1.17251 ## 2 19 0.82529 2 0.34722 3.9969 0.03558 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Thus, we see that there are significant differences among the sites, when comparing fish of the same age (\\(F_{2, 19} = 4.00\\), \\(p = 0.036\\)). For what it’s worth, we also could have obtained this test by running the anova command on the full model. anova(fish_model_full) ## Analysis of Variance Table ## ## Response: log(hg) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## age 1 2.89448 2.89448 66.6374 1.24e-07 *** ## site 2 0.34722 0.17361 3.9969 0.03558 * ## Residuals 19 0.82529 0.04344 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The anova command also gives an \\(F\\)-test for the null hypothesis of no association between age and the response when comparing fish from the same lake, that is, that \\(\\beta_1 = 0\\). This \\(F\\)-test gives the same result as the \\(t\\)-test provided in the earlier summary, because the two tests are identical. Arguably, for single regression coefficients, the output from summary is more useful, because it gives us the estimate of \\(\\beta_1\\), while the anova output only gives us an \\(F\\)-test. Finally, now that we have established that there are significant differences among the sites when comparing fish of the same age, we can go back and make sense of the estimates of \\(\\beta_2\\) and \\(\\beta_3\\) in the full model. The partial regression coefficients associated with an indicator variable quantify the difference between the level that the variable is an indicator for and the reference. In other words, for the fish data, the value \\(\\hat{\\beta}_2 = 0.071\\) tells us that if we compare two fish of the same age, then a fish from Bennett will have a response that is on average 0.071 larger than a fish from Adger (the reference site). The value \\(\\hat{\\beta}_3 = -0.261\\) tells us that, on average, a fish from Waterville will have a response that is 0.261 less than a similarly aged-fish from Adger. Note that these values also give us enough information to compute the average difference in \\(y\\) between two similarly aged fish from Bennett and Waterville, even though that value isn’t directly included in the output. Indicator variables may seem like a bit of an awkward device. Why can’t we just fit a model with a separate intercept for each lake? In fact, we can. In R, the program lm includes the intercept \\(\\beta_0\\) in any model by default, because most regression models include it. However, if we instruct lm to omit the baseline intercept \\(\\beta_0\\), then the program will parameterize the model by the lake-specific intercepts. We instruct lm to omit the intercept by including a -1 on the right-hand side of the model formula as follows: fish_model_alt &lt;- lm(log(hg) ~ age + site - 1, data = fish) summary(fish_model_alt) ## ## Call: ## lm(formula = log(hg) ~ age + site - 1, data = fish) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.47117 -0.08896 0.03796 0.13910 0.31327 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## age 0.14309 0.01967 7.276 6.66e-07 *** ## siteAdger -1.80618 0.15398 -11.730 3.80e-10 *** ## siteBennett -1.73511 0.09440 -18.381 1.47e-13 *** ## siteWaterville -2.06723 0.16351 -12.643 1.07e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2084 on 19 degrees of freedom ## Multiple R-squared: 0.9721, Adjusted R-squared: 0.9662 ## F-statistic: 165.2 on 4 and 19 DF, p-value: 1.781e-14 The model is now parameterized by the lake-specific intercepts and the common slope. While this parameterization is more straightforward, note that the \\(R^2\\) value and the model utility test are now wrong. The software’s routine for calculating \\(R^2\\) and the model utility test assumes that the intercept \\(\\beta_0\\) will be present in the model. Of course, it’s easy to use our original parameterization to get the correct \\(R^2\\) value (and model utility test) and use the alternative parameterization to get the lake-specific slopes. We just have to be careful with regard to the rest of the output when the baseline intercept is omitted. None of this behavior is unique to R. In SAS, PROC GLM has a similar option for reparameterizing the model without the common intercept, but it will cause the computation of \\(R^2\\) and the model utility test to break there as well. Models that combine a single numerical predictor and a single categorical predictor can be visualized by plotting the trend lines for each level of the categorical predictor. In making this plot, it is easier to use the alternative parameterization of the model described in the gray text above. with(fish, plot(log(hg) ~ age, xlab = &quot;age (years)&quot;, ylab = &quot;log(tissue Hg)&quot;, type = &quot;n&quot;)) with(subset(fish, site == &quot;Adger&quot;), points(log(hg) ~ age, pch = &quot;A&quot;, col = &quot;forestgreen&quot;)) with(subset(fish, site == &quot;Bennett&quot;), points(log(hg) ~ age, pch = &quot;B&quot;, col = &quot;red&quot;)) with(subset(fish, site == &quot;Waterville&quot;), points(log(hg) ~ age, pch = &quot;W&quot;, col = &quot;blue&quot;)) abline(a = fish_model_alt$coefficients[2], b = fish_model_alt$coefficients[1], col = &quot;forestgreen&quot;) abline(a = fish_model_alt$coefficients[3], b = fish_model_alt$coefficients[1], col = &quot;red&quot;) abline(a = fish_model_alt$coefficients[4], b = fish_model_alt$coefficients[1], col = &quot;blue&quot;) legend(&quot;topleft&quot;, legend = c(&quot;Lake A&quot;, &quot;Lake B&quot;, &quot;Lake W&quot;), pch = 16, col = c(&quot;forestgreen&quot;, &quot;red&quot;, &quot;blue&quot;)) 2.4 Interactions between predictors Consider (again) the BAC data, with our working model \\[ y =\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +\\varepsilon \\] where \\(y\\) is the response (BAC), \\(x_1\\) is beers consumed, \\(x_2\\) is weight, and \\(\\varepsilon\\) is iid normal error. This is an additive model, in the sense that the joint association between beers consumed and weight (as a pair) and BAC can be found by adding together the individual associations between each of the two predictors and the response. However, we might instead want to allow for the possibility that the association between one predictor and the response itself depends on the value of a second predictor. This state of affairs is called a statistical interaction. More specifically, we call it a statistical interaction between the two predictors with respect to their association with the response. Note that whether or not two predictors interact has nothing to do with whether the predictors themselves are associated.21 An interaction between the two predictors allows the effect of beers consumed to depend on weight, and vice versa. A model with an interaction can be written as: \\[ y =\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +\\beta_3 x_1 x_2 +\\varepsilon \\tag{2.2} \\] There are two equally good ways to code this model in R: fm1 &lt;- lm(BAC ~ beers + weight + beers:weight, data = beer) or fm2 &lt;- lm(BAC ~ beers * weight, data = beer) In the first notation, the colon (:) tells R to include the interaction between the predictors that appear on either side of the colon. In the second notation, the asterisk (*) is shorthand for both the individual predictors and their interaction. summary(fm2) ## ## Call: ## lm(formula = BAC ~ beers * weight, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.0169998 -0.0070909 0.0008463 0.0084267 0.0164373 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.010e-02 3.495e-02 0.861 0.40601 ## beers 2.162e-02 5.760e-03 3.754 0.00275 ** ## weight -2.993e-04 2.241e-04 -1.336 0.20646 ## beers:weight -1.066e-05 3.627e-05 -0.294 0.77393 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0108 on 12 degrees of freedom ## Multiple R-squared: 0.9521, Adjusted R-squared: 0.9402 ## F-statistic: 79.57 on 3 and 12 DF, p-value: 3.453e-08 Interpreting the interaction. The partial regression coefficient associated with an interaction between two predictors (call them “A” and “B”) quantifies the effect that predictor A has on the linear association between predictor B and the response. Or, equivalently, the same partial regression coefficient quantifies the effect that predictor B has on the linear association between predictor A and the response. (It may not be obvious right away that the same regression coefficient permits both interpretations, but you might be able to convince yourself that this is true by doing some algebra with the regression model.) Thus, if we reject the null hypothesis that a partial regression coefficient associated with an interaction equals zero, then we conclude that the effects of the two predictors depend on one another. With the BAC data, the interaction term is not statistically significant. Thus, these data provide no evidence that the association between beers consumed and BAC depends on weight, or vice versa. For the sake of argument, let’s examine the estimate of the interaction between beers consumed and weight, despite the fact that it is not statistically significant. How can we interpret the value \\(\\hat{\\beta}_3 = -1.07 \\times 10^{-5}\\)? This value tells us how the association between beers consumed and BAC changes as weight changes. In other words, the model predicts that a heavier person’s BAC will increase less for each additional beer consumed, compared to a lighter person. How much less? If the heavier person weighs 1 lb more than the lighter person, then one additional beer will increase the heavier person’s BAC by \\(1.07 \\times 10^{-5}\\) less than it increases the lighter person’s BAC. This agrees with our expectations, but these data do not provide enough evidence to declare that this interaction is statistically significant. Alternatively, \\(\\beta_3\\) also tells us how the association between weight and BAC changes as the number of beers consumed changes. That is, as the number of beers consumed increases, then the association between weight and BAC becomes more steeply negative. Again, this coincides with our expectation, despite the lack of statistical significance. Interpreting partial regression coefficients in the presence of an interaction. There is a major and unexpected complication that ensues from including an interaction term in a regression model. This complication concerns how we interpret the partial regression coefficients associated with the individual predictors engaged in the interaction, that is, the regression coefficients \\(\\beta_1\\) and \\(\\beta_2\\) in eq. (2.2). It turns out that, in (2.2), the partial regression coefficient associated with an individual predictor quantifies the relationship between that predictor and the response when the other predictor involved in the interaction equals 0. Sometimes this interpretation is scientifically meaningful, but usually it isn’t. For example, in the BAC model that includes the interaction above, the parameter \\(\\beta_1\\) now quantifies the association between beers consumed and BAC for people who weigh 0 lbs. Obviously, this is a meaningless quantity. Alternatively, the parameter \\(\\beta_2\\) quantifies the association between weight and BAC for people who have consumed 0 beers. This is a bit less ridiculous — in fact, it makes a lot of sense — but still requires extrapolating the model fit outside the range of the observed data, because there are no data points here for people who have had 0 beers.22 To summarize, the subtlety here is that if we compare the additive model \\[ y =\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +\\varepsilon \\] with the model that includes an interaction \\[ y =\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +\\beta_3 x_1 x_2 +\\varepsilon, \\] the parameters \\(\\beta_1\\) and \\(\\beta_2\\) have completely different meanings in these two models. In other words, adding an interaction between two predictors changes the meaning of the regression coefficient associated with the individual predictors involved in the interaction. This is a subtlety that routinely confuses even top investigators. It’s a hard point to grasp, but essential for interpreting models that include interactions correctly. We can ease the interpretation of partial regression coefficients in a model that includes an interaction by centering the predictors. Recall that centering the predictors means creating new versions of each predictor by subtracting off their respective averages. For the BAC data, we could create centered versions of the two predictors by: \\[ \\begin{array}{l} {x_1^{ctr} =x_1 -\\bar{x}_1 } \\\\ {x_2^{ctr} =x_2 -\\bar{x}_{2} } \\end{array} \\] We then fit the model with the interaction, using the centered predictors instead: beer$beers.c &lt;- beer$beers - mean(beer$beers) beer$weight.c &lt;- beer$weight - mean(beer$weight) head(beer) ## BAC weight beers beers.c weight.c ## 1 0.100 132 5 0.1875 -39.5625 ## 2 0.030 128 2 -2.8125 -43.5625 ## 3 0.190 110 9 4.1875 -61.5625 ## 4 0.120 192 8 3.1875 20.4375 ## 5 0.040 172 3 -1.8125 0.4375 ## 6 0.095 250 7 2.1875 78.4375 fm3 &lt;- lm(BAC ~ beers.c * weight.c, data = beer) summary(fm3) ## ## Call: ## lm(formula = BAC ~ beers.c * weight.c, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.0169998 -0.0070909 0.0008463 0.0084267 0.0164373 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.402e-02 2.849e-03 25.984 6.45e-12 *** ## beers.c 1.980e-02 1.447e-03 13.685 1.10e-08 *** ## weight.c -3.506e-04 7.206e-05 -4.865 0.000388 *** ## beers.c:weight.c -1.066e-05 3.627e-05 -0.294 0.773927 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0108 on 12 degrees of freedom ## Multiple R-squared: 0.9521, Adjusted R-squared: 0.9402 ## F-statistic: 79.57 on 3 and 12 DF, p-value: 3.453e-08 Note that centering the predictors does not change the estimated interaction or its statistical significance. The main advantage of centering the predictors is that the partial regression coefficients associated with the centered versions of the predictors have a nice interpretation. Now, the partial regression coefficients associated with the main effects quantify the relationship between the predictor and the response when the other predictor involved in the interaction equals its average value. While we’re at it, we can also return to the fish data to ask if fish accumulate mercury in their tissues at different rates in the three lakes. fish_model_interaction &lt;- lm(log(hg) ~ age * site, data = fish) fish_model_additive &lt;- lm(log(hg) ~ age + site, data = fish) anova(fish_model_additive, fish_model_interaction) ## Analysis of Variance Table ## ## Model 1: log(hg) ~ age + site ## Model 2: log(hg) ~ age * site ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 19 0.82529 ## 2 17 0.75322 2 0.07207 0.8133 0.4599 The interaction between age and lake is not significant (\\(F_{2, 17} = 0.81\\), \\(p=0.46\\)). There is no evidence that the rate at which fish accumulate mercury in their tissues differs among the three lakes. Just for fun, we can plot the model with the interaction to see how the plot differs from the additive model that we considered earlier. As we did before, we’ll use the trick of fitting the model without the baseline intercept to make it easier to extract the slopes and intercepts of the lake-specific trend lines. fish_model_alt &lt;- lm(log(hg) ~ site + age:site - 1, data = fish) with(fish, plot(log(hg) ~ age, xlab = &quot;age (years)&quot;, ylab = &quot;log(tissue Hg)&quot;, type = &quot;n&quot;)) with(subset(fish, site == &quot;Adger&quot;), points(log(hg) ~ age, pch = &quot;A&quot;, col = &quot;forestgreen&quot;)) with(subset(fish, site == &quot;Bennett&quot;), points(log(hg) ~ age, pch = &quot;B&quot;, col = &quot;red&quot;)) with(subset(fish, site == &quot;Waterville&quot;), points(log(hg) ~ age, pch = &quot;W&quot;, col = &quot;blue&quot;)) abline(a = fish_model_alt$coefficients[1], b = fish_model_alt$coefficients[4], col = &quot;forestgreen&quot;) abline(a = fish_model_alt$coefficients[2], b = fish_model_alt$coefficients[5], col = &quot;red&quot;) abline(a = fish_model_alt$coefficients[3], b = fish_model_alt$coefficients[6], col = &quot;blue&quot;) legend(&quot;topleft&quot;, legend = c(&quot;Lake A&quot;, &quot;Lake B&quot;, &quot;Lake W&quot;), pch = 16, col = c(&quot;forestgreen&quot;, &quot;red&quot;, &quot;blue&quot;)) A picture may be worth a thousand words, but it isn’t worth a test! The plot of the model suggests that fish in lake W accumulate mercury more slowly than fish in the other two lakes, but the differences among the slopes are no more greater we would have expected from random variation. 2.5 (Multi-)Collinearity Collinearity (or what is sometimes also called multi-collinearity) refers to correlations among predictors, or among weighted sums of predictors. In a designed experiment, collinearity should not be an issue: The experimenter should be able to assign predictors in such a way that predictors are not strongly correlated with one another, or even better, are perfectly uncorrelated.23 With observational data, however, collinearity is often the rule more than the exception. This is especially true when the number of predictors becomes large relative to the number of data points. For example, the problem is especially acute in genomic studies, in which one may seek to find genetic correlates of phenotypic differences with a sample of a few dozen genomes, each of which contains genotypes at several thousand or more loci. In this section, we will explain what collinearity is, how it affects regression modeling, how it can be measured, and what (if anything) can be done about it. To illustrate, we’ll use a data set that details the tar content, nicotine content, weight, and carbon monoxide content of a couple dozen brands of cigarettes. I found these data in McIntyre (1994), who offers the following context: The Federal Trade Commission annually rates varieties of domestic cigarettes according to their tar, nicotine, and carbon monoxide content. The United States Surgeon General considers each of these substances hazardous to a smoker’s health. Past studies have shown that increases in the tar and nicotine content of a cigarette are accompanied by an increase in the carbon monoxide emitted from the cigarette smoke. The dataset presented here contains measurements of weight and tar, nicotine, and carbon monoxide (CO) content for 25 brands of cigarettes. The data were taken from Mendenhall and Sincich (2012). The original source of the data is the Federal Trade Commission.” cig &lt;- read.table(&quot;data/cigarettes.txt&quot;, head = T) head(cig) ## Brand tar nicotine weight co ## 1 Alpine 14.1 0.86 0.9853 13.6 ## 2 Benson&amp;Hedges 16.0 1.06 1.0938 16.6 ## 3 BullDurham 29.8 2.03 1.1650 23.5 ## 4 CamelLights 8.0 0.67 0.9280 10.2 ## 5 Carlton 4.1 0.40 0.9462 5.4 ## 6 Chesterfield 15.0 1.04 0.8885 15.0 (Note that the first variable in the data set is a character string that gives the brand name of each cigarette. In this case, we do not want to treat this as a categorical predictor, so we exclude the stringsAsFactors = T argument from the read.table command.) Here is a pairs plot of the data: pairs(cig[, 2:5]) We wish to use tar content, nicotine content, and weight to build a predictive model of carbon monoxide content.24 Let’s first observe that, when considered on their own, both tar content and nicotine content have strongly significant associations with carbon monoxide content, as the pairs plot suggests. summary(lm(co ~ tar, data = cig)) ## ## Call: ## lm(formula = co ~ tar, data = cig) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.1124 -0.7167 -0.3754 1.0091 2.5450 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.74328 0.67521 4.063 0.000481 *** ## tar 0.80098 0.05032 15.918 6.55e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.397 on 23 degrees of freedom ## Multiple R-squared: 0.9168, Adjusted R-squared: 0.9132 ## F-statistic: 253.4 on 1 and 23 DF, p-value: 6.552e-14 summary(lm(co ~ nicotine, data = cig)) ## ## Call: ## lm(formula = co ~ nicotine, data = cig) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.3273 -1.2228 0.2304 1.2700 3.9357 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.6647 0.9936 1.675 0.107 ## nicotine 12.3954 1.0542 11.759 3.31e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.828 on 23 degrees of freedom ## Multiple R-squared: 0.8574, Adjusted R-squared: 0.8512 ## F-statistic: 138.3 on 1 and 23 DF, p-value: 3.312e-11 Yet in a model that includes all three predictors, only tar content seems to be statistically significant: cig.model &lt;- lm(co ~ tar + nicotine + weight, data = cig) summary(cig.model) ## ## Call: ## lm(formula = co ~ tar + nicotine + weight, data = cig) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.89261 -0.78269 0.00428 0.92891 2.45082 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.2022 3.4618 0.925 0.365464 ## tar 0.9626 0.2422 3.974 0.000692 *** ## nicotine -2.6317 3.9006 -0.675 0.507234 ## weight -0.1305 3.8853 -0.034 0.973527 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.446 on 21 degrees of freedom ## Multiple R-squared: 0.9186, Adjusted R-squared: 0.907 ## F-statistic: 78.98 on 3 and 21 DF, p-value: 1.329e-11 The multiple regression model suggests that if we compare cigarettes with the same nicotine content (and weight), then there will be a (strongly) significant statistical association between tar content and carbon monoxide content. On the other hand, if we compare cigarettes with the same tar content (and weight), then there will not be a significant association between the nicotine content and the carbon monoxide content. This seems like a strong distinction. Do we trust it? The issue here is that tar and nicotine content are strongly correlated with one another, raising legitimate questions about whether we can separate the associations between one of the predictors and the response from the other. Indeed, in light of the strong correlation between tar content and nicotine content, does our comparative interpretation of the regression coefficients even make sense? If two cigarettes have the same nicotine content and weight, then by how much can their tar content differ? This is the issue of collinearity. Perfect collinearity occurs when two predictors are perfectly correlated with one another. Perfect collinearity is rare (unless the number of predictors exceeds the number of data points, in which case it is inevitable). However, if predictors are strongly (but nor perfectly) correlated, trouble still lurks. Indeed, collinearity is not just caused by strong correlations between pairs of predictors: It can also be caused by strong correlations between weighted sums of predictors. For this reason, when there are many predictors relative to the number of data points, collinearity is nearly inevitable. The usual guidance is that collinearity makes the estimated regression coefficients unreliable or unstable, in the sense that small changes in the data set can trigger large changes in the model fit (Bowerman and O’Connell (1990)). This sensitivity to small changes makes it difficult, if not impossible, to have confidence in our inferences about the estimated partial regression coefficients. Collinearity is not a problem for prediction, however. As Quinn and Keough (2002) (p. 127) say: As long as we are not extrapolating beyond the range of our predictor variables and we are making predictions from data with a similar pattern of collinearity as the data to which we fitted our model, collinearity doesn’t necessarily prevent us from estimating a regression model that fits the data well and has good predictive power (Rawlings, Pantula, and Dickey (1998)). It does, however, mean that we are not confident in our estimates of the model parameters. A different sample from the same population of observations, even using the same values of the predictor variables, might produce very different parameter estimates. In other words, we can still use our regression model for prediction, as long as we supply values of tar content and nicotine content that are consistent with the strong correlation between those two variables in the original data set. Collinearity is usually assessed by a variance inflation factor (VIF). The VIF is so named because it measures the amount by which the correlations among the predictors increase the standard error (and thus the variance) of the estimated regression coefficients. A separate VIF can be calculated for each predictor in the model. There is a relatively simple recipe for calculating VIFs that is somewhat edifying, although it’s easier to let the software compute the VIFs for you. Here’s the recipe if you are interested: To calculate the VIF for predictor \\(x_j\\), do the following: Regress \\(x_j\\) against all other predictors. That is, fit a new regression model in which \\(x_j\\) is the response. Note that the actual response \\(y\\) is not included in this model. Calculate \\(R^2\\). The VIF associated with predictor \\(x_j\\) is \\(1/\\left(1-R^2 \\right)\\). The interesting feature of the recipe is that it depends only on the values of the predictors — the response \\(y\\) plays no part. The recipe also has a certain logic, in the sense that if a predictor is strongly collinear with the other predictors in the model, then we should be able to predict that predictor well using the other predictors in the model. The reason why the VIF is calculated as \\(1/\\left(1-R^2 \\right)\\) in the last step is a bit of a mystery, and has to do with the fact that VIFs were originally developed to measure the increase in the variance of the regression coefficients caused by the collinearity. Here, we’ll use the vif function found in the car package to compute the VIFs for the cigarette model: car::vif(cig.model) ## tar nicotine weight ## 21.630706 21.899917 1.333859 Larger values of VIF indicate stronger collinearity. For the cigarette data, the VIFs tell us that both tar and nicotine are strongly collinear with the other predictors in the model, but weight is not strongly collinear with tar and nicotine. So that’s the good news: that collinearity is readily measured. The bad news is two-fold. First, VIFs are a continuous measure. The natural question is how large the VIF needs to be before one needs to worry about it. There’s no bright line to be found here. Most texts suggests that a VIF \\(\\geq\\) 10 indicates strong enough collinearity that additional measures should be taken. As always, don’t take the bright-line aspect of this rule too seriously; a VIF of 9.9 is not meaningfully different from a VIF of 10.1. The second half of the bad news is that there’s no easy fix for collinearity. In some sense, this is just a statement of common sense: If two predictors and a response vary together, then it is difficult (if not impossible) to tease apart the effect of one predictor on the response from the effect of the other predictor with a statistical model. With the cigarette data, for example, if we really wanted to characterize the effects of tar and nicotine content separately, then we’d need to find some cigarettes with high tar content and low nicotine content, or vice versa. The usual recommendations for coping with collinearity attempt to stabilize the estimated partial regression coefficients at the expense of accepting a (hopefully) small bias.25 Here are two: Omit predictors that are highly correlated with other predictors in the model. The rationale here is that highly correlated predictors may just be redundant measures of the same thing. As an example, in the cigarette data, tar content and nicotine content may both be driven by the same underlying features of the cigarette’s ingredients. If this is true, there is little to be gained by including both variables as predictors in a regression model. Use principal components analysis (PCA) to reduce the number of predictors, and use principal components as predictors. PCA is a multivariate statistical method that takes several variables and produces a smaller number of new variables (the “principal components”) that contain the majority of the information in the original variables. The advantage of using a principal component as a predictor is that different principal components are guaranteed to be independent of one another, by virtue of how they are calculated. The major disadvantage of using principal components is that the newly created predictors (the principal components) are amalgams of the original variables, and thus it is more difficult to assign a scientific interpretation to the partial regression coefficients associated with each. So, using PCA to find new predictors yields a more robust statistical model, albeit at the expense of reduced interpretability. 2.6 Variable selection: Choosing the best model So far, we’ve learned how to construct and fit regression models that can potentially include many different types of terms, including multiple predictors, transformations of the predictors, indicator variables for categorical predictors, interactions, and polynomial terms. Even a small set of possible predictors can produce a large array of possible models. How do we go about choosing the “best” model? First, we have to define what we mean by a “best” model. What are the qualities of a good model? Parsimony. We seek a model that adequately captures the “signal” in the data, and no more. There are both philosophical and statistical reasons for seeking a parsimonious model. The statistical motivation is that a model with too many unnecessary predictors is prone to detect spurious patterns that would not appear in repeated samples from the same population. We call this phenomenon “overfitting” or “fitting the noise”. In technical terms, fitting a model with too many unnecessary predictors increases the standard errors of the parameter estimates. Interpretability. We often want to use regression models to understand associations between predictors and the response and to shed light on the data-generating process. This argues for keeping models simple. There are occasions where the sole goal of regression modeling is prediction and in this case interpretability is less important. This is often the case in so-called “big data” applications, where prediction is the primary goal and understanding is only secondary. Statistical inference. As scientists, we are not interested merely in describing patterns in our data set. Instead, we want to use the data to draw inferences about the population from which the sample was drawn. Therefore, we want a model that meets the assumptions of regression so that we can use regression theory to draw statistical inferences. In statistical jargon, the process of choosing which predictors to include in a regression model and which to leave out is called variable selection. More generally, beyond a regression context, the problem of identifying the best statistical model is called model selection. We will look at several automated routines for choosing the best model. Helpful as these routines are, they are no substitute for intelligent analysis. Feel free to use an automated variable selection route to get started, but don’t throw your brain out the window in the process. Also, remember that there is a hierarchy among model terms in regression. Most automated variable selection routines do not incorporate this hierarchy, so we must impose it ourselves. In most cases, the following rules should be followed: Models that include an interaction between predictors should also include the predictors individually. Models that include polynomial powers of a predictor should include all lower-order terms of that predictor. Automated variable selection routines can be grouped into two types: ranking methods and sequential methods. Cross-validation is a type of ranking method that is popular in machine learning. It is important enough that we will study it in more depth. But first we need a word of caution. 2.6.1 Model selection and inference Model selection of any sort destroys the opportunity for statistical inference, because any model that has been identified through model selection is bound to have predictors that are associated with the response. Inference procedures only have their nominal properties (coverage rates for confidence intervals, error rates for hypothesis tests) when the plan for statistical analysis is determined before looking at the data. A thought experiment makes this clear. Consider a hypothetical data set with a single response and a large number (10? 100? 1000?) of candidate predictors that are perfectly uncorrelated with the response in the population from which the data are sampled. In any sample from that population, model selection will identify the predictors that are spuriously correlated with the response in that particular sample. If one then conducts the usual hypothesis tests on those predictors in the best-fitting model, the chances that the predictors will be spuriously statistically significant will be much larger than the nominal error rate of the test. This seems to present a problem for practical analysis. To be sure, there are some particular instances in which analyses are specified before data are observed, namely studies in which either the government or the analyst’s scruples motivate them to preregister the study beforehand. In these cases, the inference procedures have their nominal properties, and accordingly we may place great faith in the results. In all other cases, though, most statistical analysis follows some sort of model selection, whether that selection is conducted formally using the methods such as those described below or informally via a casual glance at the data. Indeed, this is likely the norm in academic science. When inference follows model selection, any subsequent statistical inferences should be regarded as descriptive with unknown statistical properties (Berry (2016)). 2.6.2 Ranking methods Ranking methods work best when it is computationally feasible to fit every possible candidate model. In these situations, we calculate a metric that quantifies the model’s adequacy, and select the model that scores the best with respect to that metric. There are several possible metrics to choose from, and they don’t always point to the same best model. We will look at three different metric that enjoy wide use. Throughout this section, we will refer to the number of predictors in a model, and denote this quantity by \\(k\\). To remove ambiguity, what we mean in this case is the number of partial regression coefficients to be estimated. So, for example, we would say that the model \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1^2 + \\beta_4 x_1 x_2 + \\varepsilon\\) has \\(k = 4\\) predictors. Before beginning, we should note that \\(R^2\\) is not a good choice for a ranking metric. This is because adding a predictor will never decrease \\(R^2\\). Therefore, \\(R^2\\) can only be used to compare models that have the same number of predictors. 2.6.2.1 Adjusted \\(R^2\\). Adjusted \\(R^2\\) is a penalized version of \\(R^2\\) that imposes a penalty for each additional parameter added to the model. Recall that the formula for (unadjusted) \\(R^2\\) can be written as \\[ R^2 = 1 - \\dfrac{SSE}{SSTotal}. \\] Adjusted \\(R^2\\) “adjusts” both sums-of-squares based on their associated df. That is, the formula for adjusted \\(R^2\\) is \\[ {\\rm Adj-}R^2 =1-\\dfrac{SSE / (n - (k + 1))}{SSTotal / (n - 1)}. \\] Although it isn’t easy to see from the formula, the effect of the adjustment is to impose a penalty on the \\(R^2\\) term that increases as the number of predictors (\\(k\\)) increases. The model with the largest Adj-\\(R^2\\) is considered best. It is easy to see that we can rewrite the formula for adjusted \\(R^2\\) as \\[ {\\rm Adj-}R^2 =1-\\dfrac{s^2_\\varepsilon}{SSTotal / (n - 1)}. \\] Thus, the model with the largest adjusted \\(R^2\\) will be exactly the model with the smallest residual variance (or mean-squared error) \\(s^2_\\varepsilon\\). 2.6.2.2 AIC (Akaike’s Information Criterion) AIC is also a penalized goodness-of-fit measure, like adjusted \\(R^2\\). AIC enjoys a bit more theoretical support than adjusted \\(R^2\\) and is more versatile, although its derivation is a bit more opaque. (As the name suggests, AIC has its roots in information theory.) The general form of AIC involves math that is beyond the scope of ST 512, but we can write down the specific formula for regression models, which is \\[ AIC=n\\ln \\left[\\frac{SSE}{n} \\right]+2\\left(k+1\\right) \\] The smallest value of AIC is best. (Smaller here means algebraically smaller — that is, further to the left on the number line — not closer to zero.) Despite its theoretical support, AIC tends to favor models with too many predictors. Here’s a bit more about the idea behind AIC. To understand AIC, we first have to understand the notion of Kullback-Leibler (KL) divergence from the field of information theory. Suppose we have a data set in hand. In a very abstract way, we can think about the collection of all possible processes that could have generated those data. Somewhere in that collection is the true data-generating process. While we have no hope of ever finding the true data-generating process, we can propose models for that process, and those models are also in the abstract collection of all possible processes that could have generated our data. The KL divergence is a measure of how much each of the models that we might propose diverges from the true data-generating process. Smaller divergence values mean that the model is closer to the true process. If we could measure the KL divergence from the truth to each of our candidate models, then we would favor the model with the smallest divergence. But we can’t measure the KL divergence, because we can never find the true data-generating process. Akaike’s genius was to show that we could develop an asymptotically unbiased estimate of the KL divergence plus an unknown constant that depends only on the data set in hand. (“Unbiased” in this sense means that the estimate is neither systematically too big or systematically too small. “Asymptotically unbiased” means that the bias of the estimate only vanishes as the data set becomes large. We’ll say more about this below.) Akaike named this value AIC, for “an information criterion”, although today the “A” is often understood to stand for Akaike, in his honor. Because AIC only estimates the KL divergence plus some unknown constant, any one value of AIC is meaningless, because we never know the value of that constant that is baked in. But if our goal is to compare several models and determine which one has the smallest KL divergence, then we can compare AIC values for models fit to the same data set, because the unknown constant — whatever it is — will be the same for all the models that we compare. Thus, if we compute the AIC values for several models fit to the same data, we should favor the model with the smallest AIC. There are a number of important caveats to using AIC, all of which are foreshadowed by the explanation above. First, AIC values cannot be compared between models fit to different data sets. Those comparisons have no meaning. Second, AIC is only an estimate of the KL divergence. Like any estimate, it is contaminated with error, yet (to the best of my knowledge) no one has ever come up with a good method for quantifying that error. (In other words, there’s a standard error associated with AIC, but no one knows how to compute it.) Third, AIC is only “asymptotically unbiased”. Asymptotic unbiasedness is a quality that we hope all good statistical estimators possess, because it assures us that our estimates will get better as we collect more data. But it is a weak desideratum nevertheless, because it only guarantees that AIC will accurately estimate the KL divergence for very large data sets. With small data sets, all bets are basically off, but this doesn’t usually faze anyone from using AIC for small data sets.26 There is a small-sample correction available for AIC, known as AICc (the second “c” stands for corrected), but the properties of AICc are only known for normally distributed data. How small is small in this case? That’s a good question, and it lacks a satisfying answer. 2.6.2.3 Cross-validation Cross validation is a type of ranking method that is popular in machine learning. The appeal of cross validation is that it doesn’t rely on any abstract statistical ideas; instead, the ideas flow directly from common sense. The difficulty lies in the fact that implementing a cross-validation procedure requires a small bit of programming chops. As of this writing, there seem to be several R packages that implement cross-validation, including one called cv. Cross-validation is most compelling when we wish to use our regression model primarily for prediction. The basic idea of cross-validation is that a good regression model (especially a good predictive mode) should make accurate predictions. At first, this may not seem like a useful observation: it’s usually hard to collect new data that we can use to evaluate how well our candidate models make predictions. The genius of cross-validation is that if we have enough data to begin with, we don’t have to collect new data to evaluate predictive performance. Instead, we can mimic the collection of new data by splitting our data into two subsets, fitting the model to the first subset, and then using that fit to predict the responses in the second subset. Hence the name “cross-validation”: we are validating our model by seeing how well it predicts data that we already have in hand. The important point to note here is that we do not use the same data both for fitting and for prediction. Doing so would be circular and would exaggerate the predictive ability of the model. Of course a model should make good predictions for the data that were used to fit the model in the first place! When we split the data, the subset to which the data are fit is called the training subset, and the subset which are held out for prediction is called the testing subset. Usually the training subset is larger than the testing subset. Of course, if we can split the data once, then we can split it many times, and we obtain a better idea of how well our model makes predictions by averaging its predictive performance over many splits. To quantify predictive performance at each split, we can use any sensible measure of predictive accuracy such as the mean-squared error of the predictions (or more commonly the square root of the mean squared error) or mean absolute error of the predictions. The averaged predictive performance over many splits provides a measure of prediction accuracy that we can use to rank candidate models. There are many variations on this theme. \\(K\\)-fold cross-validation splits the data into \\(k\\) different subsets (or folds) of roughly equal size, and then uses each fold once as the testing subset, for a total of \\(k\\) different splits. Leave-one-out cross validation is \\(k\\)-fold cross validation with \\(k=n\\) folds. In other words, each data point constitutes its own fold. Leave-one-out cross validation combined with the use the sum of squared prediction errors as the measure of predictive performance produces a special statistic that SAS dubbed the PRESS statistic (an acronym for [pre]dicted [s]um of [s]quares); it isn’t clear to me whether this term is used much nowadays. It’s also worth noting that \\(k\\)-fold cross-validation doesn’t actually require going to the trouble of splitting the data and re-fitting the model \\(k\\) times; some clever math can be used to compute the mean-squared error for the \\(k\\)-fold cross-validation without manually performing all the splits and refits, which speeds up the computation time considerably. For more on cross-validation using R, see the vignette here. 2.6.3 Sequential methods Sequential methods work best for problems where the set of candidate models is so vast that fitting all the candidate models is not feasible. Because computers are faster today than they were years ago, it is now feasible to fit a large number of candidate models quickly, and thus sequential methods are less necessary today than they were years ago. Nevertheless, the ideas are straightforward. There are three different types of sequential methods, based on the direction in which the model is built. In forward selection, we begin with the simplest possible model (namely, \\(y = \\beta_0 + \\varepsilon\\)), and grow the model by adding predictors to it. In backwards elimination, we start with the most expansive possible model and shrink it by removing unnecessary predictors. In stepwise selection, we start with the simplest possible model (namely, \\(y = \\beta_0 + \\varepsilon\\)) and then merge forwards and backwards steps, either growing and shrinking the model until converging on one that cannot be improved. Stepwise selection is used more often than the other two variations. Each of the three procedures could possibly lead to a different best model. Here is the algorithm for forward selection: Initiation step: Start with the model \\(y=\\beta_0 +\\varepsilon\\). This is the initial “working” model. Iteration step: Fit a set of candidate models, each of which differs from the working model by the inclusion of a single additional model term. Be aware that the set of candidate models must abide our rules of thumb about hierarchy. (That is, we wouldn’t consider a model like \\(y = \\beta_0 + \\beta_1 x_1 x_2 + \\varepsilon\\).) Ask: Do any of the candidate models improve upon the working model? If the answer to this question is “yes”, then choose the model that improves upon the working model the most. Making this model the working model, and begin a new iteration (return to step 2). (Termination step): If the answer to this question is “no”, then stop. The current working model is the final model. The algorithm for backwards elimination is similar: Initiation step: Start with the most expansive possible model. Usually, this will be the model with all possible predictors, and potentially with all possible first- (and conceivably second-) order interactions. Note that we can consider a quadratic term to be an interaction of a predictor with itself in this context. This is the initial “working” model. Iteration step: Fit a set of candidate models, each of which differs from the working model by the elimination of a single model term. Again, be aware that the set of candidate models must abide our rules of thumb about hierarchy, so (for example) we wouldn’t consider a model that removes \\(x_1\\) if the interaction \\(x_1 x_2\\) is still in the model. (Same as forward selection.) Ask: Do any of the candidate models improve upon the working model? If the answer to this question is “yes”, then choose the model that improves upon the working model the most. Making this model the working model, and begin a new iteration (return to step 2). (Termination step): If the answer to this question is “no”, then stop. The current working model is the final model. The algorithm for stepwise selection initiates the algorithm with \\(y=\\beta_0 +\\varepsilon\\), and then forms a set of candidate models that differ from the working model by either the addition or elimination of a single model term. The algorithm proceeds until the working model cannot be improved either by a single addition or elimination. So far, we have been silent about how we determine whether a candidate model improves on the working model, and if so, how to find the candidate model that offers the most improvement. We can use any of our ranking methods at this step. Historically, \\(p\\)-values have often been used to determine whether a candidate model improves on the working model, though this practice has largely been discontinued. The argument against it is that using \\(p\\)-values for variable selection destroys their interpretation in the context of hypothesis testing. This being said, any statistical tests can only be regarded as descriptive if the tests occur in the context of a model that has been identified by model selection. Statistical tests only have their advertised properties if decisions about which predictors to include are made before looking at the data. The `step’ routine in R uses AIC as its default criterion for adding or dropping terms in stepwise selection. Here is an example of stepwise selection with the cheese data, considering only models without interactions or polynomial terms. fm0 &lt;- lm(taste ~ 1, data = cheese) # the initial model y = b0 + eps step(fm0, taste ~ Acetic + H2S + Lactic, data = cheese) ## Start: AIC=168.29 ## taste ~ 1 ## ## Df Sum of Sq RSS AIC ## + H2S 1 4376.7 3286.1 144.89 ## + Lactic 1 3800.4 3862.5 149.74 ## + Acetic 1 2314.1 5348.7 159.50 ## &lt;none&gt; 7662.9 168.29 ## ## Step: AIC=144.89 ## taste ~ H2S ## ## Df Sum of Sq RSS AIC ## + Lactic 1 617.2 2669.0 140.65 ## &lt;none&gt; 3286.1 144.89 ## + Acetic 1 84.4 3201.7 146.11 ## - H2S 1 4376.7 7662.9 168.29 ## ## Step: AIC=140.65 ## taste ~ H2S + Lactic ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 2669.0 140.65 ## + Acetic 1 0.55 2668.4 142.64 ## - Lactic 1 617.18 3286.1 144.89 ## - H2S 1 1193.52 3862.5 149.74 ## ## Call: ## lm(formula = taste ~ H2S + Lactic, data = cheese) ## ## Coefficients: ## (Intercept) H2S Lactic ## -27.592 3.946 19.887 There is no prescription for building models automatically. (If there were, someone would have written a computer package implementing the procedure and would be filthy rich.) Here is one cycle of steps for building a regression model, courtesy of Dr. Roger Woodard, formerly of NCSU: Examine univariate (ST 511) summaries of the data (summary statistics, boxplots, etc.). Identify unusual values or possible problems. (Don’t take it on faith that your data are all correct!) Examine scatterplots with all variables. Find variables that are closely correlated with the response and with each other. Candidate model selection: Identify a model that includes relevant variables. Use automated selection procedures if you wish. Assumption checking: Check (standardized) residuals. Determine if polynomial terms or transformations may be needed. Examine collinearity diagnostics. Inspect VIFs and pairwise correlations between variables. Decide if some variables may be removed or added. Revision. Add or remove terms based on steps 4-5. Return to step 3. Prediction and testing: Consider validating the model with a sample that was not included in building the model. 2.7 Leverage, influential points, and standardized residuals 2.7.1 Leverage Recall that in SLR, a data point can have undue influence on the regression model if the value of the predictor, \\(x\\), is for away from \\(\\bar{x}\\). The same notion applies in MLR: data points can be unduly influential if their combination of predictors lies far away from the “center of mass” of the other predictors. However, with multiple predictors, it is harder to detect influential points visually. The leverage of a data point is a measure of its distance from the center of mass of the predictors, and hence its influence. The formula for calculating leverages is complicated, so we’ll use a computer to calculate them. Leverages are usually denoted with the letter \\(h\\), so the leverage for the \\(i\\)th data point is \\(h_i\\). If we looked at the equation for a leverage, however, we would discover that leverages are strictly functions of the predictors, and do not depend on th response. To calculate leverages in R, first pass the regression model object to the function influence.lm. This function returns several diagnostic measures; the leverages are contained in the component called hat. To extract the leverages, use code like the following: fm1 &lt;- lm(BAC ~ weight + beers, data = beer) fm1.diagnostics &lt;- lm.influence(fm1) lev &lt;- fm1.diagnostics$hat Here is a look at some of the leverage values for the BAC data: head(cbind(beer, lev)) ## BAC weight beers beers.c weight.c lev ## 1 0.100 132 5 0.1875 -39.5625 0.1118535 ## 2 0.030 128 2 -2.8125 -43.5625 0.1948842 ## 3 0.190 110 9 4.1875 -61.5625 0.5176556 ## 4 0.120 192 8 3.1875 20.4375 0.2029871 ## 5 0.040 172 3 -1.8125 0.4375 0.1111125 ## 6 0.095 250 7 2.1875 78.4375 0.2588899 Not surprisingly, the point with the greatest leverage is the 110-lb person who drank 9 beers. What qualifies as a large leverage? It can be shown that, in a regression model with \\(k\\) predictors, the total of the leverages for all the data points must equal \\(k+1\\). Therefore, a typical leverage value will be \\((k+1)/n\\). 2.7.2 Standardized residuals Data points associated with large leverages tend to have smaller (raw) residuals. A better choice than analyzing the raw residuals is to analyze the standardized residuals. The formula for a standardized residual is: \\[ e_i^{(s)} =\\frac{e_i}{s_\\varepsilon \\sqrt{1-h_i} } \\] The benefit of standardized residuals is that if our model assumptions are appropriate, then they should behave like an iid sample from a normal distribution with mean 0 and variance 1. That is to say, most standardized residuals should be between -2 and +2, and only a few should be \\(&lt; -3\\) or \\(&gt; +3\\). Some texts call these “studentized residuals” instead of standardized residuals. R does not have a built-in function for calculating standardized residuals. However, there is a library of functions called the MASS library that contains the function stdres. (MASS is an acronym for , which is one of the original advanced texts for using R. It is written by W.N. Venables and B.D. Ripley.) MASS::stdres(fm1) ## 1 2 3 4 5 6 7 ## 0.8307561 -0.3611695 1.4198337 -1.0767727 0.2664003 0.6708175 1.6189097 ## 8 9 10 11 12 13 14 ## -1.6125272 -1.6623992 1.2179975 -0.2650284 0.1241508 -0.8509379 -0.0485307 ## 15 16 ## 1.0854287 -0.6259662 2.7.3 Cook’s distance Leverages and standardized residuals can be combined into various quantities that measure the influence each observation has on the fitted regression line. One of the most popular of these are Cook’s distance, \\(D_i\\). In R, if you pass a regression model to the command plot, it will produce four (somewhat sophisticated) diagnostic plots. The last of these shows Cook’s distance. plot(fm1) Observations with large values of Cook’s distance merit greater scrutiny. Appendix: Regression as a linear algebra problem This section assumes familiarity with linear algebra. Ultimately, the linear statistical model (that encompasses regression and ANOVA) is a linear-algebra problem. In short, the least-squares estimates are found by projecting the data vector (an element of \\(\\mathcal{R}^n\\)) onto the linear subspace of \\(\\mathcal{R}^n\\) spanned by the predictors. In matrix notation, the regression equations for a model with \\(k\\) predictors can be written compactly as \\[ \\mathbf{Y}= \\mathbf{X}\\mathbf{\\beta}+ \\mathbf{\\epsilon} \\] where \\[ \\mathbf{Y}= \\left[\\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array}\\right], \\] \\[ \\mathbf{X}= \\left[\\begin{array}{cccc} 1 &amp; x_{11} &amp; \\cdots &amp; x_{1k} \\\\ 1 &amp; x_{21} &amp; \\cdots&amp; x_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; \\cdots &amp; x_{nk} \\end{array}\\right], \\] \\[ \\mathbf{\\beta}=\\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_k \\end{array}\\right], \\] \\[ \\mathbf{\\epsilon}=\\left[\\begin{array}{c} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{array}\\right]. \\] The most important component of the above equation is the \\(\\mathbf{X}\\) matrix, also called the design matrix. Here are the first few rows of the design matrix for the BAC regression that includes beers consumed and weight as the two predictors: \\[ \\mathbf{X}=\\left[\\begin{array}{ccc} {1} &amp; {5} &amp; {132} \\\\ {1} &amp; {2} &amp; {128} \\\\ {1} &amp; {9} &amp; {110} \\\\ {1} &amp; {8} &amp; {192} \\\\ {\\vdots } &amp; {\\vdots } &amp; {\\vdots } \\end{array}\\right] \\] The short of the long is that the vector of least-squares estimates can be found by the matrix equation \\[ \\hat{\\mathbf{\\beta}}= \\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39;\\mathbf{Y} \\] Singular, or pathological, design matrices The power and beauty of the equation \\(\\hat{\\mathbf{\\beta}}= \\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39;\\mathbf{Y}\\) is that it works for any regression or ANOVA model, not just SLR. However, the matrix inverse \\(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\) does not exist for every possible choice of \\(\\mathbf{X}\\) matrices. Roughly, there are some pathological \\(\\mathbf{X}\\) matrices for which trying to find the matrix inverse \\(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\) is equivalent to dividing by zero. For example, suppose you are studying the effect of temperature on weight gain in fish, and measure temperature in both degrees Fahrenheit and Centigrade. (Recall that one can convert between Fahrenheit and Centigrade by the equation F = (9/5) C + 32.) The design matrix might be \\[ \\mathbf{X}=\\left[\\begin{array}{ccc} {1} &amp; {5} &amp; {41} \\\\ {1} &amp; {10} &amp; {50} \\\\ {1} &amp; {15} &amp; {59} \\\\ {1} &amp; {20} &amp; {68} \\end{array}\\right] \\] where the predictor in the 2\\(^{nd}\\) column is degrees Centigrade and the predictor in the 3\\(^{rd}\\) column is degrees Fahrenheit. Let’s try to fit a regression model in R for some made up values of \\(y\\): bad.data &lt;- data.frame(y = c(2.4, 6.1, 4.4, 7.0), degC = c(5, 10, 15, 20), degF = c(41, 50, 59, 68)) summary(lm(y ~ degC + degF, data = bad.data)) ## ## Call: ## lm(formula = y ~ degC + degF, data = bad.data) ## ## Residuals: ## 1 2 3 4 ## -0.76 1.73 -1.18 0.21 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.9500 1.9378 1.006 0.420 ## degC 0.2420 0.1415 1.710 0.229 ## degF NA NA NA NA ## ## Residual standard error: 1.582 on 2 degrees of freedom ## Multiple R-squared: 0.5938, Adjusted R-squared: 0.3908 ## F-statistic: 2.924 on 1 and 2 DF, p-value: 0.2294 The NA values for the partial regression coefficient associated with degrees Fahrenheit tells us that something has gone awry. While this may seem to be cause for concern, all that is happening here is that the model contains two predictors that are perfectly correlated with one another. (Actually, \\(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\) will fail to exist if the regression contains two different weighted sums of predictors that are perfectly correlated with one another.) When regression models contain perfectly correlated predictors (or perfectly correlated weighted sums of predictors), then there is no way to separate the linear associations between each of the (combinations of) predictors and the response. Thus, the inability to find least-squares regression estimates in some cases is just the logical consequence of the fact that it is impossible to separate the effects of two perfectly correlated predictors on a single response. Unfortunately, the computer output doesn’t provide a plain-language explanation of the problem. Each software package behaves a bit differently when faced with a case where the matrix inverse \\(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\) does not exist. In R, the program lm lops off columns of the design matrix (starting from the right-most column) until it obtains a design matrix where the inverse \\(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\) does exist, and the computations can proceed. This is why the output above does not provide values for the regression coefficient associated with degrees Fahrenheit. There is nothing about degrees Fahrenheit as opposed to degrees Celsius that makes degrees Fahrenheit a problematic predictor. It is simply that the two predictors are perfectly correlated, so lm responds by deleting the last predictor given — in this case degrees Fahrenheit — and proceeding. Additional results We can derive additional results by using using the theory of multivariate normal random variables. The distributional assumptions of the regression model are encapsulated in the assumption that \\(\\mathbf{\\epsilon}\\) has a multivariate normal distribution with mean \\(\\mathbf{0}\\) (a vector of 0’s) and variance matrix \\(\\sigma^2_\\varepsilon \\mathbf{I}\\), where \\(\\mathbf{I}\\) is the (\\(n\\)-by-\\(n\\)) identity matrix. (Recall that a variance matrix includes variances on the diagonal and covariances in the off-diagonal elements. Thus, the statement \\(\\bf{\\Sigma} = \\sigma^2_\\varepsilon \\mathbf{I}\\) says that every component of \\(\\mathbf{\\epsilon}\\) has variance \\(\\sigma^2_\\varepsilon\\), and that the covariance between any two components is 0.) Or, using \\(\\mathcal{N}\\) to denote a (univariate or multivariate) normal distribution, we can write \\[ \\mathbf{\\epsilon}\\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2_\\varepsilon \\mathbf{I}). \\] It then follows that \\[ \\mathbf{Y}\\sim \\mathcal{N}(\\mathbf{X}\\mathbf{\\beta}, \\sigma^2_\\varepsilon \\mathbf{I}). \\] Let \\(\\mathrm{E}\\left[\\cdot\\right]\\) and \\(\\mbox{Var}\\left(\\cdot\\right)\\) denote the expectation (mean) and variance of a random variable, respectively. First, we can show that \\(\\hat{\\mathbf{\\beta}}\\) is an ``unbiased’’ estimate of \\(\\mathbf{\\beta}\\), using the linearity of expectations: \\[\\begin{eqnarray*} \\mathrm{E}\\left[\\hat{\\mathbf{\\beta}}\\right] &amp; = &amp; \\mathrm{E}\\left[\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39;\\mathbf{Y}\\right] \\\\ &amp; = &amp; \\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39;\\mathrm{E}\\left[\\mathbf{Y}\\right] \\hspace{0.5in} \\mbox{(linearity)}\\\\ &amp; = &amp; \\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39; \\mathbf{X}\\mathbf{\\beta}\\\\ &amp; = &amp; \\mathbf{\\beta}. \\end{eqnarray*}\\] Next, we can find the variance of \\(\\hat{\\mathbf{\\beta}}\\) using a result for the variance of linear combinations of random variables: \\[\\begin{eqnarray*} \\mbox{Var}\\left(\\hat{\\mathbf{\\beta}}\\right) &amp; = &amp; \\mbox{Var}\\left(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39;\\mathbf{Y}\\right) \\\\ &amp; = &amp; \\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39; \\mbox{Var}\\left(\\mathbf{Y}\\right) \\mathbf{X}\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\\\ &amp; = &amp; \\sigma^2_{\\varepsilon} \\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39; \\mathbf{X}\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\\\ &amp; = &amp; \\sigma^2_{\\varepsilon} \\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}. \\end{eqnarray*}\\] The second equality above is a quadratic form, and uses the fact that \\(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\) is symmetric (and thus equal to its transpose). The final result, \\(\\mbox{Var}\\left(\\hat{\\mathbf{\\beta}}\\right) = \\sigma^2_{\\varepsilon} \\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\), shows that the variances of the least squares estimates (and thus their standard errors) are proportional to the diagonal elements of \\(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\). Finally, let \\(\\mathbf{\\hat{Y}}\\) be a vector of fitted values, i.e., \\(\\mathbf{\\hat{Y}}= \\left[ \\hat{y}_1, \\hat{y_2}, \\ldots, \\hat{y_n} \\right]&#39;\\). We can find an experssion for \\(\\mathbf{\\hat{Y}}\\) simply as: \\[\\begin{eqnarray*} \\mathbf{\\hat{Y}}&amp; = &amp; \\mathbf{X}\\hat{\\mathbf{\\beta}}\\\\ &amp; = &amp; \\mathbf{X}\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39; \\mathbf{Y} \\end{eqnarray*}\\] The matrix \\(\\mathbf{X}\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39;\\) (sometimes called the hat matrix, because it maps \\(\\mathbf{Y}\\) to \\(\\mathbf{\\hat{Y}}\\)) is a projection matrix, and thus the fitted values are the orthogonal projection of \\(\\mathbf{Y}\\) onto the columnspace of \\(\\mathbf{X}\\). The right-triangle that results from the vectors \\(\\mathbf{Y}\\) (the hypotenuse), \\(\\mathbf{\\hat{Y}}\\), and \\(\\mathbf{e}= \\mathbf{Y}- \\mathbf{\\hat{Y}}\\) gives rise to the sum-of-squares decomposition behind \\(R^2\\). There are many good texts that present the linear-algebra formulation of the linear statistical model, including Sen and Srivastava (1997) and Monahan (2008). Bibliography Berry, Donald A. 2016. “P-Values Are Not What They’re Cracked up to Be.” Online Commentary to ASA Statement on Statistical Significance and \\(p\\)-Values.(doi: 10.1080/00031305.2016. 1154108). Bowerman, Bruce L, and Richard T O’Connell. 1990. Linear Statistical Models: An Applied Approach. Brooks/Cole. Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press. McIntyre, Lauren. 1994. “Using Cigarette Data for an Introduction to Multiple Regression.” Journal of Statistics Education 2 (1). Mendenhall, William M, and Terry L Sincich. 2012. Statistics for Engineering and the Sciences. New York: Dellen Publishing Co. Monahan, John F. 2008. A Primer on Linear Models. CRC Press. Moore, David S, and George P McCabe. 1989. Introduction to the Practice of Statistics. WH Freeman. Quinn, Gerry P, and Michael J Keough. 2002. Experimental Design and Data Analysis for Biologists. Cambridge university press. Rawlings, John O, Sastry G Pantula, and David A Dickey. 1998. Applied Regression Analysis: A Research Tool. Springer. Sackett, Dana K, W Gregory Cope, James A Rice, and D Derek Aday. 2013. “The Influence of Fish Length on Tissue Mercury Dynamics: Implications for Natural Resource Management and Human Health Risk.” International Journal of Environmental Research and Public Health 10 (2): 638–59. Sen, Ashish, and Muni Srivastava. 1997. Regression Analysis: Theory, Methods, and Applications. Springer Science &amp; Business Media. Whether a variable should be treated as quantitative or categorical is not always clear cut. For example, consider an individual’s political leaning, and suppose that for the sake of argument we could classify each individual’s political leaning as either conservative, moderate, or liberal. Most everyone would agree that these three values have a natural ordering, with moderates occupying a middle ground between conservatives and liberals. However, the operative question for the analyst is whether to assume that moderates are exactly halfway between conservatives and liberals. In other words, is the difference between conservatives and moderates exactly the same size as the difference between moderates and liberals? If we are willing to make this assumption, then we could code political leaning with a quantitative variable, taking values (say) 0, 1, and 2 for conservatives, moderates, and liberals, respectively. If we weren’t willing to make this assumption, then we should treat political leaning as a categorical variable with three levels. There is scope here for thoughtful analysts to disagree.↩︎ The contrast command does much more than report how indicator variables are coded for a categorical variable, as we will see later.↩︎ If we prefer, it is easy enough to select a different reference level in R. Suppose we wanted Bennett to serve as the reference site. We could do so with the code contrasts(fish$site) &lt;- contr.treatment(n = 3, base = 2) See the R documentation for contr.treatment for more details.↩︎ While the possibility of correlations among predictors is important, that is not what we are discussing here. Instead, correlations among predictors are the subject of collinearity.↩︎ To take this argument a little further, we don’t expect an association between weight and BAC for people who have not drunk any beers. Thus, it is perhaps not surprising that we cannot reject \\(H_0: \\beta_2 = 0\\) in favor of \\(H_0: \\beta_2 \\ne 0\\) in the model that includes the interaction between weight and beers consumed.↩︎ Indeed, if the predictors are perfectly uncorrelated with one another, then the multiple regression coefficients will be identical to the slopes from individual simple regression models.↩︎ Another feature of these data that immediately catches the eye is that there is one cigarette — Bull Durham brand — that has noticeably larger values of all variables. There is also a brand — Now brand — that has noticeably lower values of all variables. These two data points will have large leverage, and we might wonder to what extent the fit will be driven by these two data points alone. That’s a fair question, and one that we should ask in a complete analysis of these data. But it’s beside the point for the present purposes, so we won’t engage with it here.↩︎ Here, we use “bias” in its technical sense, meaning the difference between the average of a sampling distribution of an estimate, and the parameter we are trying to estimate. All else being equal, we prefer estimators that are unbiased. However, sometimes a small amount of bias may be acceptable if it leads to big improvements in other properties of the estimator.↩︎ Indeed, this isn’t just true for AIC. There are many estimators in statistics that are only asymptotically unbiased but which are routinely used for small data sets anyway, without any guarantees that those estimators behave well or predictably for small data sets.↩︎ "],["non-linear-regression.html", "Chapter 3 Non-linear regression 3.1 Polynomial regression 3.2 Non-linear least squares", " Chapter 3 Non-linear regression In this chapter, we examine several methods for characterizing non-linear associations between a predictor variable and the response. To keep things simple, we return to focusing on settings with a single predictor. However, the ideas in this chapter can readily be incorporated into models with several predictor variables. 3.1 Polynomial regression Polynomial regression uses the machinery of multiple regression to model non-linear relationships. A \\(k^{th}\\) order polynomial regression model is \\[ y=\\beta_0 +\\beta_1 x+\\beta_2 x^2 +\\beta_3 x^{3} +\\ldots +\\beta_k x^{k} +\\varepsilon \\] where the error term is subject to the standard regression assumptions. In practice, the most commonly used models are quadratic (\\(k=2\\)) and cubic (\\(k=3\\)) polynomials. Before proceeding, a historical note is worthwhile. It used to be that polynomial regression was the only way to accommodate non-linear relationships in regression models. In the present day, non-linear least squares allows us to fit a much richer set of non-linear models to data. However, in complex models (especially complex ANOVA models for designed experiments), there are still cases where it is easier to add a quadratic term to accommodate a non-linear association than it is to adopt the machinery of non-linear least squares. Thus, it is still worthwhile to know a little bit about polynomial regression, but don’t shoehorn every non-linear association into a polynomial regression if an alternative non-linear model is more suitable. Example. In the cars data, the relationship between highway mpg and vehicle weight is clearly non-linear: cars &lt;- read.table(&quot;data/cars.txt&quot;, head = T, stringsAsFactors = T) with(cars, plot(mpghw ~ weight, xlab = &quot;Vehicle weight (lbs)&quot;, ylab = &quot;Highway mpg&quot;)) To fit a quadratic model, we could manually create a predictor equal to weight-squared. Or, in R, we could create the weight-squared predictor within the call to “lm” by using the following syntax: quad &lt;- lm(mpghw ~ weight + I(weight^2), data = cars) summary(quad) ## ## Call: ## lm(formula = mpghw ~ weight + I(weight^2), data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.4386 -1.8216 0.1789 2.3617 7.5031 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.189e+01 6.332e+00 14.511 &lt; 2e-16 *** ## weight -2.293e-02 3.119e-03 -7.353 1.64e-11 *** ## I(weight^2) 1.848e-06 3.739e-07 4.942 2.24e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.454 on 136 degrees of freedom ## Multiple R-squared: 0.7634, Adjusted R-squared: 0.7599 ## F-statistic: 219.4 on 2 and 136 DF, p-value: &lt; 2.2e-16 In the quadratic regression \\(y=\\beta_0 +\\beta_1 x+\\beta_2 x^2 +\\varepsilon\\), the test of \\(H_0\\): \\(\\beta_2=0\\) vs. \\(H_a\\): \\(\\beta_2 \\ne 0\\) is tantamount to a test of whether the quadratic model provides a significantly better fit than the linear model. In this case, we can conclusively reject \\(H_0\\): \\(\\beta_2=0\\) in favor of \\(H_a\\): \\(\\beta_2 \\ne 0\\) , and thus conclude that the quadratic model provides a significantly better fit than the linear model. However, in the context of the quadratic model, the test of \\(H_0\\): \\(\\beta_1=0\\) vs. \\(H_a\\): \\(\\beta_1 \\ne 0\\) doesn’t give us much useful information. In the context of the quadratic model, the null hypothesis \\(H_0\\): \\(\\beta_1=0\\) is equivalent to the model \\(y=\\beta_0 +\\beta_2 x^2 +\\varepsilon\\). This is a strange model, and there is no reason why we should consider it. Thus, we disregard the inference for \\(\\beta_1\\), and (by similar logic) we disregard the inference for \\(\\beta_0\\) as well. If a quadratic model is good, will the cubic model \\(y=\\beta_0 +\\beta_1 x+\\beta_2 x^2 +\\beta_3 x^{3} +\\varepsilon\\) be even better? Let’s see: cubic &lt;- lm(mpghw ~ weight + I(weight^2) + I(weight^3), data = cars) summary(cubic) ## ## Call: ## lm(formula = mpghw ~ weight + I(weight^2) + I(weight^3), data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.247 -1.759 0.281 2.411 7.225 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.164e+02 2.697e+01 4.318 3.03e-05 *** ## weight -4.175e-02 2.033e-02 -2.054 0.042 * ## I(weight^2) 6.504e-06 4.984e-06 1.305 0.194 ## I(weight^3) -3.715e-10 3.966e-10 -0.937 0.351 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.456 on 135 degrees of freedom ## Multiple R-squared: 0.7649, Adjusted R-squared: 0.7597 ## F-statistic: 146.4 on 3 and 135 DF, p-value: &lt; 2.2e-16 In the cubic model, the test of \\(H_0\\): \\(\\beta_3=0\\) vs. \\(H_a\\): \\(\\beta_3 \\ne 0\\) is tantamount to a test of whether the cubic model provides a significantly better fit than the quadratic model. The \\(p\\)-value associated with the cubic term suggests that the cubic model does not provide a statistically significant improvement in fit compared to the quadratic model. At this point, you might wonder if we are limited only to comparing models of adjacent orders, that is, quadratic vs. linear, cubic vs. quadratic, etc. The answer is no — we can, for example, test whether a cubic model provides a significantly better fit than a linear model. To do so, we would have to test \\(H_0\\): \\(\\beta_2 = \\beta_3 = 0\\) in the cubic model. We can test this null hypothesis with an \\(F\\)-test. Even though as cubic model does not offer a significantly better fit than a quadratic model, we have not necessarily ruled out the possibility that a higher-order polynomial model might provide a significantly better fit. However, higher-order polynomials (beyond a cubic) are typically difficult to justify on scientific grounds, and offend our sense of parsimony. Plus, a plot of the quadratic model and the associated residuals suggest that a quadratic model captures the trend in the data well: with(cars, plot(mpghw ~ weight, xlab = &quot;Vehicle weight (lbs)&quot;, ylab = &quot;Highway mpg&quot;)) quad &lt;- with(cars, lm(mpghw ~ weight + I(weight^2))) quad.coef &lt;- as.vector(coefficients(quad)) quad.fit &lt;- function(x) quad.coef[1] + quad.coef[2] * x + quad.coef[3] * x^2 curve(quad.fit, from = min(cars$weight), to = max(cars$weight), add = TRUE, col = &quot;red&quot;) plot(x = fitted(quad), y = resid(quad), xlab = &quot;Fitted values&quot;, ylab = &quot;Residuals&quot;) abline(h = 0, lty = &quot;dashed&quot;) Therefore, the quadratic model clearly provides the best low-order polynomial fit to these data. Finally, it doesn’t make sense to consider models that include higher-order terms without lower-order terms. For example, we wouldn’t usually consider a cubic model without an intercept, or a quadratic model without a linear term. Geometrically, these models are constrained in particular ways. If such a constraint makes sense scientifically, entertaining the model may be warranted, but this situation arises only rarely. Thus, our strategy for fitting polynomial models is to choose the lowest-order model that provides a reasonable fit to the data, and whose highest-order term is statistically significant. 3.2 Non-linear least squares Today, software is readily available to fit non-linear models to data using the same least-squares criterion that we use to estimate parameters in the linear model. The computation involved in fitting a non-linear model is fundamentally different from the computation involved in a linear model. A primary difference is that there is no all-purpose formula like \\(\\hat{\\mathbf{\\beta}}=\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39;\\mathbf{Y}\\) available for the non-linear model. Therefore, parameter estimates (and their standard errors) have to be found using a numerical algorithm. (We’ll see more about what this means in a moment.) However, these algorithms are sufficiently well developed that they now appear in most common statistical software packages, such as R, SAS, or others. In R, the command that we use to fit a non-linear model is nls, for [n]on-linear [l]east [s]quares. In SAS, non-linear models can be fit using PROC NLIN. Ex. Puromycin. This example is taken directly from the text Nonlinear regression analysis and its applications, by D.M. Bates and D.G. Watts Bates and Watts (1988). The data themselves are from Treloar (1974, MS Thesis, Univ of Toronto), who studied the relationship between the velocity of an enzymatic reaction (the response, measured in counts / minute\\(^2\\)) vs. the concentration of a particular substrate (the predictor, measured in parts per million). The experiment was conducted in the presence of the antibiotic Puromycin. The data are shown below. puromycin &lt;- read.table(&quot;data/puromycin.txt&quot;, head = T, stringsAsFactors = T) with(puromycin, plot(velocity ~ conc, xlab = &quot;concentration&quot;, ylab = &quot;velocity&quot;)) It is hypothesized that these data can be described by the Michaelis-Menten model for puromycin kinetics. The Michaelis-Menten model is: \\[ y=\\frac{\\theta_1 x}{\\theta_2 +x} +\\varepsilon \\] We continue to assume that the errors are iid normal with mean 0 and unknown but constant variance, i.e., \\(\\varepsilon_i \\sim \\mathcal{N}\\left(0,\\sigma_{\\varepsilon}^2 \\right)\\). With non-linear models, it is helpful if one can associate each of the parameters with a particular feature of the best-fitting curve. With these data, it seems that the best fitting curve is one that will increase at a decelerating rate until it approaches an asymptote. A little algebra shows that we can interpret \\(\\theta_1\\) directly as the asymptote (that is, the limiting value of the curve as \\(x\\) gets large), and \\(\\theta_2\\) as the value of the predictor at which the fitted curve reaches one-half of its asymptotic value. To estimate parameters, we can define a least-squares criterion just as before. That is to say, the least-squares estimates of \\(\\theta_1\\) and \\(\\theta_2\\) will be the values that minimize \\[ SSE=\\sum_{i=1}^ne_i^2 = \\sum_{i=1}^n\\left(y_i -\\hat{y}_i \\right)^2 =\\sum_{i=1}^n\\left(y_i -\\left[\\frac{\\hat{\\theta }_1 x_i }{\\hat{\\theta }_{2} +x_i } \\right]\\right)^2 \\] Unlike with the linear model, there is no formula that can be solved directly to find the least-squares estimates. Instead, the least-squares estimates (and their standard errors) must be found using a numerical minimization algorithm. That is, the computer will use a routine to iteratively try different parameter values (in an intelligent manner) and proceed until it thinks it has found a set of parameter values that minimize the SSE (within a certain tolerance). While we can trust that the numerical minimization routine implemented by R or SAS is a reasonably good one, all numerical minimization routines rely critically on finding a good set of starting values for the parameters. That is, unlike in a linear model, we must initiate the algorithm with a reasonable guess of the parameter values that is in the ballpark of the least-squares estimates. Here is where it is especially beneficial to have direct interpretations of the model parameters. Based on our previous analysis, we might choose a starting values of (say) \\(\\theta_1 = 200\\) and \\(\\theta_2 = 0.1\\). (Note that R will try to find starting values if they aren’t provided. However, the documentation to nls says that these starting values are a “very cheap guess”.) Equipped with our choice of starting values, we are ready to find the least-squares estimates using nls: fm1 &lt;- nls(velocity ~ theta1 * conc / (theta2 + conc), data = puromycin, start = list(theta1 = 200, theta2 = 0.1)) summary(fm1) ## ## Formula: velocity ~ theta1 * conc/(theta2 + conc) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## theta1 2.127e+02 6.947e+00 30.615 3.24e-11 *** ## theta2 6.412e-02 8.281e-03 7.743 1.57e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.93 on 10 degrees of freedom ## ## Number of iterations to convergence: 6 ## Achieved convergence tolerance: 6.093e-06 In the call to nls, the first argument is a formula where we specify the non-linear model that we wish to fit. In this data set, “velocity” is the response and “conc” is the predictor. The last argument to nls is a list of starting values. The list contains one starting value for each parameter in the model. (In R, “lists” are like vectors, except that lists can contain things other than numbers.) The output shows that the least squares estimates are \\(\\hat{\\theta}_1 =212.7\\) and \\(\\hat{\\theta}_2 =0.064\\). We also get estimated standard errors for each of the parameters, as well as \\(t\\)-tests of \\(H_0\\): \\(\\theta =0\\) vs. \\(H_a\\): \\(\\theta \\ne 0\\). Note that the \\(t\\)-tests are not particularly useful in this case — there’s no reason why we would entertain the possibility that either \\(\\theta_1\\) or \\(\\theta_2\\) are equal to 0. The last portion of the output from nls tells us about the performance of the numerical algorithm that was used to find the least-squares estimates. We won’t delve into this information here, but if you need to use non-linear least squares for something important, be sure to acquaint yourself with what this output means. Like linear least-squares, there are cases where non-linear least squares will not work (or will not work well), and it is this portion of the output that will give you a clue when you’ve encountered one of these cases. We can examine the model fit by overlaying a fitted curve: with(puromycin, plot(velocity ~ conc, xlab = &quot;concentration&quot;, ylab = &quot;velocity&quot;)) mm.fit &lt;- function(x) (212.7 * x) / (0.06412 + x) curve(mm.fit, from = min(puromycin$conc), to = max(puromycin$conc), col = &quot;red&quot;, add = TRUE) It is instructive to compare the fit of this non-linear model with the fit from a few polynomial regressions. Neither the quadratic nor the cubic models fits very well in this case. Polynomial models often have a difficult time handling a data set with an asymptote. In this case, the Michaelis-Menten model clearly seems preferable. quad &lt;- lm(velocity ~ conc + I(conc^2), data = puromycin) cubic &lt;- lm(velocity ~ conc + I(conc^2) + I(conc^3), data = puromycin) quad.coef &lt;- as.vector(coefficients(quad)) quad.fit &lt;- function(x) quad.coef[1] + quad.coef[2] * x + quad.coef[3] * x^2 cubic.coef &lt;- as.vector(coefficients(cubic)) cubic.fit &lt;- function(x) cubic.coef[1] + cubic.coef[2] * x + cubic.coef[3] * x^2 + cubic.coef[4] * x^3 with(puromycin, plot(velocity ~ conc, xlab = &quot;concentration&quot;, ylab = &quot;velocity&quot;, ylim = c(min(velocity), 230))) curve(quad.fit, from = min(puromycin$conc), to = max(puromycin$conc), add = TRUE, col = &quot;blue&quot;) curve(cubic.fit, from = min(puromycin$conc), to = max(puromycin$conc), add = TRUE, col = &quot;forestgreen&quot;) legend(x = 0.6, y = 100, legend = c(&quot;quadratic&quot;, &quot;cubic&quot;), col = c(&quot;blue&quot;, &quot;darkgreen&quot;), lty = &quot;solid&quot;, bty = &quot;n&quot;) Bibliography Bates, Douglas M, and Donald G Watts. 1988. Nonlinear Regression Analysis and Its Applications. Wiley. "],["generalized-linear-models.html", "Chapter 4 Generalized linear models 4.1 Poisson regression 4.2 Binary responses 4.3 Implementation in SAS", " Chapter 4 Generalized linear models Generalized linear modes extend the machinery of the “general linear model” (regression and ANOVA) to data sets in which the response variable may have a non-Gaussian distribution. Generalized linear models do not encompass all possible distributions for the response variable. Instead, the distribution of the response variable must belong to a group of distributions known as the “exponential family”. (Confusingly, there is also such a thing as an exponential distribution. The exponential distribution is a member of the exponential family, but it is not the only one.) The exponential family of distributions includes many of the distributions that we encounter in practical data analysis, including Poisson, negative binomial, binomial, gamma, and beta distributions. The Gaussian distribution is included in the exponential family as well. One notable distribution that is not part of the exponential family is the \\(t\\)-distribution. Distributions in the exponential family all give rise to likelihoods that share the same general form, and thus can be handled with a unified fitting scheme. In practice, logistic regression (with binomial responses) and Poisson regression are far and away the two most common forms of generalized linear models that one encounters. 4.1 Poisson regression We will begin with an example of Poisson regression. These data are originally from Poole (1989), and were analyzed in Ramsey and Schafer (2012). They describe an observational study of 41 male elephants over 8 years at Amboseli National Park in Kenya. Each record in this data set gives the age of a male elephant at the beginning of a study and the number of successful matings for the elephant over the study’s duration. The number of matings is a count variable. Our goal is to characterize how the number of matings is related to the elephant’s age. We’ll start by fitting a model with the canonical log link. elephant &lt;- read.table(&quot;data/elephant.txt&quot;, head = T) head(elephant) ## age matings ## 1 27 0 ## 2 28 1 ## 3 28 1 ## 4 28 1 ## 5 28 3 ## 6 29 0 with(elephant, plot(matings ~ age)) fm1 &lt;- glm(matings ~ age, family = poisson(link = &quot;log&quot;), data = elephant) # log link is the default summary(fm1) ## ## Call: ## glm(formula = matings ~ age, family = poisson(link = &quot;log&quot;), ## data = elephant) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.58201 0.54462 -2.905 0.00368 ** ## age 0.06869 0.01375 4.997 5.81e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 75.372 on 40 degrees of freedom ## Residual deviance: 51.012 on 39 degrees of freedom ## AIC: 156.46 ## ## Number of Fisher Scoring iterations: 5 Thus the so-called pseudo-\\(R^2\\) for the model with the log link is \\[ \\mathrm{pseudo}-R^2 = 1 - \\frac{51.012}{75.372} = 32.3\\% \\] We can visualize the fit by plotting a best-fitting line with a 95% confidence interval. Because the scale parameter is not estimated here, we will use a critical value from a standard normal distribution. Later, when we estimate the scale parameter based on data, we will use a critical value from a \\(t\\)-distribution instead. new.data &lt;- data.frame(age = seq(from = min(elephant$age), to = max(elephant$age), length = 100)) predict.fm1 &lt;- predict(fm1, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) with(elephant, plot(matings ~ age)) lines(x = new.data$age, y = predict.fm1$fit, col = &quot;red&quot;) # add lines for standard errors lines(x = new.data$age, y = predict.fm1$fit - 1.96 * predict.fm1$se.fit, col = &quot;red&quot;, lty = &quot;dashed&quot;) lines(x = new.data$age, y = predict.fm1$fit + 1.96 * predict.fm1$se.fit, col = &quot;red&quot;, lty = &quot;dashed&quot;) While the canonical link is a natural starting point, we are free to try other link functions as well. Below, we try the identity link and plot the fit. fm2 &lt;- glm(matings ~ age, family = poisson(link = &quot;identity&quot;), data = elephant) summary(fm2) ## ## Call: ## glm(formula = matings ~ age, family = poisson(link = &quot;identity&quot;), ## data = elephant) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.55205 1.33916 -3.399 0.000676 *** ## age 0.20179 0.04023 5.016 5.29e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 75.372 on 40 degrees of freedom ## Residual deviance: 50.058 on 39 degrees of freedom ## AIC: 155.5 ## ## Number of Fisher Scoring iterations: 5 predict.fm2 &lt;- predict(fm2, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) with(elephant, plot(matings ~ age)) lines(x = new.data$age, y = predict.fm2$fit, col = &quot;blue&quot;) lines(x = new.data$age, y = predict.fm2$fit - 1.96 * predict.fm2$se.fit, col = &quot;blue&quot;, lty = &quot;dashed&quot;) lines(x = new.data$age, y = predict.fm2$fit + 1.96 * predict.fm2$se.fit, col = &quot;blue&quot;, lty = &quot;dashed&quot;) Note that the choice of the link function has a substantial impact on the shape of the fit. The canonical (log) link suggests that the average number of matings increases with age at an accelerating rate, while the identity link suggests that the average number of matings increases steadily with age. The AIC favors the identity link here. We can also have a look at the residuals to see if they suggest any model deficiencies. In general, we prefer the deviance residuals, so we will look at them. plot(x = elephant$age, y = residuals(fm2, type = &quot;deviance&quot;), xlab = &quot;age&quot;, ylab = &quot;Deviance residuals&quot;) abline(h = 0, lty = &quot;dashed&quot;) The residuals do not suggest any deficiency in the fit. For this fit, the residual deviance suggests a small amount of overdispersion. To be on the safe side, we can fit a quasi-Poisson model in which the scale (overdispersion) parameter is estimated from the data. Note that when we estimate the overdispersion parameter, the estimates of the model parameters do not change, but their standard errors increase. Consequently, the uncertainty in the fit increases as well. In this case, however, the increase is so slight that it is barely noticeable. fm3 &lt;- glm(matings ~ age, family = quasipoisson(link = &quot;identity&quot;), data = elephant) summary(fm3) ## ## Call: ## glm(formula = matings ~ age, family = quasipoisson(link = &quot;identity&quot;), ## data = elephant) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.55205 1.42164 -3.202 0.00272 ** ## age 0.20179 0.04271 4.725 2.97e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 1.126975) ## ## Null deviance: 75.372 on 40 degrees of freedom ## Residual deviance: 50.058 on 39 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 5 predict.fm3 &lt;- predict(fm3, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) with(elephant, plot(matings ~ age)) lines(x = new.data$age, y = predict.fm3$fit, col = &quot;blue&quot;) lines(x = new.data$age, y = predict.fm3$fit + qt(0.025, df = 39) * predict.fm3$se.fit, col = &quot;blue&quot;, lty = &quot;dashed&quot;) lines(x = new.data$age, y = predict.fm3$fit + qt(0.975, df = 39) * predict.fm3$se.fit, col = &quot;blue&quot;, lty = &quot;dashed&quot;) As an alternative, we could fit a model that uses a negative binomial distribution for the response. Negative binomial distributions belong to the exponential family, so we can fit them using the GLM framework. However, the authors of glm did not include a negative binomial family in their initial code. Venables &amp; Ripley’s MASS package includes a program called glm.nb which is specifically designed for negative binomial responses. MASS::glm.nb uses the parameterization familiar to ecologists, although they use the parameter \\(\\theta\\) instead of \\(k\\). So, in their notation, if \\(y \\sim \\mathrm{NB}(\\mu, \\theta)\\), then \\(\\mathrm{Var}(y) = \\mu + \\mu^2/\\theta\\). require(MASS) ## Loading required package: MASS fm4 &lt;- glm.nb(matings ~ age, link = identity, data = elephant) summary(fm4) ## ## Call: ## glm.nb(formula = matings ~ age, data = elephant, link = identity, ## init.theta = 15.80269167) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.56939 1.45770 -3.135 0.00172 ** ## age 0.20232 0.04428 4.569 4.9e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(15.8027) family taken to be 1) ## ## Null deviance: 64.836 on 40 degrees of freedom ## Residual deviance: 43.214 on 39 degrees of freedom ## AIC: 156.87 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 15.8 ## Std. Err.: 23.0 ## ## 2 x log-likelihood: -150.872 predict.fm4 &lt;- predict(fm4, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) with(elephant, plot(matings ~ age)) lines(x = new.data$age, y = predict.fm4$fit, col = &quot;blue&quot;) lines(x = new.data$age, y = predict.fm4$fit + 1.96 * predict.fm4$se.fit, col = &quot;blue&quot;, lty = &quot;dashed&quot;) lines(x = new.data$age, y = predict.fm4$fit - 1.96 * predict.fm4$se.fit, col = &quot;blue&quot;, lty = &quot;dashed&quot;) Notice that \\(\\hat{\\theta} = 15.8\\), again indicating that the extra-Poisson variation is mild. Notice also that the error bounds on the fitted curve are ever so slightly larger than the error bounds from the Poisson fit, and nearly identical to the error bounds from the quasi-Poisson fit. 4.2 Binary responses We generally distinguish between two types of data with binary responses: Data in which each individual record is a separate a binary response, and data in which each record consists of a group of binary observations. The same methods can be used for either type of data. We will begin by studying a data set with individual binary responses, and then use the industrial melanism data to illustrate grouped binary responses. 4.2.1 Individual binary responses: TB in boar To illustrate individual binary data, we will use a data set analyzed by Zuur et al. (2009). As explained there, these data describe the incidence of “tuberculosis-like lesions in wild boar Sus scrofa” in southern Spain, and were originally collected by Vicente et al. (2006). The potential explanatory variables in the data set include a measure of the animal’s size, it’s sex, and a grouping into one of four age classes. Preparatory work: boar &lt;- read.table(&quot;data/boar.txt&quot;, head = T) # remove incomplete records boar &lt;- na.omit(boar) # convert sex to a factor boar$SEX &lt;- as.factor(boar$SEX) names(boar) &lt;- c(&quot;tb&quot;, &quot;sex&quot;, &quot;age&quot;, &quot;length&quot;) summary(boar) ## tb sex age length ## Min. :0.0000 1:206 Min. :1.000 Min. : 46.5 ## 1st Qu.:0.0000 2:288 1st Qu.:3.000 1st Qu.:107.0 ## Median :0.0000 Median :3.000 Median :122.0 ## Mean :0.4575 Mean :3.142 Mean :117.3 ## 3rd Qu.:1.0000 3rd Qu.:4.000 3rd Qu.:130.4 ## Max. :1.0000 Max. :4.000 Max. :165.0 We’ll fit the usual logistic regression model first, considering only the animal’s size as a predictor. Size in this case is a measure of the length of the animal, in cm. fm1 &lt;- glm(tb ~ length, family = binomial(link = &quot;logit&quot;), data = boar) summary(fm1) ## ## Call: ## glm(formula = tb ~ length, family = binomial(link = &quot;logit&quot;), ## data = boar) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.137107 0.695381 -5.949 2.69e-09 *** ## length 0.033531 0.005767 5.814 6.09e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 681.25 on 493 degrees of freedom ## Residual deviance: 641.23 on 492 degrees of freedom ## AIC: 645.23 ## ## Number of Fisher Scoring iterations: 4 with(boar, plot(tb ~ length)) # add a line for the fitted probabilities of tb new.data &lt;- data.frame(length = seq(from = min(boar$length), to = max(boar$length), length = 100)) predict.fm1 &lt;- predict(fm1, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) lines(x = new.data$length, y = predict.fm1$fit, col = &quot;red&quot;) # add lines for standard errors # use critical value from z distribution here because # the scale parameter is not estimated lines(x = new.data$length, y = predict.fm1$fit - 1.96 * predict.fm1$se.fit, col = &quot;red&quot;, lty = &quot;dashed&quot;) lines(x = new.data$length, y = predict.fm1$fit + 1.96 * predict.fm1$se.fit, col = &quot;red&quot;, lty = &quot;dashed&quot;) Regression coefficients in logistic regression can be a bit hard to interpret. One interpretation flows from exponentiating the regression coefficient to obtain an odds ratio. For the boar data, the regression coefficient of 0.0335 corresponds to an odds ratio of \\(e^{0.0335}\\) = 1.034. This means that for two boars that differ by one cm in length, the larger boar’s odds of having a TB-like lesion will be 1.034 times the smaller boar’s odds of having such a lesion. Overdispersion is typically not an issue with individual binary response data. Nonetheless, the pseudo-\\(R^2\\) here is fairly low. We can try the probit and complementary log-log links to see if we obtain a better fit: # probit link fm1a &lt;- glm(tb ~ length, family = binomial(link = &quot;probit&quot;), data = boar) # complementary log-log link fm1b &lt;- glm(tb ~ length, family = binomial(link = &quot;cloglog&quot;), data = boar) AIC(fm1, fm1a, fm1b) ## df AIC ## fm1 2 645.2265 ## fm1a 2 645.2665 ## fm1b 2 645.6100 # make a plot to compare the fits with the different links predict.fm1a &lt;- predict(fm1a, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) predict.fm1b &lt;- predict(fm1b, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) with(boar, plot(tb ~ length)) lines(x = new.data$length, y = predict.fm1$fit, col = &quot;red&quot;, lwd = 2) lines(x = new.data$length, y = predict.fm1a$fit, col = &quot;blue&quot;, lwd = 2) lines(x = new.data$length, y = predict.fm1b$fit, col = &quot;forestgreen&quot;, lwd = 2) legend(&quot;left&quot;, leg = c(&quot;logit&quot;, &quot;probit&quot;, &quot;cloglog&quot;), col = c(&quot;red&quot;, &quot;blue&quot;, &quot;forestgreen&quot;), pch = 16) The logit and probit links are nearly identical. The complementary log-log link differs slightly, but the logit link is AIC-best. Try adding sex and age class as predictors. Some of the MLEs cannot be found, because none of the individuals with and are infected, thus the MLE of the log odds of infection for this group (which happens to be the baseline) is \\(-\\infty\\). This phenomenon is known as “complete separation”. # fit a model with sex, age (as a categorical predictor) and their interaction fm2 &lt;- glm(tb ~ length + sex * as.factor(age), family = binomial, data = boar) summary(fm2) ## ## Call: ## glm(formula = tb ~ length + sex * as.factor(age), family = binomial, ## data = boar) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -16.55356 724.50177 -0.023 0.982 ## length 0.01840 0.01253 1.469 0.142 ## sex2 14.19739 724.50190 0.020 0.984 ## as.factor(age)2 13.83446 724.50169 0.019 0.985 ## as.factor(age)3 14.31136 724.50191 0.020 0.984 ## as.factor(age)4 14.68141 724.50219 0.020 0.984 ## sex2:as.factor(age)2 -14.53254 724.50204 -0.020 0.984 ## sex2:as.factor(age)3 -14.36861 724.50196 -0.020 0.984 ## sex2:as.factor(age)4 -14.53354 724.50196 -0.020 0.984 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 681.25 on 493 degrees of freedom ## Residual deviance: 635.43 on 485 degrees of freedom ## AIC: 653.43 ## ## Number of Fisher Scoring iterations: 14 with(boar, table(tb, age, sex)) ## , , sex = 1 ## ## age ## tb 1 2 3 4 ## 0 4 37 37 28 ## 1 0 14 34 52 ## ## , , sex = 2 ## ## age ## tb 1 2 3 4 ## 0 7 40 62 53 ## 1 2 11 48 65 There are several possible remedies here. The first is to try to reduce the number of parameters in the model, perhaps by eliminating the interaction between sex and age class. # fit a model with sex, age (as a categorical predictor) and their interaction fm3 &lt;- glm(tb ~ length + sex + as.factor(age), family = binomial, data = boar) summary(fm3) ## ## Call: ## glm(formula = tb ~ length + sex + as.factor(age), family = binomial, ## data = boar) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.67730 1.07306 -2.495 0.0126 * ## length 0.01959 0.01237 1.584 0.1133 ## sex2 -0.24297 0.19354 -1.255 0.2093 ## as.factor(age)2 -0.19847 0.92641 -0.214 0.8304 ## as.factor(age)3 0.33908 1.06938 0.317 0.7512 ## as.factor(age)4 0.59041 1.20582 0.490 0.6244 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 681.25 on 493 degrees of freedom ## Residual deviance: 637.41 on 488 degrees of freedom ## AIC: 649.41 ## ## Number of Fisher Scoring iterations: 4 A second option is to use so-called ``exact’’ methods for inference. There doesn’t appear to be a good package available for implementing these methods in R. Other software packages might be necessary. 4.2.2 Grouped binary data: Industrial melanism We’ll start with the standard logistic regression model for the industrial melanism data. We are primarily interested in determining if the effect of color morph on removal rate changes with distance from Liverpool. For grouped binary data, we need to specify both the number of “successes” and number of “failures” as the response variable in the model. Here, we use cbind to create a two-column matrix with the number of “successes” (moths removed) in the first column, and the number of “failures” (moths not removed) in the second column. See the help documentation for glm for more details. moth &lt;- read.table(&quot;data/moth.txt&quot;, head = TRUE, stringsAsFactors = TRUE) fm1 &lt;- glm(cbind(removed, placed - removed) ~ morph * distance, family = binomial(link = &quot;logit&quot;), data = moth) summary(fm1) ## ## Call: ## glm(formula = cbind(removed, placed - removed) ~ morph * distance, ## family = binomial(link = &quot;logit&quot;), data = moth) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.128987 0.197906 -5.705 1.17e-08 *** ## morphlight 0.411257 0.274490 1.498 0.134066 ## distance 0.018502 0.005645 3.277 0.001048 ** ## morphlight:distance -0.027789 0.008085 -3.437 0.000588 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 35.385 on 13 degrees of freedom ## Residual deviance: 13.230 on 10 degrees of freedom ## AIC: 83.904 ## ## Number of Fisher Scoring iterations: 4 Grouped binary data are often overdispersed relative to the variance implied by a binomial distribution. In this case, we would call the overdispersion “extra-binomial” variation. As with count data, we can deal with overdispersion through a quasi-likelihood approach: fm1q &lt;- glm(cbind(removed, placed - removed) ~ morph * distance, family = quasibinomial(link = &quot;logit&quot;), data = moth) summary(fm1q) ## ## Call: ## glm(formula = cbind(removed, placed - removed) ~ morph * distance, ## family = quasibinomial(link = &quot;logit&quot;), data = moth) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.128987 0.223104 -5.060 0.000492 *** ## morphlight 0.411257 0.309439 1.329 0.213360 ## distance 0.018502 0.006364 2.907 0.015637 * ## morphlight:distance -0.027789 0.009115 -3.049 0.012278 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasibinomial family taken to be 1.270859) ## ## Null deviance: 35.385 on 13 degrees of freedom ## Residual deviance: 13.230 on 10 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 4 As with count data, using quasi-likelihood to estimate the scale (or dispersion) parameter increases the estimates of the standard errors of the coefficients by an amount equal to the square root of the estimated scale parameter. The \\(t\\)-test of the interaction between color morph and distance indicates that there is a statistically significant difference in how the proportion of moth removes changes over the distance transect between the two color morphs. plot(x = moth$distance, y = residuals(fm1q, type = &quot;deviance&quot;), xlab = &quot;distance&quot;, ylab = &quot;Deviance residuals&quot;) abline(h = 0, lty = &quot;dashed&quot;) The plot of the residuals suggests that we should include a random effect for the sampling station. This makes complete sense. The data for the two color morphs at each station share whatever other characteristics make the station unique, and are thus correlated. To account for this correlation, we need to introduce a random effect for the station. This again gets us into the world of generalized linear mixed models. Before proceeding, we’ll write the model down. Let \\(i=1,2\\) index the two color morphs, and let \\(j = 1, \\ldots, 7\\) index the stations. Let \\(y_{ij}\\) be the number of moths removed, let \\(n_{ij}\\) be the number of moths placed, and let \\(x_j\\) be the distance of the station from Liverpool. We wish to fit the model \\[\\begin{align*} y_{ij} &amp; \\sim \\mathrm{Binom}(p_{ij}, n_{ij})\\\\ \\mathrm{logit}(p_{ij}) &amp; = \\eta_{ij} \\\\ \\eta_{ij} &amp; = a_i + b_i x_j + L_j \\\\ L_j &amp; \\sim \\mathcal{N}(0, \\sigma^2_L) \\end{align*}\\] The \\(L_j\\)’s are our [l]ocation-specific random effects that capture any other station-to-station differences above and beyond the station’s distance from Liverpool. (It turns out that an observation-level random effect does not improve the model, at least as indicated by DIC.) Because this model includes both a random effect for the station and a non-Gaussian response, it is a generalized linear mixed model (GLMM). We postpone our discussion accordingly. 4.3 Implementation in SAS The Donner party was a well-known group of settlers who got stuck in the Sierra Nevada snows in the winter of 1846-47. The Donner party included 45 adults (defined here as individuals at least 15 years of age), of whom only 20 survived the famous winter Grayson (1993). We have a data set that includes age, gender, and fate for all 45 members of the party. The first few records of the data set are: age gender fate 23 male died 40 female survived 40 male survived 30 male died 28 male died 40 male died 45 female died ... Suppose we want to model the relationship between age, gender, and fate, where age and gender are predictors, and fate is the response. Fate is a binary response because it has only two possible values. Clearly, we can’t use the usual multiple regression model, because the residuals are sure to be severely non-normal. Our goal is to relate the predictors to the probability of one outcome or the other. The challenge in doing so is that probabilities are constrained to take values between 0 and 1, and are thus awkward to work with mathematically. Instead, we model the log odds of oue outcome for the other. To explain log odds, we first need to explain odds. uppose that the probability of an event occurring is \\(p\\). The odds of that event occurring are \\[ \\mathrm{odds} = \\dfrac{p}{1-p} \\] The logit, or log odds are simply the natural log of the odds, or \\[ \\mathrm{logit} = \\ln \\mathrm{odds} = \\ln(\\dfrac{p}{1-p}). \\] We can work backwards from log odds or odds to probabilities by inverting the formulas above. Going from log odds to odds is easy: \\[ \\mathrm{odds} = e^{\\mathrm{logit}}. \\] Going from odds to probabilities is also easy: \\[ p = \\dfrac{\\mathrm{odds}}{1 + \\mathrm{odds}} \\] Now, we can use \\[ \\mathrm{logit}(p) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots, \\] which will work just fine. In SAS, we can fit this model using PROC LOGISTIC: proc logistic descending; model fate = age male; run; Output: The LOGISTIC Procedure Response Profile Ordered Total Value fate Frequency 1 survived 20 2 died 25 Probability modeled is fate=&#39;survived&#39;. Analysis of Maximum Likelihood Estimates Standard Wald Parameter DF Estimate Error Chi-Square Pr &gt; ChiSq Intercept 1 3.2304 1.3870 5.4248 0.0199 age 1 -0.0782 0.0373 4.3988 0.0360 male 1 -1.5973 0.7555 4.4699 0.0345 Odds Ratio Estimates Point 95% Wald Effect Estimate Confidence Limits age 0.925 0.860 0.995 male 0.202 0.046 0.890 To interpret the SAS output correcty, it is critical to know whether the software is modeling the probability of an individual dying or of an individual surviving. By default, SAS models the probability of the value of the response variable for the first record in the data set. In this case, the first record in the data set is for someone who died, so SAS’s default is to model the probability of dying. There are several ways to override this default. Here, we do so by using the DESCENDING option. The parameter estimates describe a fitted regression surface on the log odds scale. For example, for individuals of a given gender, increasing age by one year decreases the log odds of survival by 0.078. We will work towards converting this estimate back to something that is easier to interpret. For now, though, recognize that log odds, odds, and probabilities all increase and decrease together, so that the sign of the estimated regression coefficient is meaningful. Here, the sign of the regression coefficient for age tells us that when comparing two people of the same gender, older people are less likely to survive. The sign of the regression coefficient for gender tells us that when comparing two people of the same age, men are less likely to survive than women. Of course, we have yet to determine if these effects are statistically significant. Example: Calculate the predicted survival probability for a 30-year old man. \\[ \\begin{align} \\mathrm{logit} &amp; = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times 30 + \\hat{\\beta}_2 \\times 1 = -0.713 \\\\ \\mathrm{odds} &amp; =e^{-0.713} = 0.49\\\\ \\mathrm{probability} &amp; = 0.49/1.49 = 0.33. \\end{align} \\] Sometimes we interpret the logistic regression parameters in terms of the odds ratio. In the logistic regression model, if we hold all other predictors constant and increase the predictor \\(x_i\\) by 1, then the predicted odds of success are multiplied by the factor \\(e^{\\beta_i}\\). This is called the odds ratio for predictor \\(i\\), and is computed automatically by the SAS output. SAS also provides 95% CIs for odds ratios. As an example, suppose we repeated the procedure above to predict the odds of survival for a 31-year old man. We would find that this man’s odds of survival is 0.45. The odds ratio, \\(0.45/0.49 = 0.92 = e^{\\hat{\\beta}_1}\\). SAS reports (estimated) standard errors for each logistic regression parameter as well as Wald \\(\\chi^2\\) tests. The Wald \\(\\chi^2\\) tests are tests of \\(H_0: \\beta_i = 0\\) vs. \\(H_a:\\beta_i \\neq 0\\). Wald \\(\\chi^2\\) tests are the analog of Type III \\(F\\) (or \\(t\\)-) tests in regression and ANOVA. In the example above, both the effects of age and gender are statistically significant at the 5% level. Do the Donner party data contain evidence that the effect of age on survival is different for men vs. women? Fit a model with an age-by-gender interaction and determine if the interaction is significant. proc logistic descending; model fate = age|male; run; Analysis of Maximum Likelihood Estimates Standard Wald Parameter DF Estimate Error Chi-Square Pr &gt; ChiSq Intercept 1 7.2450 3.2046 5.1114 0.0238 age 1 -0.1940 0.0874 4.9289 0.0264 male 1 -6.9267 3.3983 4.1546 0.0415 age*male 1 0.1616 0.0942 2.9385 0.0865 Conclusion: There is no evidence that the effect of age depends on gender (\\(\\chi^2_1 =2.94\\), \\(p = 0.0865\\)). 4.3.1 Complete separation Consider the following hypothetical data set: x y 0 fail 1 fail 2 success 3 success The best-fitting model for these data is one where the success probability is 0 for values of \\(x \\leq 1\\) and the success probability is 1 for values of \\(x \\geq 2\\). On a log odds scale, the log odds should be \\(-\\infty\\) for \\(x \\leq 1\\) and \\(+\\infty\\) for \\(x \\geq 2\\). Thus, on a log odds scale, the best-fitting line is vertical: it has infinite slope and no intercept. Thus, the intercept and slope of the logistic regression fit are impossible to estimate. This condition is known as complete separation. A similar and equally problematic condition is quasi-complete separation. Complete and quasi-complete separation are easy to detect visually when there is only a single predictor. When there are multiple predictors, (quasi-) complete separation may exist and may not be easy to detect visually. (Quasi-) complete separation is more likely when the ratio of the number of data points to the number of predictors is small. SAS will tell you when it detects (quasi-) complete separation. When it does, the Wald-tests are unreliable. Instead, exact tests must be used. The syntax for requesting an exact test in PROC LOGISTIC proc logistic exactonly; model y = x1 x2; exact x1 x2; run; Consult the on-line documentation for details and examples. Bibliography Grayson, Donald K. 1993. “Differential Mortality and the Donner Party Disaster.” Evolutionary Anthropology: Issues, News, and Reviews 2 (5): 151–59. Poole, Joyce H. 1989. “Mate Guarding, Reproductive Success and Female Choice in African Elephants.” Animal Behaviour 37: 842–49. Ramsey, Fred, and Daniel Schafer. 2012. The Statistical Sleuth: A Course in Methods of Data Analysis. Pacific Grove, CA: Duxbury. Vicente, Joaquı́n, Ursula Höfle, Joseba Garrido, Isabel G Fernández-De-Mera, Ramón Juste, Marta Barral, and Christian Gortazar. 2006. “Wild Boar and Red Deer Display High Prevalences of Tuberculosis-Like Lesions in Spain.” Veterinary Research 37 (1): 107–19. Zuur, Alain F, Elena N Ieno, Neil J Walker, Anatoly A Saveliev, Graham M Smith, et al. 2009. Mixed Effects Models and Extensions in Ecology with R. New York: Springer. "],["smoothing.html", "Chapter 5 Smoothing 5.1 Nearest-neighbor methods 5.2 Loess smoothers 5.3 Splines 5.4 Generalized additive models (GAMs)", " Chapter 5 Smoothing To this point, we have focused on models that generate a useful equation for the trend. This model equation usually contains a small number of parameters that we hope to attach to scientific phenomena of interest. In other words, we have been working in Breiman’s “data-modeling culture”. This chapter is different. Here, we focus on obtaining a curve (or its multi-dimensional equivalent) that characterizes the relationship between a predictor variable(s) and a response, but we don’t necessarily care about finding a mathematical equation to capture this curve. In other words, in this chapter we enter what Breiman describes as “algorithmic modeling”, or what is more commonly known as machine learning. The purpose of this chapter is to illustrate some machine-learning ideas ever so briefly to see how they compare with conventional statistical models. As a running example, we will consider a data set originally published by Gillibrand et al. (2007), and analyzed extensively in the textbook by Zuur et al. (2009). Zuur et al. say that these data describe the number of sources of “pelagic bioluminescence along a depth gradient in the northeast Atlantic Ocean.” We focus particulary on the data at station 16. The pattern that we wish to characterize is shown below. This chapter is in an early stage of development. ## download the data from the book&#39;s website isit &lt;- read.table(&quot;data/ISIT.txt&quot;, head = T) ## extract the data from station 16 st16 &lt;- subset(isit, Station == 16) ## retain just the variables that we want, and rename st16 &lt;- st16[, c(&quot;SampleDepth&quot;, &quot;Sources&quot;)] names(st16) &lt;- c(&quot;depth&quot;, &quot;sources&quot;) with(st16, plot(sources ~ depth)) 5.1 Nearest-neighbor methods The simplest of all possible smoothing methods is a \\(k\\)-nearest neighbor method. For this method, the value of the smooth at a target depth (\\(x\\)) is given by the average of the response (\\(y\\)) values of the \\(k\\) nearest observed depths to \\(x\\). We implement this with kknn::kknn() using kernel = \"rectangular\", which assigns equal weights to the k nearest neighbors. depth_grid &lt;- data.frame(depth = seq(min(st16$depth), max(st16$depth), length.out = 300)) fit_knn_grid &lt;- function(train_df, new_df, k) { mod &lt;- kknn::kknn( formula = sources ~ depth, train = train_df, test = new_df, k = k, kernel = &quot;rectangular&quot; ) data.frame(depth = new_df$depth, fit = fitted(mod)) } Ks_demo &lt;- c(5, 15, 31) cols &lt;- 1:length(Ks_demo) # base palette par(mfrow = c(1, 3)) for (i in seq_along(Ks_demo)) { # Base scatter with(st16, plot(depth, sources, pch = 19, cex = 0.7, xlab = &quot;Depth&quot;, ylab = &quot;Sources&quot;)) # Add k-NN smooth lines k &lt;- Ks_demo[i] pred &lt;- fit_knn_grid(st16, depth_grid, k) lines(pred$depth, pred$fit, lwd = 2, col = cols[i]) } legend(&quot;topright&quot;, legend = paste(&quot;k =&quot;, Ks_demo), col = cols, lwd = 2, bty = &quot;n&quot;) As \\(k\\) increases, the trend becomes smoother. To find the best k, we use cross-validation. Here, we evaluate a set of odd k values with 10-fold cross-validation and choose the k that minimizes out-of-fold SSE. set.seed(1) n &lt;- nrow(st16) Kfold &lt;- 10 fold_id &lt;- sample(rep(1:Kfold, length.out = n)) # Compute the minimum possible training size across folds fold_sizes &lt;- as.numeric(table(fold_id)) min_train_size &lt;- min(n - fold_sizes) # worst-case training rows # Cap k by this minimum training size (and by 61 as before), and use odd k &gt;= 3 max_k_allowed &lt;- max(3, min(61, min_train_size)) Ks &lt;- seq(3, max_k_allowed, by = 2) cv_mse &lt;- numeric(length(Ks)) names(cv_mse) &lt;- Ks for (j in seq_along(Ks)) { k &lt;- Ks[j] mse_fold &lt;- numeric(Kfold) for (f in 1:Kfold) { train_df &lt;- st16[fold_id != f, , drop = FALSE] test_df &lt;- st16[fold_id == f, , drop = FALSE] if (nrow(test_df) == 0 || nrow(train_df) &lt; k) { mse_fold[f] &lt;- NA_real_; next } mod &lt;- kknn::kknn(sources ~ depth, train = train_df, test = test_df, k = k, kernel = &quot;rectangular&quot;) mse_fold[f] &lt;- mean((test_df$sources - as.numeric(fitted(mod)))^2, na.rm = TRUE) } cv_mse[j] &lt;- mean(mse_fold, na.rm = TRUE) } best_k &lt;- as.integer(names(cv_mse)[which.min(cv_mse)]) best_k ## [1] 3 plot(as.integer(names(cv_mse)), cv_mse, type = &quot;b&quot;, xlab = &quot;Number of Neighbors (k)&quot;, ylab = &quot;Out-of-fold MSE&quot;, main = &quot;10-fold CV to Choose k&quot;) grid() abline(v = best_k, lty = 2) text(best_k, min(cv_mse, na.rm = TRUE), labels = paste(&quot;best k =&quot;, best_k), pos = 4, cex = 0.9) In this case, cross-validations suggests we should set \\(k=3\\)!. Here’s the final fit with \\(k=3\\). pred_final &lt;- fit_knn_grid(st16, depth_grid, best_k) plot(st16$depth, st16$sources, pch = 19, cex = 0.7, xlab = &quot;Depth&quot;, ylab = &quot;Sources&quot;, main = paste(&quot;k-NN Smoother with CV-selected k =&quot;, best_k)) lines(pred_final$depth, pred_final$fit, lwd = 2) 5.2 Loess smoothers A loess smoother takes the logic of \\(k\\)-nearest neighbor fitting one step further. Now, instead of simply averaging the \\(k\\) nearest neighbors, we fit a regression model to the nearest neighbors, and use the predicted value of the regression trend as our smooth. “Loess” is an acronym for [lo]cal regr[ess]ion. Nomenclature can be a bit frustrating with loess models. As we will see later, some versions of loess models use weighted least squares instead of ordinary least squares, and are called “lowess” models to emphasize the use of weighted least squares. However, the basic R routine for fitting lo(w)ess models is called loess, but uses the weighted least-squares fitting with its default factory settings. Fit a loess smoother using the factory settings: st16.lo &lt;- loess(sources ~ depth, data = st16) summary(st16.lo) ## Call: ## loess(formula = sources ~ depth, data = st16) ## ## Number of Observations: 51 ## Equivalent Number of Parameters: 4.33 ## Residual Standard Error: 4.18 ## Trace of smoother matrix: 4.73 (exact) ## ## Control settings: ## span : 0.75 ## degree : 2 ## family : gaussian ## surface : interpolate cell = 0.2 ## normalize: TRUE ## parametric: FALSE ## drop.square: FALSE Plot the fit, this takes a little work depth.vals &lt;- with(st16, seq(from = min(depth), to = max(depth), length = 100)) st16.fit &lt;- predict(object = st16.lo, newdata = depth.vals, se = TRUE) with(st16, plot(sources ~ depth)) lines(x = depth.vals, y = st16.fit$fit, col = &quot;blue&quot;) # add 95% error bars lines(x = depth.vals, y = st16.fit$fit + st16.fit$se.fit * qt(p = .975, df = st16.fit$df), col = &quot;blue&quot;, lty = &quot;dashed&quot;) lines(x = depth.vals, y = st16.fit$fit - st16.fit$se.fit * qt(p = .975, df = st16.fit$df), col = &quot;blue&quot;, lty = &quot;dashed&quot;) Examine the residuals: ## see what the fit returns; maybe the residuals are already there names(st16.lo) # they are! ## [1] &quot;n&quot; &quot;fitted&quot; &quot;residuals&quot; &quot;enp&quot; &quot;s&quot; &quot;one.delta&quot; ## [7] &quot;two.delta&quot; &quot;trace.hat&quot; &quot;divisor&quot; &quot;robust&quot; &quot;pars&quot; &quot;kd&quot; ## [13] &quot;call&quot; &quot;terms&quot; &quot;xnames&quot; &quot;x&quot; &quot;y&quot; &quot;weights&quot; plot(st16.lo$residuals ~ st16$depth) abline(h = 0, lty = &quot;dotted&quot;) Let’s look at how changing the span changes the fit. We’ll write a custom function to fit a LOESS curve, and then call the function with various values for the span. PlotLoessFit &lt;- function(x, y, return.fit = FALSE, ...){ # Caluclates a loess fit with the &#39;loess&#39; function, and makes a plot # # Args: # x: predictor # y: response # return.fit: logical # ...: Optional arguments to loess # # Returns: # the loess fit my.lo &lt;- loess(y ~ x, ...) x.vals &lt;- seq(from = min(x), to = max(x), length = 100) my.fit &lt;- predict(object = my.lo, newdata = x.vals, se = TRUE) plot(x, y) lines(x = x.vals, y = my.fit$fit, col = &quot;blue&quot;) lines(x = x.vals, y = my.fit$fit + my.fit$se.fit * qt(p = .975, df = my.fit$df), col = &quot;blue&quot;, lty = &quot;dashed&quot;) lines(x = x.vals, y = my.fit$fit - my.fit$se.fit * qt(p = .975, df = my.fit$df), col = &quot;blue&quot;, lty = &quot;dashed&quot;) if (return.fit) { return(my.lo) } } Now we’ll call the function several times, each time chanigng the value of the span argument to the loess function: PlotLoessFit(x = st16$depth, y = st16$sources, span = 0.5) PlotLoessFit(x = st16$depth, y = st16$sources, span = 0.25) PlotLoessFit(x = st16$depth, y = st16$sources, span = 0.1) Let’s try a loess fit with a locally linear regression: PlotLoessFit(x = st16$depth, y = st16$sources, span = 0.25, degree = 1) 5.3 Splines We’ll use the gam function in the mgcv package to fit splines and additive models. The name of the package is an acronym for “Mixed GAM Computation Vehicle”. GAM is an acronym for Generalized Additive Model. Warning. I do not understand much of the functionality of mgcv::gam. What follows is my best guess of how the procedure works. The code below fits a regression spline to the bioluminescence data. Actually, the code fits an additive model with the spline as the only predictor. We will say more about additive models later. For now, it is sufficient to think about an additive model as a type of regression in which the linear effect of the predictor has been replaced by a spline. In other words, in terms of a word equation, the model can be represented as \\[ \\mbox{response = intercept + spline + error} \\] The s() component of the model formula designates a spline, and specifies details about the particular type of spline to be fit. The fx = TRUE component of the formula indicates that the amount of smoothing is fixed. The default value for the fx argument is fx = FALSE, in which case the amount of smoothing is determined by (generalized) cross-validation. When fx = TRUE, the parameter k determines the dimensionality (degree of flexibility) of the spline. Larger values of k correspond to greater flexibility, and a less smooth fit. I think that the number of knots is \\(k-4\\), such that setting \\(k=4\\) fits a familiar cubic polynomial with no knots. Setting \\(k=5\\) then fits a regression spline with one knot, etc. I have not been able to figure out where the knots are placed. In any case, we’ll fit a regression spline with two knots: library(mgcv) ## Loading required package: nlme ## This is mgcv 1.9-3. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. st16.rspline &lt;- mgcv::gam(sources ~ s(depth, k = 6, fx = TRUE), data = st16) plot(st16.rspline, se = TRUE) Note that the plot includes only the portion of the model attributable to the covariate effect. This is because we have actually fit an additive model (e.g., a GAM). The plot shows only the spline component, which thus does not include the intercept. To visualize the fit, we’ll need to do a bit more work. with(st16, plot(sources ~ depth)) st16.fit &lt;- predict(st16.rspline, newdata = data.frame(depth = depth.vals), se = TRUE) lines(x = depth.vals, y = st16.fit$fit) ## add +/- 2 SE following Zuur; this is only approximate. ## should probably use a critical value from a t-dist with n - edf df, that is, 51 - 5 = 46 df lines(x = depth.vals, y = st16.fit$fit + 2 * st16.fit$se.fit, lty = &quot;dashed&quot;) lines(x = depth.vals, y = st16.fit$fit - 2 * st16.fit$se.fit, lty = &quot;dashed&quot;) We see that this particular fit is not flexible enough to capture the trend in luminescence at low depth. Let’s take a look at the information produced by a call to summary: summary(st16.rspline) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## sources ~ s(depth, k = 6, fx = TRUE) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.4771 0.5858 21.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(depth) 5 5 122.6 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.924 Deviance explained = 93.2% ## GCV = 19.837 Scale est. = 17.503 n = 51 This summary requires a bit more explanation as well. In this GAM, the spline component of the model effectively creates a set of new predictor variables. A regression spline with \\(x\\) knots requires \\(x+3\\) new regression predictors to fit the spline. In this fit, there are two knots, so the spline requires 5 new predictor variables. Because the predictors are determined in advance with regression splines, we can use the usual theory of \\(F\\)-tests from regression to assess the statistical significance of the spline terms. In the section of the output labeled “Approximate significance of smooth terms”, we see that these 5 predictors together provide a significantly better fit than a model that does not include the spline. I believe this test is actually exact. I think that it is labeled “approximate” because the default behavior of mgcv::gam is to fit a smoothing spline, for which the test is indeed only approximate. We’ll discuss this more when we study a smoothing spline fit. Now we’ll fit and plot a smoothing spline. A smoothing spline differs from a regression spline by using generalized cross-validation to determine the appropriate smoothness. st16.spline &lt;- mgcv::gam(sources ~ s(depth), data = st16) plot(st16.spline, se = TRUE) # note that the plot does not include the intercept Again, we make a plot that includes both the points and the fit with(st16, plot(sources ~ depth)) st16.fit &lt;- predict(st16.spline, newdata = data.frame(depth = depth.vals), se = TRUE) lines(x = depth.vals, y = st16.fit$fit) ## add +/- 2 SE following Zuur; this is only approximate. ## should probably use a critical value from a t-dist with n - edf df, that is, 51 - 9.81 = 41.19 df lines(x = depth.vals, y = st16.fit$fit + 2 * st16.fit$se.fit, lty = &quot;dashed&quot;) lines(x = depth.vals, y = st16.fit$fit - 2 * st16.fit$se.fit, lty = &quot;dashed&quot;) Let’s ask for a summary: summary(st16.spline) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## sources ~ s(depth) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.4771 0.3921 31.82 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(depth) 8.813 8.99 158.2 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.966 Deviance explained = 97.2% ## GCV = 9.7081 Scale est. = 7.8402 n = 51 Note especially the edf component in the “Approximate significance of smooth terms” section. The label edf stands for effective degrees of freedom. We can think of the edf as the effective number of new predictors that have been added to the model to accommodate the spline. For a smoothing spline, the number and values of the newly created predictors are determined by fitting the model to the data. Because the predictors are calculated in this way, the usual theory of \\(F\\)-testing does not apply. This is why the \\(F\\)-test shown for the smoothing spline is labeled as “approximate”. Find the AIC for the smoothing spline fit: AIC(st16.spline) ## [1] 260.4811 Here’s a small detail. Notice that the syntax of the call to predict is slightly different when making a prediction for a loess object vs. making a prediction for a gam object (which the spline fit is). For a call to predict with a loess object, the new predictor values can be provided in the form of a vector. So, we were able to use depth.vals &lt;- with(st16, seq(from = min(depth), to = max(depth), length = 100)) st16.fit &lt;- predict(object = st16.lo, newdata = depth.vals, se = TRUE) However, for a call to predict with a gam object, the new predictor values must be provided in the form of a new data frame, with variable names that match the variables in the gam model. So, to get predicted values for the spline fit, we needed to use the more cumbersome depth.vals &lt;- with(st16, seq(from = min(depth), to = max(depth), length = 100)) st16.fit &lt;- predict(st16.spline, newdata = data.frame(depth = depth.vals), se = TRUE) 5.4 Generalized additive models (GAMs) Generalized additive models replace the usual linear terms that appear in multiple regression models with splines. That is, suppose we seek to model the relationship between a response \\(y\\) and two predictors, \\(x_1\\) and \\(x_2\\). A standard regression model without polynomial effects or interactions would be written as \\[ y = \\beta_0 + \\beta_1 x_1 +\\beta_2 x_2 + \\varepsilon \\] where \\(\\varepsilon\\) is assumed to be an iid Gaussian random variate with variance \\(\\sigma^2_\\varepsilon\\). This is an additive model, in the sense that the combined effects of the two predictors equal the sum of their individual effects. A generalized additive model (GAM) replaces the individual regression terms with splines. Continuing with the generic example, a GAM would instead model the effects of the two predictors as \\[ y = \\beta_0 + s(x_1) +s(x_2) + \\varepsilon \\] where \\(s(\\cdot)\\) represents a spline. We continue to assume that, conditional on the covariate effects, the responses are normally distributed with constant variance \\(\\sigma^2_\\varepsilon\\). We will illustrate additive modeling using the bird data found in Appendix A of Zuur et al. (2009). Zuur et al. report that these data originally appeared in Loyn (1987) and were featured in Quinn &amp; Keough (2002)’s text. Zuur et al. describe these data in the following way: Forest bird densities were measured in 56 forest patches in south-eastern Victoria, Australia. The aim of the study was to relate bird densities to six habitat variables; size of the forest patch, distance to the nearest patch, distance to the nearest larger patch, mean altitude of the patch, year of isolation by clearing, and an index of stock grazing history (1 = light, 5 = intensive). We first read the data and perform some light exploratory analysis and housekeeping. rm(list = ls()) require(mgcv) bird &lt;- read.table(&quot;data/Loyn.txt&quot;, head = T) summary(bird) ## Site ABUND AREA DIST ## Min. : 1.00 Min. : 1.50 Min. : 0.10 Min. : 26.0 ## 1st Qu.:14.75 1st Qu.:12.40 1st Qu.: 2.00 1st Qu.: 93.0 ## Median :28.50 Median :21.05 Median : 7.50 Median : 234.0 ## Mean :28.50 Mean :19.51 Mean : 69.27 Mean : 240.4 ## 3rd Qu.:42.25 3rd Qu.:28.30 3rd Qu.: 29.75 3rd Qu.: 333.2 ## Max. :56.00 Max. :39.60 Max. :1771.00 Max. :1427.0 ## LDIST YR.ISOL GRAZE ALT ## Min. : 26.0 Min. :1890 Min. :1.000 Min. : 60.0 ## 1st Qu.: 158.2 1st Qu.:1928 1st Qu.:2.000 1st Qu.:120.0 ## Median : 338.5 Median :1962 Median :3.000 Median :140.0 ## Mean : 733.3 Mean :1950 Mean :2.982 Mean :146.2 ## 3rd Qu.: 913.8 3rd Qu.:1966 3rd Qu.:4.000 3rd Qu.:182.5 ## Max. :4426.0 Max. :1976 Max. :5.000 Max. :260.0 # get rid of the &#39;Site&#39; variable; it is redundant with the row label bird &lt;- bird[, -1] # log-transform area, distance, ldistance, to remove right-skew bird$L.AREA &lt;- log(bird$AREA) bird$L.DIST &lt;- log(bird$DIST) bird$L.LDIST &lt;- log(bird$LDIST) # change YR.ISOL to years since isolation (study was published in 1987) bird$YR.ISOL &lt;- 1987 - bird$YR.ISOL # keep the only the variables we want bird &lt;- bird[, c(&quot;ABUND&quot;, &quot;L.AREA&quot;, &quot;L.DIST&quot;, &quot;L.LDIST&quot;, &quot;YR.ISOL&quot;, &quot;ALT&quot;, &quot;GRAZE&quot;)] summary(bird) ## ABUND L.AREA L.DIST L.LDIST ## Min. : 1.50 Min. :-2.3026 Min. :3.258 Min. :3.258 ## 1st Qu.:12.40 1st Qu.: 0.6931 1st Qu.:4.533 1st Qu.:5.064 ## Median :21.05 Median : 2.0127 Median :5.455 Median :5.824 ## Mean :19.51 Mean : 2.1459 Mean :5.102 Mean :5.859 ## 3rd Qu.:28.30 3rd Qu.: 3.3919 3rd Qu.:5.809 3rd Qu.:6.816 ## Max. :39.60 Max. : 7.4793 Max. :7.263 Max. :8.395 ## YR.ISOL ALT GRAZE ## Min. :11.00 Min. : 60.0 Min. :1.000 ## 1st Qu.:21.00 1st Qu.:120.0 1st Qu.:2.000 ## Median :24.50 Median :140.0 Median :3.000 ## Mean :37.25 Mean :146.2 Mean :2.982 ## 3rd Qu.:59.50 3rd Qu.:182.5 3rd Qu.:4.000 ## Max. :97.00 Max. :260.0 Max. :5.000 Our first attempt at a GAM will entertain smoothing splines for all of the continuous predictors in the model. We will use a linear term for GRAZE because there are too few unique values to support a smooth term: bird.gam1 &lt;- mgcv::gam(ABUND ~ s(L.AREA) + s(L.DIST) + s(L.LDIST) + s(YR.ISOL) + GRAZE + s(ALT), data = bird) summary(bird.gam1) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## ABUND ~ s(L.AREA) + s(L.DIST) + s(L.LDIST) + s(YR.ISOL) + GRAZE + ## s(ALT) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.4443 2.7798 9.153 9.42e-12 *** ## GRAZE -1.9885 0.8968 -2.217 0.0318 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(L.AREA) 2.446 3.089 12.635 3.98e-06 *** ## s(L.DIST) 3.693 4.559 0.855 0.461 ## s(L.LDIST) 1.000 1.000 0.386 0.538 ## s(YR.ISOL) 1.814 2.238 1.231 0.262 ## s(ALT) 1.000 1.000 0.629 0.432 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.72 Deviance explained = 77.6% ## GCV = 40.987 Scale est. = 32.238 n = 56 The output reports the partial regression coefficient for the lone quantitative predictor GRAZE, and approximate significance tests for the smooth terms for each of the other predictors. We can visualize these smooth terms with a call to plot: plot(bird.gam1) In the interest of time, we take a casual approach to variable selection here. We’ll drop smooth terms that are clearly not significant to obtain: bird.gam2 &lt;- mgcv::gam(ABUND ~ s(L.AREA) + GRAZE, data = bird) summary(bird.gam2) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## ABUND ~ s(L.AREA) + GRAZE ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 28.400 2.201 12.903 &lt; 2e-16 *** ## GRAZE -2.980 0.686 -4.344 6.56e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(L.AREA) 2.284 2.903 13.18 3.4e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.68 Deviance explained = 69.9% ## GCV = 39.992 Scale est. = 36.932 n = 56 plot(bird.gam2) Note that the GRAZE variable is currently treated as a numerical predictor. We’ll try fitting a model with GRAZE as a factor. First we’ll create a new variable that treats GRAZE as a factor. We’ll use the summary command to confirm that the new variable fGRAZE is indeed a factor. bird$fGRAZE &lt;- as.factor(bird$GRAZE) summary(bird) ## ABUND L.AREA L.DIST L.LDIST ## Min. : 1.50 Min. :-2.3026 Min. :3.258 Min. :3.258 ## 1st Qu.:12.40 1st Qu.: 0.6931 1st Qu.:4.533 1st Qu.:5.064 ## Median :21.05 Median : 2.0127 Median :5.455 Median :5.824 ## Mean :19.51 Mean : 2.1459 Mean :5.102 Mean :5.859 ## 3rd Qu.:28.30 3rd Qu.: 3.3919 3rd Qu.:5.809 3rd Qu.:6.816 ## Max. :39.60 Max. : 7.4793 Max. :7.263 Max. :8.395 ## YR.ISOL ALT GRAZE fGRAZE ## Min. :11.00 Min. : 60.0 Min. :1.000 1:13 ## 1st Qu.:21.00 1st Qu.:120.0 1st Qu.:2.000 2: 8 ## Median :24.50 Median :140.0 Median :3.000 3:15 ## Mean :37.25 Mean :146.2 Mean :2.982 4: 7 ## 3rd Qu.:59.50 3rd Qu.:182.5 3rd Qu.:4.000 5:13 ## Max. :97.00 Max. :260.0 Max. :5.000 Now we’ll proceed to fit the model bird.gam3 &lt;- gam(ABUND ~ s(L.AREA) + fGRAZE, data = bird) plot(bird.gam3) summary(bird.gam3) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## ABUND ~ s(L.AREA) + fGRAZE ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22.727275 1.944080 11.691 1.11e-15 *** ## fGRAZE2 0.006623 2.845343 0.002 0.998152 ## fGRAZE3 -0.660124 2.585878 -0.255 0.799592 ## fGRAZE4 -2.170994 3.050736 -0.712 0.480122 ## fGRAZE5 -11.913966 2.872911 -4.147 0.000136 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(L.AREA) 2.761 3.478 11.67 4.71e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.723 Deviance explained = 75.7% ## GCV = 37.013 Scale est. = 31.883 n = 56 To formally compare the models with GRAZE as a numerical vs. categorical predictor, we’ll have to use AIC. We can’t use an \\(F\\)-test here because we have used smoothing splines to capture the effect of L.AREA. Thus, the models are not nested. (If we had used regression splines for L.AREA, then the models would have been nested.) We can extract the AICs for these models by a simple call to the AIC function. AIC(bird.gam2) ## [1] 367.1413 AIC(bird.gam3) ## [1] 361.9655 We can see the contrasts used to incorporate the factor fGRAZE in the model by a call to contrasts: with(bird, contrasts(fGRAZE)) ## 2 3 4 5 ## 1 0 0 0 0 ## 2 1 0 0 0 ## 3 0 1 0 0 ## 4 0 0 1 0 ## 5 0 0 0 1 The output here is somewhat opaque because the levels of fGRAZE are 1, 2, \\(\\ldots\\), 5. The output of the call to contrasts shows each of the newly created indicator variables as a column. For example, the first column shows that the predictor named fGRAZE2 takes the value of 1 when the variable fGRAZE equals 2, and is 0 otherwise. Fit an additive model with only a smooth effect of L.AREA, in order to show residuals vs. GRAZE: bird.gam4 &lt;- gam(ABUND ~ s(L.AREA), data = bird) plot(x = bird$GRAZE, y = bird.gam4$residuals) abline(h = 0, lty = &quot;dashed&quot;) Both the plot and the model output suggest that the effect of grazing is primarily due to lower bird abundance in the most heavily grazed category. To conclude, we’ll conduct a formal test of whether the model with GRAZE as a factor provides a significantly better fit than the model with a linear effect of GRAZE. In this case, we have to use regression splines for the smooth effect of L.AREA. We’ll use regression “splines” without any internal knots, (which are actually not splines at all, just a cubic trend) because the effect of log area seems to be reasonably well captured by a cubic trend anyway: bird.gam5 &lt;- gam(ABUND ~ s(L.AREA, k = 4, fx = TRUE) + GRAZE, data = bird) bird.gam6 &lt;- gam(ABUND ~ s(L.AREA, k = 4, fx = TRUE) + fGRAZE, data = bird) anova(bird.gam5, bird.gam6, test = &quot;F&quot;) ## Analysis of Deviance Table ## ## Model 1: ABUND ~ s(L.AREA, k = 4, fx = TRUE) + GRAZE ## Model 2: ABUND ~ s(L.AREA, k = 4, fx = TRUE) + fGRAZE ## Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) ## 1 51 1869.0 ## 2 48 1543.1 3 325.93 3.3796 0.02565 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Both AIC and the \\(F\\)-test suggest that the model with GRAZE as a factor provides a significantly better fit than the model with a linear effect of GRAZE (\\(F_{3,48} = 3.38, p = 0.026\\)). As a final note, Zuur et al. (p.550) observe that “the non-linear L.AREA effect is mainly due to two large patches. It would be useful to sample more of this type of patch in the future.” (Note the rug plots in any of the plots of the area effect above.) Bibliography Gillibrand, EJV, P Bagley, A Jamieson, PJ Herring, JC Partridge, MA Collins, R Milne, and Imants George Priede. 2007. “Deep Sea Benthic Bioluminescence at Artificial Food Falls, 1,000–4,800 m Depth, in the Porcupine Seabight and Abyssal Plain, North East Atlantic Ocean.” Marine Biology 150 (6): 1053–60. Zuur, Alain F, Elena N Ieno, Neil J Walker, Anatoly A Saveliev, Graham M Smith, et al. 2009. Mixed Effects Models and Extensions in Ecology with R. New York: Springer. "],["one-way.html", "Chapter 6 One-factor layout 6.1 Grouped data and the design of experiments (DoE): an overview 6.2 One-factor ANOVA 6.3 Contrasts of group means 6.4 Multiple comparisons 6.5 \\(^\\star\\)Power and sample-size determination in ANOVA 6.6 Using SAS: The effects parameterization of the one-factor ANOVA", " Chapter 6 One-factor layout 6.1 Grouped data and the design of experiments (DoE): an overview In this part of the notes, we study methods to compare data from several groups. Most often, data from several groups arise from a designed experiment in which the groups are different experimental treatments that we wish to compare. The statistical methods extend, however, to observational studies in which the groups that we compare are different populations that already exist in nature. As with regression models, the statistical methods for comparing groups are identical regardless of whether the data are generated by an experiment or an observational study. What differs between experimental or observational studies is how we interpret the results. Again, as with regression studies, designed experiments support a causal interpretation of the differences among groups, whereas observational studies only support a comparative interpretation. 6.1.1 A vocabulary for describing designed experiments Here we present a scheme and a vocabulary for describing designed experiments. We will see that our scheme aligns closely with the statsitical methods that we use to analyze the resulting data. In other words, if we can identify the experimental design, then the design will often suggest the appropriate statistical analysis. There are two basic elements of a designed experiment. The treatment structure of an experiment describes how the different experimental treatments are constructed. The randomization structure of an experiment describes how those experimental treatments are assigned among different units. It is also useful to distinguish the entity to which the treatment is assigned from the entity that is measured. The experimental unit (EU) is the physical entity to which a treatment is assigned, while the measurement unit (MU) is the entity that is measured. The measurement unit is often the same as the EU, but not always. The experimental error is the variation in the response among EUs assigned to the same treatment. The treatment and randomization structures of an experiment align tightly with the different components of a statistical model. The fixed effects in a statistical model describe how the average response differs among the treatment groups. The random effects in a statistical model describe how the replicate experimental units assigned to the same treatment vary. (We will have more to say about the distinction between fixed vs. random effects later. In short, the treatment structure of an experiment determines how the fixed effects should be specified, and the randomization structure determines how the random effects should be specified. 6.1.2 Roadmap We will begin our study of designed experiments by exploring the treatment structure of an experiment. In this chapter and the next, we will study various types of treatment structures, beginning with the simplest possible treatment structure and progressing to more complex structures. While we study different treatment structures, we will keep the randomization structure simple. Thus, with regard to the statistical models, we will initially focus on the fixed-effects component of the model. After studying different treatment structures, we will then pivot to studying different randomization structures. Accordingly, we will also turn our attention to studying the random-effects component of the statistical model. Finally, we note that all of our study of designed experiments assumes that the response is normally distributed. The great advantage of this assumption is that, because of the special properties of a normal distribution, the fixed-effect and random-effects components of the statistical model can be separated into two parts that are added together to construct the full model. After having completed our study, we will then study models in which the response has something other than a normal distribution. Things get more complicated here, because without our assumption of normality, we can’t simply break the model apart into its fixed- and random-effect components. The two parts become more tightly intertwined in complicated ways. 6.1.3 The simplest experiment The simplest type of treatment structure is a one-factor layout, also known as a one-factor classification or one-way layout or single-factor design. A one-factor layout consists of a single experimental factor with 2 or more levels. Here is an example from Moore and McCabe (1989). A researcher is interested in comparing three different methods of teaching reading on students’ reading comprehension. The methods are labeled “Basal”, “DRTA’, and”Strat”. Let \\(\\mu_B\\), \\(\\mu_D\\), and \\(\\mu_S\\) denote the average student comprehension for each of the three treatments. We are interested in comparing \\(\\mu_B\\), \\(\\mu_D\\), and \\(\\mu_S\\). As a bit of bookkeeping, it is usually awkward to distinguish statistical parameters by letter subscripts. It is usually convenient to recode the parameters by numerical subscripts. In this case, we might code the three mean responses as \\(\\mu_1\\), \\(\\mu_2\\), and \\(\\mu_3\\), referring to the average response for the ” Basal”, “DRTA’, and”Strat” groups, respectively. The simplest type of randomization structure is a completely randomized design (CRD). The term “completely randomized design” virtually defines itself: in a CRD, treatments are assigned to EUs completely at random. CRDs are appropriate when EUs do not differ in ways that are known to affect the response. When the EUs do differ in ways known to affect the response, there are other randomization structures that are preferable to a CRD. A balanced CRD is each treatment is assigned to the same number of replicates. In the reading example, each of the three methods was randomly assigned to 22 different students. One of the measures of reading comprehension in the data set is called “POST3”. The data for this response are shown in the strip chart below. reading &lt;- read.table(&quot;data/reading.txt&quot;, head = T, stringsAsFactors = T) with(reading, stripchart(POST3 ~ Group, method = &quot;jitter&quot;, pch = 16, las = 1)) In the stripchart above, only left-to-right variation is meaningful. Vertical variation within each of the treatment groups is random scatter added to make sure the data points do not lie directly on top of one another. The statistical model that we use for analyzing a one-factor layout with a CRD is a one-factor analysis of variance (ANOVA), which we study next. 6.2 One-factor ANOVA 6.2.1 \\(F\\)-test to compare means To introduce one-factor ANOVA, we will use a data set from an observational study. These data provide the calorie content for 20 randomly selected beef hotdogs, 17 randomly selected poultry hotdogs, and 17 randomly selected meat (!) hotdogs. The data are shown in the stripchart below. Figure 6.1: Strip chart of calorie content for hot dog data. Let \\(\\mu_B\\), \\(\\mu_M\\) and \\(\\mu_P\\) denote the average calorie content for beef, meat and poultry hotdogs, respectively. We wish to characterize if and how these means differ. To do so, we will first test the null hypothesis \\(H_0\\): \\(\\mu_B = \\mu_M = \\mu_P\\) with a one-factor analysis of variance (ANOVA). Before we begin, we need to develop some notation. Let \\(g\\) denote the number of groups to be compared. In the hot dog example, \\(g = 3\\). Let \\(i = 1,\\ldots,g\\) be an index that distinguishes the different populations. The size of the random sample drawn from population \\(i\\) is written \\(n_i\\). When the sample sizes are the same in every group, we say that the data are balanced, and sometimes replace the individual sample sizes \\(n_i\\) by a common sample size \\(n\\). Let \\(n_T = \\sum_{i = 1}^g n_i\\) denote the total number of data points in the data set. Let \\(j=1, \\ldots,n_i\\) be an index that distinguishes the different data points within each sample; that is, \\(y_{ij}\\) is observation \\(j\\) from population \\(i\\). Finally, let \\(\\mu_i\\) denote the population mean for group \\(i\\). We will also use the subscript “+” to indicate summation over the values of an index. For example, if we wanted to add together all the data points from the sample from group \\(i\\), we could write \\[ y_{i+} = \\sum_{j=1}^{n_{ij}} y_{ij} \\] Or, if we wanted to add up all the data points in the data set, we could write \\[ y_{++} = \\sum_{i=1}^g \\sum_{j=1}^{n_{ij}} y_{ij} \\] Lastly, we use bars to denote sample averages. The sample mean from population \\(i\\) is written \\[ \\bar{y}_{i+} = \\frac{1}{n_i}\\sum_{j=1}^{n_{ij}} y_{ij} \\] and the “grand mean” is written \\[ \\bar{y}_{++} = \\frac{1}{n_T}\\sum_{i=1}^g \\sum_{j=1}^{n_{ij}} y_{ij}. \\] A word about subscripting: As we progress, the subscripting that we use will become increasingly complicated. Remember that the basic rule for selecting a subscripting scheme is that each unique combination of subscripts must identify a unique data point. In regression, one subscript was sufficient (i.e., \\(i = 1, \\ldots, n\\)), because the value of \\(i\\) was sufficient to specify a unique data point. In ANOVA, we need one subscript to distinguish the different groups, and a second subscript to distinguish the individual observations within each group. The first order of business in an ANOVA is testing the null hypothesis that all the group means are equal, \\(H_0\\): \\(\\mu_1 = \\mu_2 = \\ldots = \\mu_g\\), vs. the alternative that at least two group means differ. As we will see below, this is identical to a model utility test in regression with indicators, and so we shouldn’t be surprised that this is an \\(F\\)-test. In the context of ANOVA, however, it is traditional to represent this test via a sums-of-squares decomposition. This decomposition leads to computing formulas that were important in the era before desktop computing. We begin by partitioning the variation in the data into two pieces: one quantifying the variation among populations and a second quantifying variation within populations.27 \\[ \\mbox{Total variation: } SS_{Total} = \\sum_{i=1}^g \\sum_{j=1}^{n_{ij}} \\left( y_{ij} - \\bar{y}_{++} \\right)^2 \\] \\[ \\mbox{Variation among groups: } SS_{Groups} = \\sum_{i=1}^g \\sum_{j=1}^{n_{ij}} \\left( \\bar{y}_{i+} - \\bar{y}_{++} \\right)^2 = \\sum_{i=1}^g n_i \\left( \\bar{y}_{i+} - \\bar{y}_{++} \\right)^2 \\] \\[ \\mbox{Variation within groups: } SS_{Error} = \\sum_{i=1}^g \\sum_{j=1}^{n_{ij}} \\left( y_{ij} - \\bar{y}_{i+} \\right)^2 \\] Although it is not obvious, the \\(SS_{Groups}\\) and \\(SS_{Error}\\) add together to give \\(SS_{Total}\\), that is, \\[ SS_{Total}= SS_{Groups} + SS_{Error} \\] Heuristically, we want to compare the variation among groups to the variation within groups. However, we cannot compare the \\(SS_{Groups}\\) to the \\(SS_{Error}\\) directly, because these sums of squares are based on a different number of free differences, or degrees of freedom. Thus, we must standardize each sum of squares by dividing through by the number of free differences on which the sum of squares is based. We divide both \\(SS_{Groups}\\) and \\(SS_{Error}\\) by their respective df to obtain the corresponding mean squares. Mean squares can be directly compared, and the ratio of the \\(MS_{Groups}\\) to the \\(MS_{Error}\\) yields our \\(F\\)-statistic: \\[ F = \\frac{MS_{Groups}}{MS_{Error}} = \\frac{SS_{Groups} / (g - 1)}{SS_{Error} / (n_T - g)}. \\] Mathematically, it can be shown that if the null hypothesis is true, then \\(MS_{Groups} \\approx MS_{Error}\\), and so \\(F \\approx 1\\). If the null is false and the alternative is true, then \\(MS_{Groups} &gt; MS_{Error}\\), and so \\(F &gt; 1\\). Of course, both mean squares have some inherent randomness, so we measure the degree of evidence against the null by comparing the \\(F\\) statistic to the appropriate reference distribution. If the null is true, then the \\(F\\) ratio has an \\(F\\)-distribution with numerator df equal to \\(g - 1\\) and denominator df equal to \\(n_T - g\\). Large values of the \\(F\\) statistic provide evidence against the null and in favor of the alternative, and so we compute the \\(p\\)-value with a one-tailed test. It is customary to package all the information that goes into an \\(F\\)-test into an ANOVA table. Although the layout of this table may differ slightly from one text to the next, the basic pieces are all the same. source df SS MS \\(F\\) \\(p\\) Groups \\(g-1\\) \\(SS_{Groups}\\) \\(MS_{Groups}\\) \\(MS_{Groups} / MS_{Error}\\) Error \\(n_T-g\\) \\(SS_{Error}\\) \\(MS_{Error}\\) Total \\(n_T-1\\) \\(SS_{Total}\\) Let’s compare ANOVA tables for the hot-dog calorie data from both R and SAS. Note that although the formatting differs slightly between the two software packages, the components of the \\(F\\)-test are identical. In R, the ANOVA table is produced by fitting a linear model using lm(), and then passing that model fit to the command anova(). fm1 &lt;- lm(calories ~ type, data = hotdog) anova(fm1) ## Analysis of Variance Table ## ## Response: calories ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## type 2 17692 8846.1 16.074 3.862e-06 *** ## Residuals 51 28067 550.3 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 For the sake of these notes, we can make the ANOVA table look a bit nicer using the following code. options(knitr.kable.NA = &#39;&#39;) anova(fm1) %&gt;% knitr::kable(format = &quot;html&quot;) Df Sum Sq Mean Sq F value Pr(&gt;F) type 2 17692.20 8846.098 16.07399 3.9e-06 Residuals 51 28067.14 550.336 A PROC GLM program to fit a one-factor ANOVA model to these data is shown below, followed by some curated output. data hotdog; infile &quot;data\\hotdog.txt&quot; firstobs = 2; input type$ calorie sodium; run; proc glm data = hotdog; class type; /* declare categorical variables here */ model calorie = type; run; The GLM Procedure Sum of Source DF Squares Mean Square F Value Pr &gt; F Model 2 17692.19510 8846.09755 16.07 &lt;.0001 Error 51 28067.13824 550.33604 Corrected Total 53 45759.33333 Thankfully, we get the same output regardless of which computer package we use. We interpret this output in the following way: There is strong evidence that the average calorie content of meat, beef, and poultry hot dogs are not all equal (\\(F_{2,51}=16.07\\), \\(p&lt;.001\\)). Example 2. We also have sodium content (in mg) available for each hotdog. A strip-chart of the sodium data is shown below. with(hotdog, stripchart(sodium ~ type, method = &quot;jitter&quot;, pch = 16)) fm2 &lt;- lm(sodium ~ type, data = hotdog) anova(fm2) ## Analysis of Variance Table ## ## Response: sodium ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## type 2 31739 15869.4 1.7778 0.1793 ## Residuals 51 455249 8926.4 There is no evidence that the average sodium content of meat, beef, and poultry hot dogs differ (\\(F_{2,51}=1.78\\), \\(p=0.18\\)). 6.2.2 Connections between one-factor ANOVA and other statistical procedures It may have occurred to you that we could also have analyzed the hot dog data by using a regression model with indicator variables. In fact, ANOVA and regression with indicators are the same statistical model, a point which we will illustrate more explicitly now. For the hot-dog data, let’s consider the regression model \\[ y=\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 + \\epsilon \\] where \\(x_1\\) and \\(x_2\\) are indicator variables to code for the different types of hotdogs. We can ask if the data contain evidence that the different types of hotdogs have different calorie contents on average by testing \\(H_0\\): \\(\\beta_1 = \\beta_2 = 0\\). In this particular case, the \\(F\\)-test that we need is the model utility test. We’ll just find this test directly in the R output. summary(fm1) ## ## Call: ## lm(formula = calories ~ type, data = hotdog) ## ## Residuals: ## Min 1Q Median 3Q Max ## -51.706 -18.492 -5.278 22.500 36.294 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 156.850 5.246 29.901 &lt; 2e-16 *** ## typeMeat 1.856 7.739 0.240 0.811 ## typePoultry -38.085 7.739 -4.921 9.39e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 23.46 on 51 degrees of freedom ## Multiple R-squared: 0.3866, Adjusted R-squared: 0.3626 ## F-statistic: 16.07 on 2 and 51 DF, p-value: 3.862e-06 Note that the test statistic and \\(p\\)-value for the model utility test are exactly the same as the test statistic and \\(p\\)-value from the ANOVA \\(F\\)-test. One-factor ANOVA also generalizes a two-sample \\(t\\)-test. To illustrate, consider the data below, which show body temperatures for 65 randomly selected women and 65 randomly selected men. bodytemp &lt;- read.table(&quot;data/bodytemp.txt&quot;, head = T, stringsAsFactors = T) with(bodytemp, stripchart(bodytemp ~ gender, method = &#39;jitter&#39;, pch = 16, las = 1)) Suppose we want to study the difference in average body temperature between men and women. In an introductory course you might have conducted this analysis with a two-sample \\(t\\)-test. Here is how to implement this \\(t\\)-test in R. t.test(bodytemp ~ gender, data = bodytemp, var.equal = TRUE) ## ## Two Sample t-test ## ## data: bodytemp by gender ## t = 2.2854, df = 128, p-value = 0.02393 ## alternative hypothesis: true difference in means between group female and group male is not equal to 0 ## 95 percent confidence interval: ## 0.03882216 0.53963938 ## sample estimates: ## mean in group female mean in group male ## 98.39385 98.10462 The \\(p\\)-value of 0.0239 suggests that there is reasonably strong evidence of a difference in average body temperature for males and females. Compare this output with a one-factor ANOVA analysis: anova(lm(bodytemp ~ gender, data = bodytemp)) ## Analysis of Variance Table ## ## Response: bodytemp ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## gender 1 2.719 2.71877 5.2232 0.02393 * ## Residuals 128 66.626 0.52052 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The \\(p\\)-values are exactly the same, because one-factor ANOVA with two groups is identical to a two-sample \\(t\\)-test (with a pooled variance). 6.2.3 Assumptions in ANOVA Just like regression, ANOVA is based on assumptions about the distributions of the observations within each group. If these assumptions are badly violated, then the \\(F\\)-test is not trustworthy. In ANOVA, the errors are assumed to be independent, normally distributed, and to have equal variance. Because we have seen that ANOVA is tantamount to regression with indicators, it should not surprise you that these are the same assumptions that we encountered in regression. (The linearity assumption from regression is less pertinent to ANOVA because there are no continuously valued predictors.) As with regression, we could use the ``residuals’’ \\(e_{ij} = y_{ij} - \\bar{y}_{i+}\\) to construct diagnostic plots that allow us to assess these assumptions. With a one-factor ANOVA, however, it typically suffices just to plot the data (either using dotplots, histograms, or boxplots). Troublesome violations of assumptions should be apparent. If assumptions are violated, what can we do? As with a two-sample \\(t\\)-test, there are several options. A first option is to transform the data, just as we transformed the response in regression. Transformations are particularly useful for right-skewed data (in which case one may use a log transformation), count data (square-root transformation), or proportion data (eg, percenteges, in which case an arcsin-square root transformation is appropriate). A second option with non-normal data is to try a non-parametric procedure. A parametric method assumes that the data come from a population with a (so-called) parametric probability distribution. A non-parametric method does not make any particular assumption about the distribution of the population(s) from which the data were collected. The non-parametric equivalent to a one-factor ANOVA is the Kruskal-Wallis test. 6.3 Contrasts of group means If we reject the null hypothesis that all group means are equal in a one-factor ANOVA, we usually want to go further and characterize how the group means differ. The approach that we use to characterize differences among group means depends on whether we are interested in pre-planned comparisons of the group means or in comparisons that are suggested by the data. A pre-planned comparison is one that you planned to investigate before having collected the data. Pre-planned comparisons can be analyzed using linear contrasts, which are the topic of this section. Comparisons suggested by the data should be analyzed with multiple-comparisons procedures, which we will study later. 6.3.1 Simple contrasts A linear contrast generalizes the idea of a pairwise difference between two means. A linear contrast is a special type of linear combination. A linear combination of the group means is a weighted sum of the group means. It can be written as \\[ \\theta = w_1 \\mu_1 + w_2 \\mu_2 + \\ldots + w_g \\mu_g. \\] the \\(w\\)’s in the expression above are weights that we assign to each group mean. We can treat a linear combination just like any other statistical parameter. A (simple) linear contrast is a linear combination in which the weights add to 0: \\[ \\sum_{i=1}^g w_i = 0. \\] If all the group means are equal, then the true value of any contrast will be 0. A little algebra will verify this claim. A single linear contrast is also called a simple contrast. Later, we will encounter complex contrasts, which are collections of simple contrasts. Example. Consider the hotdog data. Let \\(\\mu_B\\), \\(\\mu_M\\), and \\(\\mu_P\\) denote the average calorie content for beef, meat, and poultry hotdogs, respectively. A linear contrast that compares the average calorie content of beef hotdogs vs. poultry hotdogs is \\[ \\theta_1 = \\mu_B - \\mu_P = (+1) \\times \\mu_B + 0 \\times \\mu_M + (-1) \\times \\mu_P. \\] For a contrast, we are almost always interested in testing the null hypothesis that the contrast is 0 (\\(H_0: \\theta_1 = 0\\)) against the alternative that the difference is not 0 (\\(H_a: \\theta_1 \\ne 0\\)). Typically, this happens by computing a sum-of-squares associated with the contrast. There’s a formula for the sum-of-squares for a contrast, but it isn’t enlightening, so we’ll let the computer do the computations. We’ll present output here and focus on coding later. The sum-of-squares associated with the contrast \\(\\theta_1\\) is \\(SS(\\theta_1) = 13328\\). To test \\(H_0: \\theta_1 = 0\\), we must first convert this sum-of-squares to a mean-square by dividing by the number of degrees of freedom associated with the contrast. For a simple contrast, the associated df is 1. We then compare the mean-square for the contrast to the MSE via an \\(F\\)-statistic. For the contrast \\(\\theta_1\\), this plays out as \\[ F_{1,51} = \\frac{MS(\\theta_1)}{MS_{Error}} = \\frac{SS(\\theta_1) / 1}{SS_{Error} / (n_T - g)} = \\frac{13328/1}{28067/51} = 24.22. \\] Comparing to an \\(F_{1,51}\\) distribution yields a \\(p\\)-value \\(&lt;0.0001\\). 6.3.2 Complex contrasts A complex contrast is a collection of several linearly independent contrasts. It is a bit easier to define “linear independence” by describing what it isn’t: several contrasts linearly dependent if one contrast in the group is perfectly correlated with a weighted sum of one or more other contrasts in the group. Most trivially, in a design that includes the two group means \\(\\mu_1\\) and \\(\\mu_2\\), the contrasts \\(\\mu_1 - \\mu_2\\) and \\(\\mu_2 - \\mu_1\\) are linearly dependent, because they are perfectly (negatively) correlated. Less trivially, consider the hot-dog data, and consider the three contrasts \\[\\begin{eqnarray*} \\theta_1 &amp; = &amp; \\mu_B - \\mu_M \\\\ \\theta_2 &amp; = &amp; \\mu_B - \\mu_P \\\\ \\theta_3 &amp; = &amp; \\mu_M - \\mu_P. \\\\ \\end{eqnarray*}\\] These contrasts are not linearly independent, because \\(\\theta_3\\) (the difference between meat and poultry) is perfectly (positively) correlated with the difference \\(\\theta_2 - \\theta_1\\). (In fact, \\(\\theta_3 = \\theta_2 - \\theta_1\\)). Linear independence is important for contrasts because we want to test if contrasts equal \\(0\\). If several contrasts are linearly independent, then the test of each contrast is informative. On the other hand, if several contrasts are not linearly independent, then those tests are redundant. For example, in the hotdog data, if \\(\\theta_1 = 0\\) and \\(\\theta_2 = 0\\), then it must be true that \\(\\theta_3 = 0\\). Thus the test of \\(H_0: \\theta_3 = 0\\) is redundant with test of \\(H_0: \\theta_1 = \\theta_2 = 0\\). We test a complex contrast by testing whether all the simple contrasts it contains equal \\(0\\). For example, in the hotdog data, and using the contrasts as defined above, we can test \\(H_0\\): \\(\\theta_1 = \\theta_2 = 0\\), vs. the alternative that at least one of \\(\\theta_1\\) and \\(\\theta_2\\) is not 0. Of course, \\(H_0\\): \\(\\theta_1 = \\theta_2 = 0\\) is equivalent to \\(H_0\\): \\(\\mu_B = \\mu_M = \\mu_P\\), the usual null hypothesis of equality of group means. Thus, contrasts give us an alternative route to understanding the ANOVA \\(F\\)-test. This alternative route will become useful when we consider ANOVA for factorial designs later. Suppose we have a complex contrasts that consists of \\(k\\) linearly independent contrasts, which we denote as \\(\\theta_1, \\theta_2, \\ldots, \\theta_k\\). We can use an \\(F\\)-test to test the null hypothesis that all \\(k\\) contrasts are equal to zero. The \\(F\\)-test involves finding a sum of squares for the collection of contrasts, which we will write as \\(SS(\\theta_1, \\theta_2, \\ldots, \\theta_k)\\), and has \\(k\\) df. We usually can’t calculate \\(SS(\\theta_1, \\theta_2, \\ldots, \\theta_k)\\) by hand, but a computer can do so for us. To test the null hypothesis that all \\(k\\) contrasts are equal to zero, we construct an \\(F\\) test in the usual way: \\[ F = \\frac{MS(\\theta_1, \\theta_2, \\ldots, \\theta_k)}{MSE} = \\frac{SS(\\theta_1, \\theta_2, \\ldots, \\theta_k) / k}{MSE} \\] Here’s PROC GLM code in SAS to test \\(H_0\\): \\(\\theta_1 = \\theta_2 = 0\\) (see the section on PROC GLM for explanation of coding). proc glm data=hotdog; class type; model calorie = type; /* --------------------------- B M P --*/ contrast &#39;B vs M, B vs P&#39; type 1 -1 0, type 1 0 -1; run; Sum of Source DF Squares Mean Square F Value Pr &gt; F Model 2 17692.19510 8846.09755 16.07 &lt;.0001 Error 51 28067.13824 550.33604 Corrected Total 53 45759.33333 Contrast DF Contrast SS Mean Square F Value Pr &gt; F B vs M, B vs P 2 17692.19510 8846.09755 16.07 &lt;.0001 The \\(F\\)-test of \\(H_0\\): \\(\\theta_1 = \\theta_2 = 0\\) yields exactly the same result as the \\(F\\)-test of \\(H_0\\): \\(\\mu_1 = \\mu_2 = \\mu_3\\), which should not surprise us. Let us collect what we have learned so far. With \\(g\\) groups, the ANOVA \\(F\\)-test that all the group means are equal, that is, of \\(H_0\\): \\(\\mu_1 = \\mu_2 = \\ldots = \\mu_g\\) is equivalent to a test of \\(g-1\\) linearly independent contrasts among the group means. Note that there are many possible sets of \\(g-1\\) linearly independent contrasts that can be used to test the equality of group means. For example, in the hot-dog data, we could also define the contrasts \\[\\begin{eqnarray*} \\theta_3 &amp; = &amp; \\mu_2 - \\mu_3 \\\\ \\theta_4 &amp; = &amp; 2 \\mu_1 - (\\mu_2 + \\mu_3). \\end{eqnarray*}\\] Because \\(\\theta_3\\) and \\(\\theta_4\\) are linearly independent contrasts, a test of \\(H_0\\): \\(\\theta_3 = \\theta_4 = 0\\) should also yield the same \\(F\\)-statistic, and in fact it does: proc glm data=hotdog; class type; model calorie = type; /* ------------------------------- B M P --*/ contrast &#39;two more contrasts&#39; type 0 1 -1, type 2 -1 -1; run; Contrast DF Contrast SS Mean Square F Value Pr &gt; F two more contrasts 2 17692.19510 8846.09755 16.07 &lt;.0001 So what is the point, if all we have done is to recreate the familiar \\(F\\)-test for equality of means? We will see that contrasts actually allow us to go much further. This is illustrated in the next example. Example. An experiment is conducted to investigate the effects of five different sugar treatments on the length (in mm) of pea sections grown in tissue culture. The five treatments are: a control (no sugar added), +2% sucrose, +2% fructose, +2% glucose, and +1% fructose +1% glucose. The data are balanced, with \\(n = 10\\) replicates per treatment. These data appear in Sokal and Rohlf (1995). A stripchart of the data is shown below. pea &lt;- read.table(&quot;data/pea.txt&quot;, head = T, stringsAsFactors = T) with(pea, stripchart(length ~ trt, method = &quot;jitter&quot;, pch = 16)) Notice that the equal variance assumption is suspect here, as the variance of the control group looks larger than the variances of the other groups. We might want to analyze the log of the response to stabilize the variance, but for now we will continue to analyze the raw data. The usual one-factor ANOVA allows us to reject the null hypothesis of no difference among group means: Sum of Mean Source DF Squares Square F Value Pr &gt; F Model 4 14.001 3.500 49.37 &lt;.0001 Error 45 3.190 0.071 Total 49 17.191 There are several further comparisons of these data that we might have deemed interesting before examining the data. These include: Does the control differ from the average of the non-control treatments? Does the fructose / glucose blend differ from the average of the pure fructose and pure glucose treatments? Do the three pure sugar treatments differ? The first two of these are simple contrasts, and the last is a complex contrast. Using subscripts to distinguish treatment means, the first contrast can be expressed as \\[ \\theta_1 = \\mu_c - \\frac{\\mu_{fg} + \\mu_{f} + \\mu_{g} + \\mu_{s}}{4}. \\] For any contrast, we can multiply the entire contrast by a number without changing the meaning of the null hypothesis that the contrast equals \\(0\\). It is sometimes handy to keep the coefficients that multiply the treatment means as whole numbers, so we will rewrite this contrast as \\[ \\theta_1 = 4\\mu_c - (\\mu_{fg} + \\mu_{f} + \\mu_{g} + \\mu_{s}). \\] The second contrast writes as \\[ \\theta_2 = 2\\mu_{fg} - (\\mu_{f} + \\mu_{g}). \\] The third contrast is a complex contrast that contains two linearly independent simple contrasts. One way to write these simple contrasts is \\[\\begin{eqnarray*} \\theta_3 &amp; = &amp; \\mu_f - \\mu_g \\\\ \\theta_4 &amp; = &amp; \\mu_g - \\mu_s. \\end{eqnarray*}\\] Here is PROC GLM code to test for each of these three contrasts, and the output it generates: proc glm data=pea; class trt; model length = trt / solution; /*-------------------------------------1f1g 2f 2g 2s ctrl --*/ contrast &#39;Control vs. sugars&#39; trt -1 -1 -1 -1 4; contrast &#39;Mixed vs pure f, pure g&#39; trt 2 -1 -1 0 0; contrast &#39;Diffs among pure sugars&#39; trt 0 1 0 -1 0, trt 0 0 1 -1 0; run; Contrast DF Contrast SS Mean Square F Value Pr &gt; F Control vs. sugars 1 10.81683072 10.81683072 152.56 &lt;.0001 Mixed vs pure f, pure g 1 0.04873500 0.04873500 0.69 0.4114 Diffs among pure sugars 2 2.55847920 1.27923960 18.04 &lt;.0001 It is often a nice touch to present contrasts of pre-planned comparisons as part of the ANOVA table, like so: Sum of Source DF Squares Mean Square F Value Pr &gt; F Groups 4 14.00085072 3.50021268 49.37 &lt;.0001 Ctrl. vs sugars 1 10.81683072 10.81683072 152.56 &lt;.0001 Mixed vs pure f, pure g 1 0.04873500 0.04873500 0.69 0.4114 Diffs among pure sugars 2 2.55847920 1.27923960 18.04 &lt;.0001 Error 45 3.19051800 0.07090040 Total 49 17.19136872 It is a bit unsatisfying to not have any sense of how the sums of squares for a set of contrasts is calculated. The best we can do here is to point out that a test that several contrasts are simultaneously equal to zero can alternatively be formulated as an \\(F\\)-test, using the machinery of “full” and “reduced” models that we studied in the context of multiple regression. Indeed, it is exactly the same idea, and if we wrote the ANOVA model as a regression with indicator variables, we could test a set of contrasts among the group means using exactly the same approach. Thus, it is no surprise that the test statistic for testing several simultaneous contrasts is an \\(F\\)-statistic. 6.3.3 Orthogonal contrasts We have already seen that if several linearly independent contrasts are not perfectly correlated, they are linearly independent. Linearly independent contrasts might still be partially correlated, however. For example, the contrasts \\(\\theta_1 = \\mu_1 - \\mu_2\\) and \\(\\theta_2 = \\mu_1 - \\mu_3\\) are linearly independent, but they both depend in the same way on \\(\\mu_1\\). Thus, if we happen to draw a sample from group 1 that is unusual in some regard, the event of drawing that sample will affect our tests of both \\(H_0\\): \\(\\theta_1 = 0\\) and \\(H_0\\): \\(\\theta_2 = 0\\) in the same way. There is a stronger notion of independence among contrasts called orthogonality. (You might remember that orthogonal is a synonym for perpendicular.) Two simple contrasts are orthogonal if they are perfectly uncorrelated. It turns out that we can check whether two simple contrasts are orthogonal in the following way. Two contrasts \\(\\theta_1 = \\sum_{i=1}^g c_i \\mu_i\\) and \\(\\theta_2 = \\sum_{i=1}^g d_i \\mu_i\\) are orthogonal if and only if \\[ \\sum_{i=1}^g \\frac{c_i d_i}{n_i} = 0. \\] Note that if two contrasts are orthogonal, then they must also be linearly independent. However, two contrasts that are linearly independent may or may not be orthogonal. Complex contrasts are orthogonal if every simple contrast in the first complex contrast is orthogonal to every simple contrast in the second complex contrast. Note that the simple contrasts within complex contrasts may or may not be orthogonal to each other; whether or not they are orthogonal with each other is irrelevant. All we care about is whether the simple contrasts in the first complex contrast are orthogonal to the simple contrasts in the second complex contrast. If this all seems to be coming out of thin air, the reason is that contrasts, linear independence, and orthogonality are all ultimately linear algebra concepts. A complex contrast defines a subspace of \\(\\mathcal{R}^n\\), the linearly independent contrasts that comprise it are a basis for that subspace, and two linearly independent contrasts are orthogonal iff the corresponding subspaces are orthogonal. Further, the sum-of-squares associated with a contrast is the squared length of the orthogonal projection of the data vector (which lives in \\(\\mathcal{R}^n\\)) onto that subspace, and hence orthogonality of the contrasts implies additivity of the sums-of-squares (see below) via Pythagoras. The major upshot is that if two complex contrasts are orthogonal, then their sums of squares are additive. That is, \\[ SS(\\theta_1, \\theta_2, \\ldots, \\theta_k, \\phi_1, \\phi_2, \\ldots, \\phi_l) = SS(\\theta_1, \\theta_2, \\ldots, \\theta_k) + SS(\\phi_1, \\phi_2, \\ldots, \\phi_l). \\] That is, the sum-of-squares for the combined complex contrast can be partitioned into the sum-of-squares for the first complex contrast plus the sum-of-squares for the second complex contrast. In the pea example above, note that the complex contasts are not orthogonal. Thus, we have \\(SS(\\theta_1) + SS(\\theta_2) + SS(\\theta_3, \\theta_4) \\neq SS_{Groups}\\). However, suppose we redefine the second contrast as the difference between the fructose / glucose blend and the average of the three pure sugar treatments, that is, replace \\(\\theta_2\\) with \\[ \\theta_{2a} = 3\\mu_{fg} - (\\mu_{f} + \\mu_{g} + \\mu_s). \\] Now the contrasts \\(\\theta_1\\), \\(\\theta_{2a}\\), and the complex constrast \\((\\theta_3\\), \\(\\theta_4\\)) are orthogonal, and so \\(SS(\\theta_1) + SS(\\theta_{2a}) + SS(\\theta_3, \\theta_4) = SS_{Groups}\\). The code below illustrates: proc glm data=pea; class trt; model length = trt / solution; /*-------------------------------------1f1g 2f 2g 2s ctrl --*/ contrast &#39;Control vs. sugars&#39; trt -1 -1 -1 -1 4; contrast &#39;Mixed vs pure sugars &#39; trt 3 -1 -1 -1 0; contrast &#39;Diffs among pure sugars&#39; trt 0 1 0 -1 0, trt 0 0 1 -1 0; run; Contrast DF Contrast SS Mean Square F Value Pr &gt; F Control vs. sugars 1 10.81683072 10.81683072 152.56 &lt;.0001 Mixed vs pure sugars 1 0.62554080 0.62554080 8.82 0.0048 Diffs among pure sugars 2 2.55847920 1.27923960 18.04 &lt;.0001 We might modify the ANOVA table accordingly: Sum of Source DF Squares Mean Square F Value Pr &gt; F Groups 4 14.00085072 3.50021268 49.37 &lt;.0001 Ctrl. vs sugars 1 10.81683072 10.81683072 152.56 &lt;.0001 Mixed vs pure sugars 1 0.62554080 0.62554080 8.82 0.0048 Diffs among pure sugars 2 2.55847920 1.27923960 18.04 &lt;.0001 Error 45 3.19051800 0.07090040 Total 49 17.19136872 While orthogonality is nice, it is something of a fringe benefit (Quinn and Keough (2002)). Contrasts should be designed to match the comparisons of scientific interest; the comparisons of interest should not be arranged to generate orthogonal contrasts. The natural question that arises is: When should we expect orthogonality? One doesn’t just stumble onto sets of orthogonal contrasts by chance. There are three types of experiments where sets of orthogonal contrasts can be expected. If you are able to recognize these types of experiments, you can use orthogonal contrasts to create an especially thorough and satisfying analysis. The three types of experiments that support sets of orthogonal contrasts are: Factorial experiments. By a large margin, factorial experiments (in which treatment groups are formed by crossing multiple experimental factors) are the most important context for orthogonal contrasts. We will study factorial experiments in depth in the next installment of the notes, and thus will say no more about them here. Experiments in which the treatment groups cluster into a hierarchy, and the data are balanced. The pea experiment above could provides an example. The five treatment groups naturally cluster into a group of sugar-addition treatments and a singleton control group. In this type of experiment, we can find a set of contrasts to test for differences among the treatment means within each cluster, and another set of contrasts to test for differences among the clusters, exactly as we have done with the pea data. Experiments in which the treatment groups are formed by different levels of a quantitative variable. We study this scenario in more depth below. 6.3.4 Polynomial contrasts Consider the following data set. Pott (1992) conducted an experiment to investigate the effect of added dietary molybdenum (Mo) on the molybdenum concentrations of sheep kidneys. Twenty (20) sheep were randomly assigned to one of four diets, where the diets differed only in the amount of supplementary molybdenum (0, 15, 30 and 45 ppm). Five sheep were assigned to each diet, and the Mo concentration in the kidney was measured after 14 days. The data are shown below: molybdenum &lt;- read.csv(&quot;data/molybdenum.csv&quot;, head = T) with(molybdenum, stripchart(conc ~ diet, method = &quot;jitter&quot;, pch = 16)) Should we model these data with an ANOVA or a regression? In this case, the treatment groups are formed by different values of a quantitative variable — in this case, the amount of supplementary molybdenum in the diet. Many experiments form treatment groups in this way. Often, the data exhibit a clear trend with respect to the quantitative variable, as the sheep example does. This is easier to see if we plot the molybdenum variable on the horizontal axis, as we would if this were a regression design: with(molybdenum, plot(conc ~ diet, xlab = &quot;molybdenum addition (ppm)&quot;, ylab = &quot;kidney Mo&quot;, pch = 16)) THis plot makes it clear that, as with much biological data, the variance increases as the average response increases. We should analyze the log of the response to stabilize the variance. with(molybdenum, plot(log(conc) ~ diet, xlab = &quot;molybdenum addition (ppm)&quot;, ylab = &quot;log(kidney Mo)&quot;, pch = 16)) Now we can essentially have our cake and eat it too, by using polynomial contrasts to characterize the underlying trend within the context of an ANOVA. Our strategy is to decompose the \\(SS_{Groups}\\) into sums-of-squares that capture the polynomial component of the underlying trend. That is, with four treatment groups (such as we have with the sheep data), we can partition the \\(SS_{Groups}\\) as \\[ SS_{Groups} = SS(\\theta_1) + SS(\\theta_2) + SS(\\theta_3) \\] where \\(\\theta_1\\) captures the linear trend in the group means, \\(\\theta_2\\) captures the quadratic trend in the group means, and \\(\\theta_3\\) captures the cubic trend in the group means. Additionally, we want all of these contrasts to be orthogonal to one another. If the data are balanced, and if the treatment groups are equally spaced with respect to the quantitative variable used to construct them, then coefficients for orthogonal polynomial contrasts can be found in a table, such as Table D.6 of Oehlert. The sheep data satisfy these special conditions, so we can go ahead and use Table D.6 in Oehlert to find that the orthogonal polynomial contrasts are: \\[\\begin{eqnarray*} \\theta_1 &amp; = &amp; -3 \\mu_1 - \\mu_2 + \\mu_3 + 3 \\mu_4 \\\\ \\theta_2 &amp; = &amp; \\mu_1 - \\mu_2 - \\mu_3 + \\mu_4 \\\\ \\theta_3 &amp; = &amp; - \\mu_1 + 3 \\mu_2 - 3 \\mu_3 + \\mu_4 \\end{eqnarray*}\\] where \\(\\mu_1\\), \\(\\mu_2\\), \\(\\mu_3\\), and \\(\\mu_4\\) are the means for the treatment groups with 0, 15, 30 and 45 ppm supplementary molybdenum, respectively. proc glm data = sheep; class diet; model logy = diet; contrast &#39;Linear&#39; diet -3 -1 1 3; contrast &#39;Quadratic&#39; diet 1 -1 -1 1; contrast &#39;Cubic&#39; diet -1 3 -3 1; run; The GLM Procedure Dependent Variable: conc Sum of Source DF Squares Mean Square F Value Pr &gt; F Model 3 2.89623371 0.96541124 12.26 0.0002 Error 16 1.26006314 0.07875395 Corrected Total 19 4.15629685 Contrast DF Contrast SS Mean Square F Value Pr &gt; F Linear 1 2.63828807 2.63828807 33.50 &lt;.0001 Quadratic 1 0.24832056 0.24832056 3.15 0.0948 Cubic 1 0.00962508 0.00962508 0.12 0.7312 Thus we see that the linear trend is statistically significant (i.e., we can reject \\(H_0\\): \\(\\theta_1 = 0\\)), but neither the quadratic nor the cubic trends are statistically significant. Our usual hope is that we can explain the trend in the group means parsimoniously with a small number of low-order polynomial contrasts. In this case, it is common to pool the higher-order, non-significant contrasts together into a single sum-of-squares that we will call the “lack-of-fit”. With the sheep data, we can pool the quadratic and cubic contrasts together into a lack-of-fit term, as shown below: proc glm data = sheep; class diet; model logy = diet; contrast &#39;Linear&#39; diet -3 -1 1 3; contrast &#39;Lack-of-fit&#39; diet 1 -1 -1 1, diet -1 3 -3 1; run; Contrast DF Contrast SS Mean Square F Value Pr &gt; F Linear 1 2.63828807 2.63828807 33.50 &lt;.0001 Lack-of-fit 2 0.25794564 0.12897282 1.64 0.2254 We conclude that there is strong evidence that the treatment groups differ, and the differences among the treatment groups are parsimoniously explained by a linear trend. As before, it makes for a nice touch to combine this all into one ANOVA table: Sum of Source DF Squares Mean Square F Value Pr &gt; F Groups 3 2.89623371 0.96541124 12.26 0.0002 Linear 1 2.63828807 2.63828807 33.50 &lt;.0001 Lack-of-fit 2 0.25794564 0.12897282 1.64 0.2254 Error 16 1.26006314 0.07875395 Total 19 4.15629685 If data are not balanced, or if the treatment groups are not equally spaced with respect to the quantitative covariate that defines them, specialized software can be used to find the coefficients for orthogonal polynomial contrasts.28 6.4 Multiple comparisons In contrast to pre-planned comparisons, when we just want to describe how population means differ, we need tools for multiple comparisons. Multiple comparisons are a concern any time we conduct several hypothesis tests in the context of a single analysis or experiment. We will introduce the general ideas behind multiple comparisons first, before proceeding to a specific discussion of how these ideas apply in the context of comparing means from a one-factor ANOVA. 6.4.1 Multiple testing in general Recall that in the context of hypothesis testing, a Type I error occurs when we erroneously reject a true null hypothesis. (In contrast, a Type II error occurs when we fail to reject a false null hypothesis.) Suppose we conducted 10 independent hypothesis tests, each of which has a 5% Type I error rate. Suppose also that \\(H_0\\) is true for each of these 10 tests. Then the probability of correctly failing to reject \\(H_0\\) on all 10 tests is \\(0.95^{10} = 0.60\\), and so the probability of committing a Type I error on at least one test is \\(1- 0.60 = 0.40\\). Thus, the overall probability of falsely rejecting a true null at least once is much greater than the 5% Type I error rate for any individual test. This is the essence of the multiple comparisons problem. To structure our thinking, it is helpful to define several different Type I error rates. Oehlert (section 5.1) describes several different Type I error rates. The experimentwise (or familywise) error rate is the probability of falsely rejecting at least one null hypothesis, if all of the null hypotheses are true. The comparisonwise (or individual) error rate is the probability of falsely rejecting the null in a single hypothesis test. The strong familywise error rate is the probability of falsely rejecting at least one null hypothesis, regardless of how many of the null hypotheses are actually true, and how many are actually false. Finally, the false discovery rate (FDR) is the proportion of rejected null hypotheses that are rejected incorrectly. Which multiple testing procedure we use depends on which error rate we want to control. Some multiple testing procedures can be applied to any collection of hypothesis tests, while others are designed specifically for detecting significant differences among a collection of treatment means. Below, we will follow Oehlert’s lead and denote the error rate that we wish to control as \\(\\mathcal{E}\\). The specific error rate to which this refers will depend on context. 6.4.2 Bonferroni and Bonferroni-like procedures The simplest way to control the strong familywise error rate is with a Bonferroni correction. Suppose we intend to perform \\(K\\) individual hypothesis tests, and want to fix the experiment-wise error rate at \\(\\mathcal{E}\\). For each individual test, we reject \\(H_0\\) if and only if \\[ p \\leq \\frac{\\mathcal{E}}{K}. \\] An interesting variation of the Bonferroni procedure was developed by Holm (1979). The steps of the Holm procedure are: Sort the \\(p\\)-values from smallest to largest. Use subscripts with parentheses to denote the sorted \\(p\\)-values, so that \\(p_{(1)} \\leq p_{(2)} \\leq \\ldots \\leq p_{(K)}\\). Starting with the smallest \\(p\\)-value, reject the associated null hypothesis if \\[ p_{(j)} \\leq \\frac{\\mathcal{E}}{K-j+1}. \\] Continue until encountering a \\(p\\)-value for which the associated null hypothesis cannot be rejected. Then stop: Neither the null hypothesis associated with this \\(p\\)-value, nor the null hypotheses associated with any larger \\(p\\)-values, are rejected. \\end{enumerate} The Holm procedure also controls the strong familywise error rate. Another interesting variation of the Bonferroni procedure is due to Benjamini and Hochberg (1995). This procedure only works with independent hypotheses, and it controls the FDR. The steps of Benjamini &amp; Hochberg’s FDR procedure are: Sort the \\(p\\)-values from smallest to largest. Use subscripts with parentheses to denote the sorted \\(p\\)-values, so that \\(p_{(1)} \\leq p_{(2)} \\leq \\ldots \\leq p_{(K)}\\). Find the largest \\(p\\)-value for which \\[ p_{(j)} \\leq \\frac{j\\mathcal{E}}{K}. \\] Once the largest \\(p\\)-value for which the above is true has been found, reject both the null hypothesis associated with this \\(p\\)-value, and the null hypotheses associated with all smaller \\(p\\)-values. 6.4.3 Multiple comparisons in ANOVA In the context of ANOVA, our goal is to simultaneously compare all possible pairs of means. Thus, if there are \\(g\\) means to be compared, then there are \\(K = g \\times (g-1) / 2\\) different comparisons to be performed. As \\(g\\) becomes larger, the number of comparisons grows quickly. For example, if \\(g = 3\\), then there are only \\(K = 3\\) different comparisons (ex., beef vs. meat, beef vs. poultry, and meat vs. poultry). However, if \\(g = 10\\), then there are \\(K = 45\\) different comparisons. Also, in the context of ANOVA, many (but not all) multiple comparisons procedures boil down to comparing the difference between each pair of means to a minimum significant difference. That is, two means will be declared significantly different from one another if the (absolute) difference between them equals or exceeds some minimum level. When the data are balanced, there is one common minimum significant difference for all pairs of means. When the data are not balanced, the minimum significant difference can itself differ for different pairs of means (because of the differences in sample sizes). 6.4.3.1 Bonferroni correction. To implement a Bonferroni correction, we can simply use a linear contrast for each pairwise comparison, and test the null hypothesis that the pairwise difference is equal to 0. To control the strong familywise error rate, we would reject the null hypothesis of no difference if \\(p \\leq \\mathcal{E} / K\\). Example. Sokal and Rohlf (1995) give the following data. H. L. Willis measured the head width of tiger beetles at \\(g=8\\) sites across the central U.S. The data are balanced, with are \\(n=15\\) tiger beetles measured at each site. The data are shown below. beetle &lt;- read.table(&quot;data/beetle.txt&quot;, head = T, stringsAsFactors = T) par(mar = c(5, 7, 1, 1)) with(beetle, stripchart(head.width ~ site, method = &quot;jitter&quot;, pch = 16, las = 1)) With \\(g=8\\) groups, there are \\(K = (8 \\times 7)/ 2 = 28\\) pairwise comparisons. Thus, for a strong familywise error rate of \\(\\mathcal{E} = 0.05\\), we could perform contrasts for all pairwise comparisons manually, and declare two group means to be different using a \\(p\\)-value threshold of \\(p \\leq 0.05 / 28 = 0.0018\\). Alternatively, we can use the MEANS statement in PROC GLM. In PROC GLM, the MEANS statement computes the mean for each treatment group, and then clusters the group means if a multiple comparisons procedure is specified. For example, to perform all pairwise comparisons using a Bonferroni correction, use the BON option on the MEANS statement: proc glm data=beetle; class site; model head_width = site; means site / bon; run; The GLM Procedure Dependent Variable: head_width Sum of Source DF Squares Mean Square F Value Pr &gt; F Model 7 0.99607399 0.14229628 6.46 &lt;.0001 Error 112 2.46675560 0.02202460 Corrected Total 119 3.46282959 Bonferroni (Dunn) t Tests for head_width NOTE: This test controls the Type I experimentwise error rate, but it generally has a higher Type II error rate than REGWQ. Alpha 0.05 Error Degrees of Freedom 112 Error Mean Square 0.022025 Critical Value of t 3.20042 Minimum Significant Difference 0.1734 Means with the same letter are not significantly different. Bon Grouping Mean N site A 3.88053 15 Stafford.K A 3.77353 15 Mayfield.O A 3.76373 15 Okeene.OK A 3.75353 15 Kackley.KS A 3.75247 15 Talmo.KS A 3.73260 15 NE A 3.70753 15 Roswell.NM B 3.53120 15 Barnard.KS PROC GLM summarizes the outcome of the multiple comparisons procedure by assigning letters to groups (A, B, …). Groups that do not share a letter in common have statistically distinguishable means. Thus, we again conclude that Barnard, KS is significantly different from all the other sites, but that none of the other sites are significantly different from one another. With balanced data, two means will be significantly different if they differ by more than a threshold difference. PROC GLM reports this difference as the `minimum significant difference’, which for the beetle data is 0.1734. Several popular multiple comparisons procedures give rise to a minimum significant difference with balanced data, although the size of the minimum significant difference will depend on the procedure being used. With unbalanced data, the minimum significant difference depends on the sample sizes of the two groups being compared. Thus, SAS doesn’t report a common minimum significant difference, and instead reports significance levels for each possible pairwise comparison. We can see this with the hotdog data: proc glm; class type; model calorie = type; means type / bon; run; Bonferroni (Dunn) t Tests for calorie NOTE: This test controls the Type I experimentwise error rate, but it generally has a higher Type II error rate than Tukey&#39;s for all pairwise comparisons. Alpha 0.05 Error Degrees of Freedom 51 Error Mean Square 550.336 Critical Value of t 2.47551 Comparisons significant at the 0.05 level are indicated by ***. Difference type Between Simultaneous 95% Comparison Means Confidence Limits Meat - Beef 1.856 -17.302 21.013 Meat - Poultry 39.941 20.022 59.860 *** Beef - Meat -1.856 -21.013 17.302 Beef - Poultry 38.085 18.928 57.243 *** Poultry - Meat -39.941 -59.860 -20.022 *** Poultry - Beef -38.085 -57.243 -18.928 *** Thus, using Bonferroni, we would declare that meat and poultry hotdogs have a significantly different average calorie content, as do beef and poultry hotdogs. However, the difference between beef and meat hotdogs is not statistically significant. Another option for unbalanced data is to calculate a common BSD using the harmonic mean of the group sizes. The harmonic mean is defined as \\[ \\tilde{n} = \\left[ \\left(\\frac{1}{n_1} + \\frac{1}{n_2} + \\cdots + \\frac{1}{n_g}\\right) / g \\right]^{-1}. \\] In PROC GLM, we implement this by asking for the LINES option in the MEANS statement. Here’s an example with the hotdog data: proc glm; class type; model calorie = type; means type / bon lines; run; Bonferroni (Dunn) t Tests for calorie NOTE: This test controls the Type I experimentwise error rate, but it generally has a higher Type II error rate than REGWQ. Alpha 0.05 Error Degrees of Freedom 51 Error Mean Square 550.336 Critical Value of t 2.47551 Minimum Significant Difference 19.415 Harmonic Mean of Cell Sizes 17.89474 NOTE: Cell sizes are not equal. Means with the same letter are not significantly different. Mean N type A 158.706 17 Meat A 156.850 20 Beef B 118.765 17 Poultry Here, the harmonic mean of the sample sizes is 17.9, resulting in a common minimum significant difference of 19.4. In general, using a harmonic mean as a common sample size is okay if the imbalance is mild, but not if the imbalance is severe. The advantage to a Bonferroni correction is that it is easy, quick, and broadly applicable. The disadvantage is that it is extremely conservative, and thus has very little power to find differences between means. Because of its extremely low power, Bonferroni is not often used in practice. 6.4.3.2 Fisher’s protected Least Significant Difference (LSD) Fisher’s protected LSD is a two-stage procedure that controls the experimentwise error rate. In the first stage, use an ANOVA \\(F\\)-test to test \\(H_0\\): \\(\\mu_1 = \\mu_2 = \\ldots = \\mu_g\\). If we fail to reject \\(H_0\\) at the \\(\\mathcal{E}\\) significance level, we will stop. (This is the “protection”). If \\(H_0\\) is rejected, then the second state entails comparing each pair of means using a two-sample \\(t\\)-test, using \\(MS_{Error}\\) as the pooled variance estimate. Fisher’s protected LSD tends to be anti-conservative. Here’s an example with the beetle data: proc glm data=beetle; class site; model head_width = site; means site / lsd; run; The GLM Procedure t Tests (LSD) for head_width NOTE: This test controls the Type I comparisonwise error rate, not the experimentwise error rate. Alpha 0.05 Error Degrees of Freedom 112 Error Mean Square 0.022025 Critical Value of t 1.98137 Least Significant Difference 0.1074 Means with the same letter are not significantly different. t Grouping Mean N site A 3.88053 15 Stafford.K B A 3.77353 15 Mayfield.O B 3.76373 15 Okeene.OK B 3.75353 15 Kackley.KS B 3.75247 15 Talmo.KS B 3.73260 15 NE B 3.70753 15 Roswell.NM C 3.53120 15 Barnard.KS Note that PROC GLM does not actually implement the first-stage of Fisher’s procedure; instead, it’s the user’s responsibility to stop if the overall \\(F\\)-test does not reject the null hypothesis that all group means are equal. Note that Fisher’s least significant difference (0.1074) is substantially smaller than Bonferroni’s minimum significant difference, and thus a greater number of significant pairwise differences have been detected. In general, Fisher’s LSD suffers the opposite sin of Bonferroni, in that it tends to be too anti-conservative. (If you use LSD, you tend to see differences that aren’t there.) 6.4.3.3 Tukey’s Honest Significant Difference (HSD) For balanced data, if all group means are equal, then it can be shown that the (standardized) difference between the largest sample mean and the smallest sample mean has a known statistical distribution (a so-called studentized range distribution). The idea of Tukey’s HSD is to compare the difference between each pair of sample means from a critical value of this known distribution. If the difference between two sample means is greater than this critical value (Tukey’s HSD), then we declare the means different from one another. Tukey’s HSD tends to be more conservative than Fisher’s LSD, though not as conservative as Bonferroni. Here’s an example of Tukey’s HSD with the beetle data: proc glm data=beetle; class site; model head_width = site; means site / tukey; run; The GLM Procedure Tukey&#39;s Studentized Range (HSD) Test for head_width NOTE: This test controls the Type I experimentwise error rate, but it generally has a higher Type II error rate than REGWQ. Alpha 0.05 Error Degrees of Freedom 112 Error Mean Square 0.022025 Critical Value of Studentized Range 4.36851 Minimum Significant Difference 0.1674 Means with the same letter are not significantly different. Tukey Grouping Mean N site A 3.88053 15 Stafford.K B A 3.77353 15 Mayfield.O B A 3.76373 15 Okeene.OK B A 3.75353 15 Kackley.KS B A 3.75247 15 Talmo.KS B A 3.73260 15 NE B 3.70753 15 Roswell.NM C 3.53120 15 Barnard.KS Note that Tukey’s HSD with the beetle data is 0.1674 – not as big a difference as Bonferroni’s minimum significant difference, but bigger than Fisher’s LSD. As with Bonferroni and Fisher, a common HSD is only available when data are balanced. When data are unbalanced, an adjustment to Tukey’s method is available called the Tukey-Kramer adjustment. Tukey-Kramer tends to be slightly conservative. 6.4.3.4 Comparisons to a control: Dunnett’s procedure Dunnett’s procedure is useful when one group serves as a control, and the comparisons of interest involve comparing the remaining groups to that control. Here is an example using the beetle data, assuming that ‘Stafford.K’ is the control group: proc glm data=beetle; class site; model head_width = site; means site / dunnett(&#39;Stafford.K&#39;); run; The GLM Procedure Dunnett&#39;s t Tests for head_width NOTE: This test controls the Type I experimentwise error for comparisons of all treatments against a control. Alpha 0.05 Error Degrees of Freedom 112 Error Mean Square 0.022025 Critical Value of Dunnett&#39;s t 2.65419 Minimum Significant Difference 0.1438 Comparisons significant at the 0.05 level are indicated by ***. Difference site Between Simultaneous 95% Comparison Means Confidence Limits Mayfield.O - Stafford.K -0.10700 -0.25083 0.03683 Okeene.OK - Stafford.K -0.11680 -0.26063 0.02703 Kackley.KS - Stafford.K -0.12700 -0.27083 0.01683 Talmo.KS - Stafford.K -0.12807 -0.27190 0.01577 NE - Stafford.K -0.14793 -0.29177 -0.00410 *** Roswell.NM - Stafford.K -0.17300 -0.31683 -0.02917 *** Barnard.KS - Stafford.K -0.34933 -0.49317 -0.20550 *** 6.5 \\(^\\star\\)Power and sample-size determination in ANOVA In the course of proposing or designing an experiment, it is often helpful to have some guidance regarding how many data points are needed to obtain the statistical precision that we desire. This sort of information comes from a power calculation. As we’ll see, power calculations require specifying parameter values that are unknown. (If we knew the parameter values, we wouldn’t need to do the experiment!). Often, even having some small amount of pilot data that can be used to make educated guesses about these unknown parameters is superior to taking blind stabs at those values. As a last resort, blind stabs are better than foregoing a power analysis altogether. A power analysis can be focused on one of two particular questions: 1. How many samples are needed for the margin of error around a group mean (or a contrast between group means) to be less than a certain desired threshold? 2. How many samples are needed for the power to reject a false null hypothesis to be at least as big as a certain desired threshold? Here, we will focus on the second of these two questions. For power analysis built around the margin of error, see Oehlert sections 7.2 and 7.4 The power associated with rejecting the null hypothesis that all group means are equal depends on all of the following quantities: The Type I error, \\(\\alpha\\) (i.e., the tolerable false-positive rate; higher \\(\\alpha\\) gives more power) How different the group means actually are (larger differences among group means give more power) The sample size (bigger sample sizes give more power) The number of groups being compared, although this is often dictated by the experiment The variance in observations within the same group, \\(\\sigma^2_{\\epsilon}\\) (greater variation within a group reduces power) A usual approach is to choose a tolerable Type I error rate (often 0.05 or 0.01), and to make educated guesses (ideally based on pilot data) about the actual values of the group means and the error variance , and then to calculate power from either an on-line calculator or a textbook chart. Oehlert provides a few such charts in his Table D.10. If values of the group means and the within-group variance are unknown, the conservative approach is to assume the smallest group-differences that one might hope to detect, and to assume a large (but still reasonable) value for the within-group. Of course, a conservative calculation will suggest a larger sample size, but many scientists prefer to err on the side of caution. To make sense of these charts, one needs to be able to calculate something called a non-centrality parameter. Basically, the non-centrality parameter is a single quantity that pulls together the combined effects of the differences in the group means, the sample sizes, and the error variance on the power. Fortunately, the non-centrality parameter is easy to calculate, even if the formula is somewhat mysterious. NB: In the formula below, the \\(\\alpha_i\\)’s refer to the effect parameters in the ANOVA model, not to the significance level. The non-centrality parameter will be the same regardless of the particular constraint that one chooses to define the effect parameters. The formula for the non-centrality parameter is: \\[ \\zeta = \\dfrac{\\sum_{i=1}^g n_i \\alpha_i^2}{\\sigma^2_{\\epsilon}} \\] As the non-centrality parameter increases, power also increases. Thus, we see that power increases as either the sample sizes increase, the differences among the group means increase, or the error variance decreases. 6.6 Using SAS: The effects parameterization of the one-factor ANOVA SAS has several procedures (or PROCs) that can be used for a one-factor ANOVA analysis. We’ll be using PROC GLM, where GLM stands for General Linear Model. The term “general linear model” encompasses both regression and ANOVA. Thus, PROC GLM can be used for regression modeling as well. At a minimum, PROC GLM code for a one-factor ANOVA model requires two components. First, a CLASS statement is used to declare which of the variables in the data set are categorical variables. Second, a MODEL statement is used to identify the response variable (placed on the left-hand side of the equals sign) and the categorical variable that identifies the factor that distinguishes the groups (placed on the right-hand side of the equals sign). For example, in the hot dog data, the categorical variable called “type” specifies the type of hot dog, and the response variable is labeled “calorie”. PROC GLM code for a one-factor ANOVA model for the calorie data is: proc glm data=hotdog; class type; /* declare categorical variables here */ model calorie = type; run; Note that in SAS, all lines must end with a semicolon (;). Also, comments can be placed between the symbols /* and */. 6.6.1 Effects-model parameterization of the one-factor ANOVA model PROC GLM uses a model parameterization called the “effects parameterization” (or “effects model”). This parameterization is not unique to SAS, and is commonly found in many statistics texts. The effects parameterization seems like overkill for a one-factor ANOVA, but it will be more useful when we contemplate multi-factor ANOVA later. The effects model simply re-expresses the mean of each treatment group as the sum of an overall reference level and a unique “effect” for that treatment group. That is, \\[ \\mu_i = \\mu + \\alpha_i. \\] Here, \\(\\mu\\) is a reference level (alternatively called an intercept), and \\(\\alpha_i\\) is the “effect” of the group \\(i\\). For the moment, we will be intentionally vague about what we mean by a “reference level” and an “effect”, and we will clarify later. We can go one step further and write the one-factor ANOVA model for the entire data set as \\[ y_{ij} = \\mu_i + \\epsilon_{ij} = \\mu + \\alpha_i + \\epsilon_{ij} \\] where the \\(\\epsilon_{ij}\\)’s are residual errors that take the usual assumptions of normality, independence, and constant variance. In the effects representation, the standard ANOVA null hypothesis of equality of the group means can be re-written as \\(H_0\\): \\(\\alpha_1 = \\alpha_2 = \\ldots = \\alpha_g = 0\\). The problem with the effects model is that it is overparameterized. For example, consider the calorie data from the hot dog data set. The average calorie content for each type of hotdog is: \\[\\begin{eqnarray*} \\bar{y}_{1+} &amp; = &amp; 156.9 \\mbox{ calories, for beef}\\\\ \\bar{y}_{2+} &amp; = &amp; 158.7 \\mbox{ calories, for meat}\\\\ \\bar{y}_{3+} &amp; = &amp; 118.8 \\mbox{ calories, for poultry.}\\\\ \\end{eqnarray*}\\] In the effects model, there is no unique way to estimate both the reference level and the individual effect parameters for each hot dog type. For example, we could choose \\[ \\hat{\\mu} = 0, \\ \\ \\hat{\\alpha}_1 = 156.9, \\ \\ \\hat{\\alpha_2} = 158.7, \\ \\ \\hat{\\alpha_3} = 118.8 \\] or we could choose \\[ \\hat{\\mu} = 100, \\ \\ \\hat{\\alpha}_1 = 56.9, \\ \\ \\hat{\\alpha_2} = 58.7, \\ \\ \\hat{\\alpha_3} = 18.8. \\] There is no way to discriminate between these two (or many other) possibilities. Thus, there is no unique set of best parameter estimates in the effects model. To estimate the parameters in the effects model, we need to impose a constraint. There are two commonly used constraints. The first is the sum-to-zero constraint: \\[ \\sum_{i=1}^g \\alpha_i = 0. \\] Under the sum-to-zero constraint, our parameter estimates for the hotdog example are: \\[ \\hat{\\mu} = 144.8, \\ \\ \\hat{\\alpha}_1 = 12.1, \\ \\ \\hat{\\alpha_2} = 13.9, \\ \\ \\hat{\\alpha_3} = -26.0. \\] Under the sum-to-zero constraint, \\(\\mu\\) is the average of the individual group means, and the effects parameters are the differences between the individual group means and \\(\\mu\\).29 The second commonly used constraint is the set-to-zero constraint. In the set-to-zero constraint, we choose one of the groups to serve as a reference and constrain its effect to be \\(0\\). If one of the treatment groups is a control, it makes sense to constrain the effect of the control group to \\(0\\). If there is no control group, then the choice of which effect to constrain = 0 is arbitrary. For the hotdog example, suppose we constrain \\(\\alpha_3 = 0\\). The remaining parameter estimates are: \\[ \\hat{\\mu} = 118.8, \\hat{\\alpha}_1 = 38.1, \\hat{\\alpha_2} = 39.9, \\hat{\\alpha_3} = 0. \\] The set-to-zero constraint is exactly like choosing a reference level in regression with indicators. Under the set-to-zero constraint, \\(\\mu\\) is the average response of the reference group, and the effects parameters \\(\\alpha_i\\) are the differences between the other individual group means and the reference group. When PROC GLM fits a one-factor ANOVA model, it uses the effects parameterization with a set-to-zero constraint. We can ask SAS to display least-squares parameter estimates for this model by adding a SOLUTION option to the MODEL statement in PROC GLM. Here’s an example with the hotdog data: proc glm; class type; model calorie = type / solution; run; Dependent Variable: calorie Standard Parameter Estimate Error t Value Pr &gt; |t| Intercept 118.7647059 B 5.68970197 20.87 &lt;.0001 type Beef 38.0852941 B 7.73883135 4.92 &lt;.0001 type Meat 39.9411765 B 8.04645369 4.96 &lt;.0001 type Poultry 0.0000000 B . . . NOTE: The X`X matrix has been found to be singular, and a generalized inverse was used to solve the normal equations. Terms whose estimates are followed by the letter &#39;B&#39; are not uniquely estimable. Thus, SAS has chosen “poultry” as the reference group. (The default in PROC GLM is to designate as the reference the group that is last alphabetically.) The B’s that appear by the parameter estimates and the NOTE that SAS provides warn us that the parameter estimates would be different under a different constraint. 6.6.2 Coding contrasts in PROC GLM PROC GLM includes two facilities for linear contrasts: the ESTIMATE statement for any linear combination, and the CONTRAST statement specifically for contrasts. The ESTIMATE statement calculates an estimate, a standard error, and a \\(t\\)-test of \\(H_0\\): \\(\\theta = 0\\) vs. \\(H_a\\): \\(\\theta \\ne 0\\). The CONTRAST statement conducts \\(F\\)-tests of simple or complex contrasts. To use either the ESTIMATE or CONTRAST statement, we must re-code each linear combination or simple contrast in terms of the effects parameterization. Consider the contrast \\(\\theta_1 = \\mu_1 - \\mu_3\\). Re-expressing this contrast in terms of the effects parameterization yields \\[\\begin{eqnarray*} \\theta_1 &amp; = &amp; \\mu_1 - \\mu_3 \\\\ &amp; = &amp; (\\mu + \\alpha_1) - (\\mu + \\alpha_3) \\\\ &amp; = &amp; \\alpha_1 - \\alpha_3 \\end{eqnarray*}\\] Once the linear contrasts have been re-expressed in terms of the effects model, we simply pass the coefficients of the effects parameters to SAS in the ESTIMATE or CONTRAST statement. In the examples above, the only terms included are effects associated with the classification factor ‘type’. (This will get more complicated for multi-factor ANOVA models.) Here is an example of how we might use ESTIMATE statements for \\(\\theta_1\\): proc glm data = hotdog; class type; model calorie = type; estimate &#39;Beef vs. Poultry&#39; type 1 0 -1 / e; run; The GLM Procedure Dependent Variable: calorie Standard Parameter Estimate Error t Value Pr &gt; |t| Beef vs. Poultry 38.0852941 7.73883135 4.92 &lt;.0001 Coefficients for Estimate Beef vs. Poultry Intercept 0 type Beef 1 type Meat 0 type Poul -1 Notes: The title that appears in single quotes is just a label that helps you remember which linear combination is which. SAS orders the effects parameters alphabetically. Be careful! Check to make sure that you’ve specified the order of the coefficients correctly. The “e” option at the end of the estimate statement provides a listing of the coefficients in tabular form, lined up against the model parameters. This provides a good way to double check whether your code is correct. For each linear combination, SAS provides a test of \\(H_0: \\theta = 0\\) vs. \\(H_0: \\theta \\ne 0\\). Of course, you can use the SAS output to calculate CIs or to conduct other hypothesis tests of interest. As another example, consider the linear combination \\(\\theta_2 = \\mu_1\\). Expressed in terms of the effects-model parameterization, this combination is simply \\(\\theta_2 = \\mu + \\alpha_1\\). SAS calls the reference level the ‘intercept’, so we could find this linear combination using the ESTIMATE statement as: proc glm data=hotdog; class type; model calorie = type; estimate &#39;Beef mean&#39; intercept 1 type 1 0 0; run; Standard Parameter Estimate Error t Value Pr &gt; |t| Beef mean 156.850000 5.24564602 29.90 &lt;.0001 An unfortunate consequence of using the effects-model parameterization is that it is possible to write down linear combinations of the effects parameters that do not correspond to a linear combination of the group means. We say that these functions are “not estimable”. For example, consider the function \\(\\mu + \\alpha_1 + \\alpha_2\\). (An even simpler example of a function that is not estimable is just \\(\\alpha_1\\).) Even though we can write this function down as a mathematical formula, its value will depend on the constraint that we choose to estimate the model parameters. However, we have said that the choice of a constraint is arbitrary. Therefore, we are only interested in functions whose values are the same, regardless of the constraint chosen. If you try to estimate a function that is not estimable in SAS, SAS will produce an error message. Of course, if we start with a linear combination of group means, and then convert that combination to its representation with the effects model parameters, we’ll be guaranteed to have an estimable function. So, it’s best to always start with the group means, and then work from the group means to the parameters of the effect model, instead of starting directly with the parameters of the effects model. Bibliography Benjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” Journal of the Royal Statistical Society: Series B (Methodological) 57 (1): 289–300. Holm, Sture. 1979. “A Simple Sequentially Rejective Multiple Test Procedure.” Scandinavian Journal of Statistics, 65–70. Moore, David S, and George P McCabe. 1989. Introduction to the Practice of Statistics. WH Freeman. Quinn, Gerry P, and Michael J Keough. 2002. Experimental Design and Data Analysis for Biologists. Cambridge university press. Sokal, Robert R, and F James Rohlf. 1995. Biometry. 3rd ed. New York: W.H. Freeman. As with the SS decomposition that lead to \\(R^2\\) in an ANOVA, the SS decomposition here is actually the Pythagorean Theorem in action, albeit in a higher-dimensional space.↩︎ The sums-of-squares for orthogonal polynomial contrasts can also be found by fitting polynomial regression models of increasingly higher order. It will turn out that the decrease in the SSE (or increase in the model sum-of-squares) obtained by increasing the degree of the polynomial by one exactly equals the sum-of-squares for the corresponding polynomial trend. For example, the sum-of-squares associated with the quadratic contrast can also be found by fitting linear and quadratic regression models, and taking the difference in the model sum-of-squares between the two fits.↩︎ When the data are not balanced, another choice for the sum-to-zero constraint is to set \\(\\mu\\) equal to the grand mean, where the grand mean is weighted by the differences in sample sizes among the groups: \\(\\mu = \\frac{1}{n_T} \\sum_{i=1}^g n_i \\mu_i\\). With this definition of \\(\\mu\\), the weighted sum of the \\(\\alpha_i\\)’s will equal zero: \\(\\sum_{i=1}^g n_i \\alpha_i = 0\\).↩︎ "],["factorial-experiments.html", "Chapter 7 Factorial experiments 7.1 Combining factors: Crossed vs. nested designs 7.2 Two-factor ANOVA 7.3 Unreplicated factorial designs 7.4 More than two factors 7.5 Analysis using PROC GLM in SAS", " Chapter 7 Factorial experiments In this section, we consider experiments where the treatment structure involves multiple experimental factors. 7.1 Combining factors: Crossed vs. nested designs Experimental factors can be combined in two different ways. The distinction is illustrated by the following two experiments. Experiment 1. (A hypothetical experiment based on example 15.8 in Ott &amp; Longnecker): A citrus orchard contains 3 different varieties of citrus trees. Eight trees of each variety are randomly selected from the orchard. Four different pesticides are randomly assigned to two trees of each variety and applied according to recommended levels. The same four pesticides are used for each variety. Yields of fruit (in bushels per tree) are recorded at the end of the growing season. Experiment 2. A study is conducted to investigate the effect of pest management practices on cotton in the central valley of California. 14 ranches are available for study. Each of the 14 ranches is managed by one consultant. In both experiments, there are two experimental factors—variety and pesticide in experiment 1, and consultant and ranch in experiment 2. Each unique combination of factors forms a separate treatment combination. In experiment 1, the treatment combinations are formed by crossing the two experimental factors. That is to say, every level of the first factor (variety) is combined with every level of the second factor (pesticide). This is an example of a factorial or crossed design. In experiment 2, each level of one factor (ranch) is only combined with one single level of the other factor (consultant). This is an example of a hierarchical design, and we would say that ranch is nested within the consultant. Experiments with more than two factors can give rise to designs that involve aspects of both crossed and nested designs. For example, if we three factors—call them factors “A”, “B”, and “C”—we might cross factors A and B and then nest factor C within the A*B cross. The remainder of this chapter concerns factorial designs. We will begin by studying a two-factor cross. Most of the ideas involved in analyzing factorial designs can be mastered by studying a two-factor cross. We will also study a three-factor design to see how ideas from two-factor designs extend to experiments with more than two factors. The analysis of factorial designs makes heavy use of the idea of contrasts, so make sure you understand that material well. 7.2 Two-factor ANOVA In a two-factor design, the two factors can be generically labeled as factors “A” and “B”. The scientific questions of most interest in a two-factor design are: Does the average response differ among the levels of factor A, and if so, how? Does the average response differ among the levels of factor B, and if so, how? Do the differences among the levels of factor A depend on the level of factor B, and vice versa? If so, how? That is to say, is there evidence of an interaction between the two factors? Example: Oleoresin collected from pine trees. Oehlert (problem 8.5) reports the following data. Low and Bin Mohd. Ali (1985) studied the collection of pine oleoresin by tapping the trunks of pine trees. Tapping involves cutting a hole in the tree trunk and collecting resin that seeps out. This experiment compared four shapes of holes (circle, diagonal, check, or rectangle) and the effect of adding acid (added vs. control) in collecting resin. Twenty-four pine trees were selected from a plantation and were randomly assigned to each of the 8 possible combinations of hole shape and acid. The response is total grams of resin collected from the hole. The data that we will work with are not the actual data but are instead a hypothetical data set for the same design. This is a balanced, replicated 4 \\(\\times\\) 2 factorial experiment with treatment combinations assigned in a CRD. 7.2.1 Interaction plots For a two-way layout, it is often convenient to visualize the differences among the treatment means with an interaction plot. (Others call these profile plots.) To construct an interaction plot, write the levels of one factor along the horizontal axis, and use the vertical axis to denote the response. Draw lines on the plot for each level of the other experimental factor. Here is an interaction plot for the oleoresin data: Figure 7.1: Interaction plot for the pine resin data 7.2.2 Getting organized: Notation and bookkeeping Organization is everything in the analysis of factorial experiments. Organize, organize, organize! To aid in organization, the analysis of factorial designs involves some fairly heavy notation. It’s easy to get lost in the notation. Remember that the notation is not the point. The notation is merely a bookkeeping device for keeping ourselves organized. Here is some notation that we will use for two-factor experiments: \\(a\\): number of levels of factor “A” (for the resin data, we’ll set “acid” as this factor, so \\(a = 2\\).) \\(b\\): number of levels of factor “B” (for the resin data, we’ll set “shape” as this factor, so \\(b = 4\\).) \\(i = 1, 2, \\ldots, a\\): an index to distinguish the different levels of factor A (in the resin data, \\(i = 1\\) for the control, \\(i = 2\\) for adding acid) \\(j = 1, 2, \\ldots, b\\): an index to distinguish the different levels of factor B (for the resin data, \\(j = 1\\) for circles, \\(j = 2\\) for diagonal incisions, \\(j=3\\) for checkmark incisions, and \\(j=4\\) for rectangular incisions) \\(n_{ij}\\): sample size for the combination of level \\(i\\) of factor A and level \\(j\\) of factor B (in a balanced design, sometimes this gets replaced by \\(n\\)). \\(k = 1, 2, \\ldots, n_{ij}\\): an index to distinguish the different replicates within each treatment combination \\(y_{ijk}\\): \\(k\\)th replicate from the combination of level \\(i\\) of factor A and level \\(j\\) of factor B. \\(n_T =\\sum _{i=1}^{a}\\sum _{j=1}^{b}n_{ij}\\) : total sample size \\(\\bar{y}_{ij+} =\\dfrac{\\sum _{k=1}^{n_{ij}} y_{ijk}}{n_{ij}}\\): sample mean for the combination of level \\(i\\) of factor A and level \\(j\\) of factor B \\(\\mu_{ij}\\) : unknown population mean for the combination of level \\(i\\) of factor A and level \\(j\\) of factor B. Sometimes called a “cell mean” because we might think of each treatment combination as corresponding to a “cell” in a table in which each factor corresponds to one of the table’s dimensions. A marginal mean is the average of all the cell means associated with one level of one factor, averaged over all the levels of the other factor(s). For example, in the resin data, the marginal mean associated with the control (no-acid-added) treatment is \\[ \\bar{\\mu}_{1+} = \\dfrac{\\mu_{11} + \\mu_{12} + \\mu_{13} + \\mu_{14}}{4}. \\] Here, we’ve used a plus (+) subscript to indicate that we are summing over the levels of an index. The marginal mean for the acid addition treatment, \\(\\bar{\\mu}_{2+}\\), is defined similarly. The marginal mean associated with the circular incisions is \\[ \\bar{\\mu}_{+1} = \\dfrac{\\mu_{11} + \\mu_{21}}{2}. \\] The marginal means for the other three incision shapes are defined similarly. 7.2.3 ANOVA \\(F\\)-tests If all we understood was how to analyze data from a one-way layout, we could conceivably analyze the \\(a \\times b\\) factorial cross using a one-factor ANOVA with \\(a \\times b\\) different treatment groups. This analysis would be unsatisfying, however, because it fails to capitalize on the additional structure provided by the factorial arrangement of the experiment treatments. The two-factor ANOVA partitions the \\(ab-1\\) free differences among the \\(ab\\) treatment means into three contrasts: a contrast of the \\(a\\) marginal means for factor \\(A\\) (itself involving \\(a-1\\) free differences), a contrast of the \\(b\\) marginal means for factor \\(B\\) (requiring \\(b-1\\) free differences), and a contrast that captures the interaction between the two factors (requiring the remaining \\((a-1)(b-1)\\) free differences. In this way, the two-factor ANOVA goes beyond merely testing if the \\(ab\\) treatment means differ, and instead tests if the treatment means differ in specific ways suggested by the factorial design. ANOVA contrast df Factor A marginal means \\(a-1\\) Factor B marginal means \\(b-1\\) A*B interaction \\((a-1)(b-1)\\) In the usual way, testing each of these contrasts entails computing a sum-of-squares associated with each, standardizing that sum-of-squares by its associated df to generate a mean-square, and then comparing the mean-square for the contrast to the MSE to generate an \\(F\\)-statistic. When the data are balanced, the ANOVA contrasts are orthogonal, and so the sum-of-squares associated with the three contrasts sum to give the sum-of-squares for the groups, that is, \\[ SS_{Groups} = SS[A] + SS[B] + SS[AB]. \\] When the data are not balanced, this formula does not hold, but that doesn’t hinder our analysis. As with one-factor ANOVA, the two-factor ANOVA provides an entry point into the analysis. The ANOVA itself is rarely enough to fully characterize the interesting patterns in the data. To fully analyze the data, the two-factor ANOVA should be followed by customized contrasts of the cell means, multiple comparisons procedures, etc., as we did in one-factor ANOVA. Before proceeding, let’s analyze the contrasts in the two-factor ANOVA in more detail. 7.2.3.1 Comparisons of marginal means The two-factor ANOVA provides \\(F\\)-tests to test for differences among the marginal means of each factor. In the resin example, one \\(F\\)-test compares the marginal means for control and acid-addition treatments: \\[ H_0: \\bar{\\mu}_{1+} =\\bar{\\mu}_{2+}. \\] A second \\(F\\)-test tests the null hypothesis that there is no difference between the marginal means for the four shapes: \\[ H_0: \\bar{\\mu}_{+1} = \\bar{\\mu}_{+2} = \\bar{\\mu}_{+3} = \\bar{\\mu}_{+4}. \\] While it may not be immediately obvious, notice that each of these null hypotheses corresponds to a (possibly complex) contrast of the cell means. For example, our test for a difference between the marginal means for the acid-addition treatment can be re-expressed as the (simple) contrast \\[ \\begin{align} \\theta &amp; = \\bar{\\mu}_{1+} -\\bar{\\mu}_{2+} \\\\ &amp; = \\dfrac{1}{4}\\mu_{11} + \\dfrac{1}{4}\\mu_{12} + \\dfrac{1}{4}\\mu_{13} + \\dfrac{1}{4}\\mu_{14} - \\dfrac{1}{4}\\mu_{21} - \\dfrac{1}{4}\\mu_{22} - \\dfrac{1}{4}\\mu_{23} - \\dfrac{1}{4}\\mu_{24}. \\end{align} \\] On the other hand the test for the equality of the four marginal means for the shape treatment is a complex contrast consisting of three linearly independent simple contrasts. One such complex contrast is \\[ \\begin{align} \\theta_1 &amp; = \\bar{\\mu}_{+1} -\\bar{\\mu}_{+2} \\\\ &amp; = \\dfrac{1}{2}\\mu_{11} + \\dfrac{1}{2}\\mu_{21} - \\dfrac{1}{2}\\mu_{12} - \\dfrac{1}{2}\\mu_{22} \\\\ \\theta_2 &amp; = \\bar{\\mu}_{+1} -\\bar{\\mu}_{+3} \\\\ &amp; = \\dfrac{1}{2}\\mu_{11} + \\dfrac{1}{2}\\mu_{21} - \\dfrac{1}{2}\\mu_{13} - \\dfrac{1}{2}\\mu_{23} \\\\ \\theta_3 &amp; = \\bar{\\mu}_{+1} -\\bar{\\mu}_{+4} \\\\ &amp; = \\dfrac{1}{2}\\mu_{11} + \\dfrac{1}{2}\\mu_{21} - \\dfrac{1}{2}\\mu_{14} - \\dfrac{1}{2}\\mu_{24}. \\end{align} \\] We will sometimes refer to comparisons of marginal means as the main-effects comparisons associated with a particular factor. 7.2.3.2 Interaction To think through the interaction, it is helpful to have a bit more terminology at our disposal. We define the simple effect of a factor as the comparisons among the levels of one factor within the level(s) of the other factor(s). For example, in the resin experiment, the simple effect of adding acid to circular incisions is the comparison between circular incisions with acid added vs. circular incisions without acid added. In our notation, this corresponds to the difference \\[ \\theta_{\\mbox{acid:circle}} = \\mu_{21} - \\mu_{11}. \\] Note that this simple-effect comparison is a simple contrast of the cell means. Similarly, we can define the simple effect of acid addition for diagonal incisions as the comparison between diagonal incisions with acid added vs. diagonal incisions without acid added. \\[ \\theta_{\\mbox{acid:diagonal}} = \\mu_{22} - \\mu_{12}. \\] The simple effect of adding acid to checkmark (\\(j=3\\)) and rectangular (\\(j=4\\)) incisions can be defined similarly. An interaction is a comparison of simple effects. In the resin data, the ANOVA \\(F\\)-test of the interaction corresponds to a test of \\[ H_0: \\theta_{\\mbox{acid:circle}} = \\theta_{\\mbox{acid:diagonal}} = \\theta_{\\mbox{acid:checkmark}} = \\theta_{\\mbox{acid:rectangle}} \\] or \\[ H_0: \\mu_{21} - \\mu_{11} = \\mu_{22} - \\mu_{12} = \\mu_{23} - \\mu_{13} = \\mu_{24} - \\mu_{14}. \\] This is also a complex contrast. In the case of the resin data, it is a complex contrast that consists of three linearly independent simple contrasts. Just as with regression, interactions go both ways: if the effect of adding acid differs among the shapes, then the differences among the shapes must also depend on whether acid was added or not. The interaction plots in Figure 7.1 are helpful for visualizing what is meant by an interaction. The simple effect of adding acid corresponds to the blue-vs-red comparison for a particular shape. The simple effect of shape corresponds to the left-vs-right comparison for one acid treatment or the other. The interaction asks if these simple effects of adding acid are the same for the different shapes, or, conversely, if the simple effects of shape are the same regardless of whether acid was added. Notice also that we can also define the main-effects comparisons as the averages of the simple effects. For example, the difference between the marginal means of the acid-addition treatment is the average of the simple effects for the acid addition treatment, averaged across shapes. With this recognition comes a crucial point: If the interaction is significant, then we need to be careful about analyzing the main effects, because the main effects are averaging over simple effects that differ. Thus, the usual guidance for breaking down a two-factor ANOVA table is to inspect the interaction first. If the interaction is not significant, proceed to analyze main effects of each factor. If the interaction is significant, then analyze simple effects, and interpret the main-effects comparison cautiously. 7.2.3.3 Back to the resin data Let’s see how this plays out with the pine resin data. We’ll let the computer calculate the various SS that comprise the ANOVA table for us. For the oleoresin data, we obtain source df SS Shape 3 19407 Acid 1 1305 Shape*Acid 3 237 Error 16 721 Total 23 21672 Now, we can use this sum-of-squares breakdown to test for the statistical significance of the interaction and/or the main effects. As always, tests of contrasts involve computing an \\(F\\)-ratio. In the two-factor ANOVA, the \\(F\\)-ratios for the main-effects comparisons are \\[ F = MS[A] / MSE \\] and \\[ F = MS[B] / MSE. \\] and the \\(F\\)-statistics to test for the interaction is \\[ F = MS[AB] / MSE. \\] the usual way, all \\(F\\)-tests yield one-tailed \\(p\\)-values. All of the information for these tests can be compiled into an ANOVA table: source df SS MS \\(F\\) \\(p\\) Shape 3 19407 6469 143.5 &lt;0.0001 Acid 1 1305 1305 29 &lt;0.0001 Shape*Acid 3 237 79.2 1.76 0.1961 Error 16 721 45.1 Total 23 21672 Thus, this two-factor ANOVA shows that there is no statistical evidence of an interaction (\\(F_{3, 16} = 1.76\\), \\(p = 0.20\\)). Because the interaction is not significant, it makes sense to analyze main effects. There is very strong evidence of a main effect of shape (\\(F_{3, 16} = 143.5\\), \\(p &lt; 0.0001\\)) and a main effect of the acid treatment (\\(F_{1, 16}\\) = 29.0, \\(p &lt; 0.0001\\)). Because the main effects are significant (and the interaction is not significant), then we can use linear contrasts and multiple comparisons procedures to analyze the marginal means of both factors just as we would in a one-factor layout. For example, we could use a MEANS statement to compare treatment means using multiple comparisons. Here is code for a Tukey’s HSD comparison of the marginal means for the hole shapes: proc glm data=resin; class shape acid; model resin = shape acid shape*acid; means shape / tukey; run; Tukey&#39;s Studentized Range (HSD) Test for resin Means with the same letter are not significantly different. Tukey Grouping Mean N shape A 90.333 6 rectangl B 73.000 6 check C 57.500 6 diagonal D 13.667 6 circular Thus, all of the shapes are significantly different from one another. Just as with a one-factor ANOVA, the sums-of-squares in a balanced two-factor ANOVA can be computed using computing formulas that involve nothing more than addition, subtraction, multiplication, and division. These computing formulas were more important in the days before desktop computing. While there may be some small historical value to seeing how this worked, we won’t be computing these quantities by hand today. The formulas follow for completeness, but don’t feel compelled to study them. The computing formulas begin with the usual one-factor SS decomposition: \\[\\begin{eqnarray*} \\mbox{Total variation: } SS_{Total} &amp; = &amp; \\sum_{i=1}^{a}\\sum_{j=1}^{b}\\sum_{k=1}^{n_{ij} }\\left(y_{ijk} -\\bar{y}_{+++} \\right)^2 \\\\ \\mbox{Variation among groups: } SS_{Groups} &amp; = &amp; \\sum_{i=1}^{a}\\sum_{j=1}^{b}\\sum_{k=1}^{n_{ij} }\\left(\\bar{y}_{ij+} -\\bar{y}_{+++} \\right)^2 \\\\ \\mbox{Variation within groups: } SS_{Error} &amp; = &amp; \\sum_{i=1}^{a}\\sum_{j=1}^{b}\\sum_{k=1}^{n_{ij} }\\left(y_{ijk} -\\bar{y}_{ij+} \\right)^2 \\end{eqnarray*}\\] Next, these formulas decompose the \\(SS_{Groups}\\) into three separate SS: one for each of the two main effects, and one for the interaction. Formulas for these sum-of-squares are: \\[\\begin{eqnarray*} SS[A] &amp; = &amp; \\sum_{i=1}^{a}\\sum_{j=1}^{b}\\sum_{k=1}^{n_{ij} }\\left(\\bar{y}_{i++} -\\bar{y}_{+++} \\right)^2 \\\\ SS[B] &amp; = &amp; \\sum_{i=1}^{a}\\sum_{j=1}^{b}\\sum_{k=1}^{n_{ij} }\\left(\\bar{y}_{+j+} -\\bar{y}_{+++} \\right)^2 \\\\ SS[AB] &amp; = &amp; \\sum_{i=1}^{a}\\sum_{j=1}^{b}\\sum_{k=1}^{n_{ij} }\\left(\\bar{y}_{ij+} -\\bar{y}_{i++} -\\bar{y}_{+j+} +\\bar{y}_{+++} \\right)^2 \\end{eqnarray*}\\] where SS[AB] denotes the sum-of-squares for the interaction. The formulas for SS[A] and SS[B] should make some sense: they consist of squared differences between the marginal means for one level of an experimental factor and the grand mean. The formula for SS[AB] is a bit more mysterious. Heuristically, the idea is this: with some algebra, the null hypothesis for no interaction \\(H_0\\): \\(\\mu_{ij} -\\bar{\\mu}_{++} =\\bar{\\mu}_{+i} -\\bar{\\mu}_{++} +\\bar{\\mu}_{+j} -\\bar{\\mu}_{++}\\) can be re-written as \\(H_0\\): \\(\\mu_{ij} -\\bar{\\mu}_{+i} -\\bar{\\mu}_{+j} +\\bar{\\mu}_{++} =0\\). Thus, the term \\(\\bar{y}_{ij+} -\\bar{y}_{i++} -\\bar{y}_{+j+} +\\bar{y}_{+++}\\) measures the extent to which the mean of group \\(ij\\) departs this null hypothesis. In ANOVA, we typically don’t remove non-significant interactions from the model, as we would in regression. The reasoning is that when we fit a “saturated” or “full-factorial” ANOVA to a factorial experiment (a “saturated” or “full-factorial” ANOVA model is one that includes all possible main effects and interactions), then the error sum-of-squares and associated MSE provide a “pure” measure of the experimental error. (Remember that in the context of a designed experiment, the experimental error refers ot the variability among replicates of the same experimental treatment.) If we were to remove the non-significant interaction, then we would contaminate our estimate of the experimental error by combining it with the variation that we had previous attributed to the interaction. We lose more than we gain by doing so, so we typically avoid the practice. 7.2.4 An example with a significant interaction This example is taken from Steel, Torrie, and Dickey (1997). In their words, Wilkinson (1954) reports the results of an experiment to study the influence of time of bleeding and diethylstilbestrol (an estrogenic compound) on plasma phospholipid in lambs. Five lambs were assigned at random to each of four treatment groups; treatment combinations are for morning and afternoon bleeding and with and without diethylstilbestrol treatment. An interaction plot of the data is shown below. Here is the output of a two-factor ANOVA model using PROC GLM: proc glm data=sheep; class time drug trt; model y = time|drug; run; Sum of Source DF Squares Mean Square F Value Pr &gt; F Model 3 1539.406600 513.135533 21.61 &lt;.0001 Error 16 379.923280 23.745205 Corrected Total 19 1919.329880 Source DF Type III SS Mean Square F Value Pr &gt; F time 1 1256.746580 1256.746580 52.93 &lt;.0001 drug 1 8.712000 8.712000 0.37 0.5532 time*drug 1 273.948020 273.948020 11.54 0.0037 In contrast to the rat example, the interaction here is statistically significant. Because the interaction is significant, the \\(F\\)-tests of the main effects may no longer have a clear interpretation. Instead, we’ll analyze the simple effects of the two factors. In this case, because each factor only involves two levels, the simple effects can be captured simple (e.g., single df) contrasts. With simple contrasts, it is often more informative to compute an estimate of the contrast instead of merely testing it. The PROC GLM code below uses ESTIMATE statements to compute an estimate of the simple contrast associated with each simple effect: proc glm data=sheep; class time drug trt; model y = time|drug; estimate &#39;Simple effect of time, drug=no&#39; time 1 -1 time*drug 1 0 -1 0; estimate &#39;Simple effect of time, drug=yes&#39; time 1 -1 time*drug 0 1 0 -1; estimate &#39;Simple effect of drug, time=AM&#39; drug -1 1 time*drug -1 1 0 0; estimate &#39;Simple effect of drug, time=PM&#39; drug -1 1 time*drug 0 0 -1 1; run; Parameter Estimate Error t Value Pr &gt; |t| Simple effect of time, drug=no -23.2560000 3.08189585 -7.55 &lt;.0001 Simple effect of time, drug=yes -8.4520000 3.08189585 -2.74 0.0145 Simple effect of drug, time=AM 6.0820000 3.08189585 1.97 0.0660 Simple effect of drug, time=PM -8.7220000 3.08189585 -2.83 0.0121 Here is a partial interpretation of these contrasts. Sheep with blood drawn in the afternoon have more plasma phospholipid than sheep with blood drawn in the morning, regardless of whether the sheep were given the drug. However, the magnitude of the effect of timing is smaller on sheep given the drug (estimated effect = 8.5 units, s.e. = 3.1) than it is on sheep not given the drug (estimated effect = 23.3 units, s.e.=3.1). For sheep with blood drawn in the afternoon, the drug decreases plasma phospholipid relative to the control (estimated effect = 8.7 units less with the drug, s.e. = 3.1). For sheep with blood drawn in the morning, there is only weak evidence that the drug has an effect on plasma phospholipid (estimated effect = 6.1 units more with the drug, s.e. = 3.1, \\(p=0.066\\)). 7.2.5 The effects model for a two-factor ANOVA Just as with one-factor ANOVA, there is an alternative parameterization of the two-factor ANOVA model that uses so-called effects parameters. This parameterization is a bit more useful with the two-factor ANOVA than it was with the one-factor ANOVA. The effects-model parameterization of the two-factor ANOVA re-expresses each cell mean as the sum of a collection of effects: \\[ \\mu_{ij} =\\mu +\\alpha_i +\\beta_j +\\left(\\alpha \\beta \\right)_{ij} \\] Here, \\(\\mu\\) is the reference level, \\(\\alpha_i\\) is the “effect” of level \\(i\\) of factor A, \\(\\beta_j\\) is the “effect” of level \\(j\\) of factor B, and \\(\\left(\\alpha \\beta \\right)_{ij}\\) is the interaction between level \\(i\\) of factor A and level \\(j\\) of factor B. In one-factor ANOVA, we saw that it was not possible to estimate all the \\(\\alpha_i\\)’s uniquely, so we had to impose a constraint. A similar phenomenon prevails in the two-factor model. How many constraints do we need? The key equivalence is that the number of effects parameters that we can estimate is equal to the number of df for each effect in the df accounting. Thus, while the effects model includes \\(a\\) different \\(\\alpha_i\\) parameters, we can only estimate \\(a-1\\) of these parameters uniquely, and thus we require a single constraint. For the interaction parameters, the effects model includes \\(a \\times b\\) different parameters, but we only have \\((a-1) \\times (b-1)\\) df available for the interaction, so we need \\(ab - (a-1)(b-1) = a + b -1\\) constraints. Perhaps the most common choice of constraints is the set-to-zero constraint, in which some of the effects parameters are forced to equal zero. Just as with one-factor ANOVA, the set-to-zero constraints are tantamount to building indicator variables in a regression analysis (the effects parameter associated with the baseline, or reference, level is the parameter set to 0). One advantage to the effects model is that it makes it easier to write down the null hypotheses associated with the ANOVA \\(F\\)-tests. The ANOVA \\(F\\)-test associated with comparison of the marginal means of factor A writes as \\[ H_0: \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_a = 0. \\] Similarly, the ANOVA \\(F\\)-test associated with comparison of the marginal means of factor A writes as \\[ H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_b = 0. \\] Finally, the ANOVA \\(F\\)-test associated with the interaction writes as \\[ H_0: (\\alpha \\beta)_{11} = (\\alpha \\beta)_{12} = \\ldots = (\\alpha \\beta)_{ab} = 0. \\] The effects-model parameterization will have additional uses moving forward, for example, when we consider unreplicated or blocked designs. 7.3 Unreplicated factorial designs Consider an experiment with 5 levels of factor A, 3 levels of factor B, and a single observation for each treatment combination. This is called an unreplicated design because there is only a single replicate for each treatment combination. Let’s try a df accounting for a model that includes main effects of both factors and an interaction: source df Factor A 4 Factor B 2 A*B interaction 8 Error 0 Total 14 This model has no df remaining to estimate the experimental error. Consequently, we cannot estimate \\(MS_{Error}\\) and hence we cannot conduct \\(F\\)-tests of the treatment effects. One option with unreplicated designs is to assume that there is no interaction between the two experimental factors. A model without an interaction is called an additive model. In effects notation, the model is: \\[ y_{ijk} =\\mu +\\alpha_i +\\beta_j +\\varepsilon _{ijk} \\] With an additive model, we use the df that had been allocated to the interaction to estimate the experimental error instead: source df Factor A 4 Factor B 2 Error 8 Total 14 The additive model can be used to test for effects of factors A and B. Obviously, these tests are only trustworthy if the assumption of no interactions is appropriate. In biology, it is usually risky to assume that there are no interactions between experimental factors. Additive models for unreplicated designs are more common in industrial statistics. John Tukey developed a test for additivity with unreplicated factorial designs, sometimes called Tukey’s single degree-of-freedom test. We will not cover Tukey’s test in ST512, although you may want to read about it on your own if you need to analyze an unreplicated factorial design in your own research. 7.4 More than two factors All of these ideas can be extended to factorial experiments with more than two factors. Example (based off of an example in Rao (1998)): An investigator is interested in understanding the effects of water temperature (cold vs. lukewarm vs. warm), light (low vs. high), and water movement (still vs. flowing) on weight gain in fish. She has 24 aquaria to serve as experimental units. Each of the 3 x 2 x 2 = 12 treatment combinations are randomly assigned to 2 of the 24 aqauria, and the average weight gain of the fish in each aquaria is measured. This is a balanced three-way factorial design with a CRD randomization structure. As a first attempt to get a handle on these data, let’s make three different interaction plots, one for each water temperature: The interaction plot suggests that for some water temperatures, there is an interaction between light levels and water movement. Thus, the way in which the effect of light depends on water movement depends in turn on temperature. Yikes! This is a three-factor interaction. We need a test to see if this interaction is statistically significant, or if it can be attributed to experimental error. To develop notation for the three-factor model, we’ll extend our ideas from two factor models. For example, \\(\\mu_{ijk}\\) will denote the unknown population mean for the combination of level \\(i\\) of factor A, level \\(j\\) of factor B, and level \\(k\\) of factor C. With three factors, there are two possible types of interactions: First-order interactions}: Interactions between two factors Second-order interactions}: Interactions between first-order interactions For example, in this experiment the first-order interaction between light level and water movement might describes how the effect of light depends on water movement and vice versa. The second-order interaction describes how this first order interaction may in turn depend on water temperature. Note that the rules for determining the df associated with higher-order interactions are the same as for first-order interactions: the df are always equal to the product of the df associated with each constituent factor. (Think of this in terms of regression with indicator variables again.) In effects notation, we can write the cell means as \\[ \\mu_{ijkl} =\\mu +\\alpha_i +\\beta_j +\\gamma _{k} +\\left(\\alpha \\beta \\right)_{ij} +\\left(\\alpha \\gamma \\right)_{ik} +\\left(\\beta \\gamma \\right)_{jk} +\\left(\\alpha \\beta \\gamma \\right)_{ijk} \\] Here, the parameters denoted by \\(\\left(\\alpha \\beta \\gamma \\right)_{ijk}\\) capture the second-order interaction among the three factors. proc glm data=fishgrowth; class light temp movement; model gain = light|temp|movement; run; Example of 3x2x2 factorial from Rao 2 Dependent Variable: gain Sum of Source DF Squares Mean Square F Value Pr &gt; F Model 11 17.94458333 1.63132576 5.60 0.0030 Error 12 3.49500000 0.29125000 Corrected Total 23 21.43958333 Source DF Type III SS Mean Square F Value Pr &gt; F light 1 2.10041667 2.10041667 7.21 0.0198 temp 2 7.64333333 3.82166667 13.12 0.0010 light*temp 2 0.64333333 0.32166667 1.10 0.3629 movement 1 2.47041667 2.47041667 8.48 0.0130 light*movement 1 1.35375000 1.35375000 4.65 0.0521 temp*movement 2 2.89333333 1.44666667 4.97 0.0268 light*temp*movement 2 0.84000000 0.42000000 1.44 0.2746 The analysis strategy with a three-way factorial design is similar to the analysis strategy with a two-way factorial design: Test for the significance of the second-order interaction. If the second-order interaction is significant, either unpack the factorial treatment structure and treat the design a one-factor ANOVA, or ``divide and conquer’’ by analyzing the effects of two factors at each level of the third factor. If the second-order interaction is not significant, you may remove the second-order interaction and re-fit the model, although this is not necessary. Test for the significance of the first-order interactions. If any of the first-order interactions are significant, analyze simple effects. If none of the first-order interactions are significant, analyze main effects. In the example above, the second-order interaction is not statistically significant (\\(F_{2, 12} = 1.44\\), \\(p = 0.27\\)). The only statistically significant first-order interaction is the interaction between water temperature and movement (\\(F_{2,12} = 4.97\\), \\(p = 0.027\\)). Neither of the first-order interactions involving light are statistically significant (although the light-by-movement interaction is on the border of statistical significance, \\(F_{1,12} = 4.65\\), \\(p = 0.052\\)). The main effect of light is statistically significant (\\(F_{1,12} = 7.21\\), \\(p = 0.020\\)). We could then proceed by quantifying the main effect of light with a linear combination, and quantifying the simple effects of movement at different water temperatures. Main effect of light: \\[\\theta _{light} =\\bar{\\mu}_{1++} -\\bar{\\mu}_{2++} \\] Simple effect of movement when temperature = cold: \\[\\theta _{m/C} =\\bar{\\mu}_{+11} -\\bar{\\mu}_{+12} \\] Simple effect of movement when temperature = lukewarm: \\[\\theta _{m/L} =\\bar{\\mu}_{+21} -\\bar{\\mu}_{+22} \\] Simple effect of movement when temperature = warm: \\[\\theta _{m/W} =\\bar{\\mu}_{+31} -\\bar{\\mu}_{+32} \\] A final note: unreplicated three-way factorial designs are not uncommon in the life sciences. To analyze these designs, one typically assumes that there is no second-order interaction, and uses the df that would have been absorbed by the second-order interaction as the df for error. Some will argue that higher-order interactions are rare in nature, and thus assuming that they do not occur is justified. Whether you agree with this or view it as a just-so rationalization is up to you. 7.5 Analysis using PROC GLM in SAS Consider the following example taken from Sokal and Rohlf (1995). This experiment was designed to examine differences in food consumption among rats. 6 male rats and 6 female rats were used in the experiment. Half the rats were fed fresh lard (fat), and half the rats were fed rancid fat. The response is total food consumption (in grams) over 73 days. This is a 2 \\(\\times\\) 2 factorial design with a CRD. The experiment is balanced. To use PROC GLM to compute the two-factor ANOVA table, we would use proc glm data=rat; class trt sex fat; model food = sex fat sex*fat; run; The ANOVA \\(F\\)-tests are contained in the output: Sum of Source DF Squares Mean Square F Value Pr &gt; F Model 3 65903.58333 21967.86111 15.06 0.0012 Error 8 11666.66667 1458.33333 Corrected Total 11 77570.25000 Source DF Type III SS Mean Square F Value Pr &gt; F sex 1 3780.75000 3780.75000 2.59 0.1460 fat 1 61204.08333 61204.08333 41.97 0.0002 sex*fat 1 918.75000 918.75000 0.63 0.4503 The first portion of the output above gives the sum-of-squares breakdown and associated ANOVA \\(F\\)-test if we were just treating the data as a one-factor ANOVA. That is, the \\(F\\)-test in the first table is a test of \\(H_0\\): \\(\\mu_{11} =\\mu_{12} =\\mu_{21} =...=\\mu_{ab}\\). (This is equivalent to a model-utility test in multiple regression.) The second portion of the output provides \\(F\\)-tests for the main and interaction effects. PROC GLM also provides two sum-of-squares decompositions, one which it calls Type I and another which it calls Type III. Type I and Type III sum-of-squares are identical for balanced factorial designs. They are not identical for unbalanced designs. We will discuss the differences for unbalanced designs later. In the model statement, we can use a vertical bar as shorthand for including both main effects and interactions. The code below would produce identical output to the code above: proc glm data=rat; class trt sex fat; model food = sex|fat; run; PROC GLM uses set-to-zero constraints. We can see the constraints by calling for SAS’s parameter estimates with the SOLUTION option to the MODEL statement in PROC GLM: proc glm data=rat; class trt sex fat; model food = sex|fat / solution; run; The GLM Procedure Standard Parameter Estimate Error t Value Pr &gt; |t| Intercept 535.3333333 B 22.04792759 24.28 &lt;.0001 sex female -18.0000000 B 31.18047822 -0.58 0.5796 sex male 0.0000000 B . . . fat fresh 160.3333333 B 31.18047822 5.14 0.0009 fat rancid 0.0000000 B . . . sex*fat female fresh -35.0000000 B 44.09585518 -0.79 0.4503 sex*fat female rancid 0.0000000 B . . . sex*fat male fresh 0.0000000 B . . . sex*fat male rancid 0.0000000 B . . . NOTE: The X&#39;X matrix has been found to be singular, and a generalized inverse was used to solve the normal equations. Terms whose estimates are followed by the letter &#39;B&#39; are not uniquely estimable. Now, to calculate the main effect of fat using SAS, we have to recode our linear combination in terms of the parameters in the effects model. Here goes: \\[\\begin{eqnarray*} \\theta _{fat} &amp; = &amp; \\frac{1}{2} \\left(\\mu_{12} +\\mu_{22} -\\mu_{11} -\\mu_{21} \\right) \\\\ &amp; = &amp; \\frac{1}{2} \\left(\\mu +\\alpha _{1} +\\beta _{2} +\\left(\\alpha \\beta \\right)_{12} +\\mu +\\alpha _{2} +\\beta _{2} +\\left(\\alpha \\beta \\right)_{22} -\\mu -\\alpha _{1} -\\beta _{1} -\\left(\\alpha \\beta \\right)_{11} -\\mu -\\alpha _{2} -\\beta _{1} -\\left(\\alpha \\beta \\right)_{21} \\right) \\\\ &amp; = &amp; \\frac{1}{2} \\left(2\\beta _{2} +\\left(\\alpha \\beta \\right)_{12} +\\left(\\alpha \\beta \\right)_{22} -2\\beta _{1} -\\left(\\alpha \\beta \\right)_{11} -\\left(\\alpha \\beta \\right)_{21} \\right) \\\\ &amp; = &amp; -\\beta _{1} +\\beta _{2} -\\frac{1}{2} \\left(\\alpha \\beta \\right)_{11} +\\frac{1}{2} \\left(\\alpha \\beta \\right)_{12} -\\frac{1}{2} \\left(\\alpha \\beta \\right)_{21} +\\frac{1}{2} \\left(\\alpha \\beta \\right)_{22} \\end{eqnarray*}\\] Now we can read off the coefficients from the last line of the expression above and feed them into an ESTIMATE statement. Note that this combination only involves parameters for the ‘fat’ effect and the interaction: proc glm data=rat; class trt sex fat; model food = sex|fat; estimate &#39;Fresh v. rancid&#39; fat -1 1 sex*fat -.5 .5 -.5 .5; run; Parameter Estimate Standard Error t Value Pr &gt; |t| Fresh v. rancid -142.833333 22.0479276 -6.48 0.0002 As always, SAS provides (for free) a \\(p\\)-value for the test of \\(H_0\\): \\(\\theta =0\\) vs. \\(H_a\\): \\(\\theta \\ne 0\\). For illustration’s sake, we can also write linear combinations for the main effect of sex, and for the interaction effect. One way to write these linear combinations is: \\[\\theta _{sex} =\\frac{\\mu_{21} +\\mu_{22} }{2} -\\frac{\\mu_{11} +\\mu_{12} }{2} \\] and \\[\\theta _{int} =\\left(\\mu_{22} -\\mu_{12} \\right)-\\left(\\mu_{21} -\\mu_{11} \\right)\\] Using PROC GLM, we estimate these effects as proc glm data=rat; class trt sex fat; model food = trt; estimate &#39;Fresh v. rancid&#39; fat -1 1 sex*fat -.5 .5 -.5 .5; estimate &#39;Male v. female&#39; sex -1 1 sex*fat -.5 -.5 .5 .5; estimate &#39;Interaction&#39; sex*fat 1 -1 -1 1; run; Parameter Estimate Standard Error t Value Pr &gt; |t| Fresh v. rancid -142.833333 22.0479276 -6.48 0.0002 Male v. female -35.500000 22.0479276 -1.61 0.1460 Interaction 35.000000 44.0958552 0.79 0.4503 Thus, to summarize our analysis of this experiment: There is no evidence of a statistically significant sex effect. There is strong evidence of an effect of fat on food consumption (\\(F_{1,8} = 41.97\\), \\(p=0.0002\\)). Rats fed rancid fat consumed on average 142.9g (s.e. 22.0g) less food than rats fed fresh fat. Bibliography Low, C. K., and A. R. Bin Mohd. Ali. 1985. “Experimental Tapping of Pine Oleoresin.” The Malaysian Forester 48: 248–53. Rao, Pejaver Vishwamber. 1998. Statistical Research Methods in the Life Sciences. Duxbury Press. Sokal, Robert R, and F James Rohlf. 1995. Biometry. 3rd ed. New York: W.H. Freeman. Steel, R. G. D., J. H. Torrie, and D. A. Dickey. 1997. “Principles and Procedures of Statistics: A Biometrical Approach.” Wilkinson, WS. 1954. “Influence of Diethylstilbestrol on Feeding Digestibility and on Blood and Liver Composition of Lambs.” PhD thesis, University of Wisconsin-Madison. "],["ancova.html", "Chapter 8 ANCOVA 8.1 Common-slopes model 8.2 Separate-slopes model 8.3 Further reading", " Chapter 8 ANCOVA Statistical tests compare the estimated magnitude of a treatment effect to the precision with which that effect is estimated. Often, the magnitude of a treatment effect is set by nature. Thus, if we want to increase our chances of observing a statistically significant result, we must find ways to make our estimates more precise. Statistical precision is a function of (a) the degree of replication and (b) the experimental error. Thus, precision can be improved by including more replicates or reducing experimental error. One technique for reducing experimental error and hence increasing precision is to use a covariate that accounts for heterogeneity among EUs. Inclusion of covariates in ANOVA models is called the analysis of covariance, or ANCOVA. Mathematically, ANCOVA is no different from regression with both quantitative and categorical predictors. However, the emphases differ depending on the context. In ANCOVA, our primary focus is analyzing the differences among the treatment groups. We model the relationship between the response and the covariate to gain a more precise estimate of the differences among treatment groups, but the relationship between the response and the covariate is of secondary interest at best. In regression, both quantitative and categorical predictors are on equal footing, and we have no reason to prioritize one versus the other. For a variable to qualify as a covariate, it is important that it not be affected or influenced by the treatment itself. If the covariate is affected by the treatment, it is best to treat the covariate as a separate response. 8.1 Common-slopes model Example (from Hanley and Shapiro (1994)): Partridge and Farquhar (1981) conducted an experiment to determine if reproduction reduces longevity in male fruitflies. (Such a cost had already been established for females.) There were 5 experimental treatments: male flies reared alone, male flies reared with 1 or 8 non-mated females, and male flies reared with 1 or 8 recently mated females. 25 male flies were assigned to each treatment. The data recorded are longevity (days lived) and thorax length. The data are shown below: Suppose we analyze these data with a one-way ANOVA and ignore differences in the sizes of the flies: proc glm; class trt; model life = trt; run; The GLM Procedure Dependent Variable: life Sum of Source DF Squares Mean Square F Value Pr &gt; F Model 4 11939.28000 2984.82000 13.61 &lt;.0001 Error 120 26313.52000 219.27933 Corrected Total 124 38252.80000 Source DF Type III SS Mean Square F Value Pr &gt; F trt 4 11939.28000 2984.82000 13.61 &lt;.0001 Although the \\(F\\)-test for treatment is significant, notice that \\(MS_{Error} = 219.3\\). However, the plot above suggests that much of the variation in lifespan among flies in the same treatment group can be explained by variation in fly size. Suppose we include thorax size as a covariate: proc glm; class trt; model life = thorax trt; run; The GLM Procedure Dependent Variable: life Sum of Source DF Squares Mean Square F Value Pr &gt; F Model 5 25108.13347 5021.62669 45.46 &lt;.0001 Error 119 13144.66653 110.45938 Corrected Total 124 38252.80000 Source DF Type III SS Mean Square F Value Pr &gt; F thorax 1 13168.85347 13168.85347 119.22 &lt;.0001 trt 4 9611.49254 2402.87314 21.75 &lt;.0001 The experimental error has been cut in half: \\(MS_{Error} = 110.5\\). The experimental error has been reduced because the covariate thorax size has accounted for half of the previously unexplained variation. This is an example of an Analysis of Covariance (ANCOVA) model. We can write the model using the following mathematical notation: \\(y_{ij}\\): observation \\(j\\) from treatment group \\(i\\) \\(x_{ij}\\): value of the covariate for observation \\(j\\) from treatment group \\(i\\) Equipped with this notation, we can write the model as \\[ y_{ij} =\\mu_i + \\beta( x_{ij} - \\bar{x}_{++}) +\\varepsilon_{ij} \\] where \\(\\mu_i\\) is the adjusted treatment mean for treatment group \\(i\\), \\(\\beta\\) is a regression slope that quantifies the (linear) relationship between the covariate and the response, \\(\\bar{x}_{++}\\) is the average value of the covariate \\(x\\) (across all treatment groups), and \\(\\varepsilon_{ij}\\) is the residual error with the standard assumptions (independence, normality, equal variance). By “adjusted treatment mean”, we mean that \\(\\mu_i\\) represents the average response in treatment group \\(i\\) when the covariate \\(x\\) exactly equals the average value in the data set, \\(\\bar{x}_{++}\\). Geometrically, we can think of this model as specifying a regression line for each level of the experimental treatment. In this model, the effect of the covariate (\\(\\beta\\)) is the same for all of the treatment groups. Consequently, comparisons of adjusted treatment means do not depend on the particular value of the covariate at which the treatment means are being compared, as long as the treatment groups are all being adjusted to the same value of the covariate. Before going any further with the fly data, we observe that residual plots clearly indicate that the variance increases as the predicted response increases. (The inclusion of a covariate makes the residual plots substantially richer.) Log-transforming the response stabilizes the variance nicely: We re-do the analysis with a log-transformed response: proc glm; class trt; model loglife = thorax trt; run; The GLM Procedure Dependent Variable: loglife Sum of Source DF Squares Mean Square F Value Pr &gt; F Model 5 2.01797568 0.40359514 57.43 &lt;.0001 Error 119 0.83630432 0.00702777 Corrected Total 124 2.85428000 Source DF Type III SS Mean Square F Value Pr &gt; F thorax 1 1.03374368 1.03374368 147.09 &lt;.0001 trt 4 0.78904783 0.19726196 28.07 &lt;.0001 The residual plot looks much better: The \\(F\\)-test of trt shown above provides a test for equality of the adjusted treatment means (\\(H_0\\): \\(\\mu_1 = \\mu_2 =... = \\mu_g\\)). In the fruitfly data, there is strong evidence that the adjusted treatment means differ among the groups (\\(F_{4,119}=28.07\\), \\(p&lt;.0001\\)). There are several routes to obtaining the adjusted treatment means themselves. In PROC GLM, the LSMEANS statement generates adjusted treatment means. The PDIFF option generates \\(p\\)-values for tests of pairwise differences, and the ADJUST = TUKEY option applies an adjustment to the \\(p\\)-values to control the strong familywise type I error rate. proc glm data = fly; class trt; model loglife = trt thorax; lsmeans trt / pdiff adjust = tukey; run; The GLM Procedure Least Squares Means Adjustment for Multiple Comparisons: Tukey-Kramer loglife LSMEAN trt LSMEAN Number a 1.77070164 1 m1 1.79321646 2 m8 1.80848343 3 u1 1.71677628 4 u8 1.58882218 5 Least Squares Means for effect trt Pr &gt; |t| for H0: LSMean(i)=LSMean(j) Dependent Variable: loglife i/j 1 2 3 4 5 1 0.8771 0.5127 0.1606 &lt;.0001 2 0.8771 0.9679 0.0140 &lt;.0001 3 0.5127 0.9679 0.0019 &lt;.0001 4 0.1606 0.0140 0.0019 &lt;.0001 5 &lt;.0001 &lt;.0001 &lt;.0001 &lt;.0001 Above, we see that the adjusted treatment mean for the “alone” treatment group is 1.77 (remember this is on the log scale). The table in the second portion of the output shows that adjusted treatment mean of treatment group “u8” is significantly different from the adjusted treatment means of all other treatment groups, and the adjusted treatment mean of the “u1” group is significantly different from all groups except the “alone” group. Alternatively, a little algebra shows that the (estimate of the) adjusted treatment mean for treatment group \\(i\\) can be written as \\[ \\hat{\\mu }_i =\\bar{y}_{i+} -\\hat{\\beta }(\\bar{x}_{i+} -\\bar{x}_{++} ) \\] where \\(\\bar{y}_{i+}\\) is the raw (unadjusted) sample mean for treatment group \\(i\\), \\(\\hat{\\beta}\\) is the estimate of the covariate effect, and \\(\\bar{x}_{i+}\\) is the average of the covariate values for treatment group \\(i\\). We can find the value of \\(\\hat{\\beta }\\) using the SOLUTION option to the MODEL statement in PROC GLM. We can find the raw treatment means and the means of the covariate values using the MEANS statement: proc glm data = fly; class trt; model loglife = trt thorax / solution; means trt; run; Standard Parameter Estimate Error t Value Pr &gt; |t| Intercept 0.600921260 B 0.08112643 7.41 &lt;.0001 trt a 0.181879457 B 0.02397873 7.59 &lt;.0001 trt m1 0.204394280 B 0.02384687 8.57 &lt;.0001 trt m8 0.219661249 B 0.02371772 9.26 &lt;.0001 trt u1 0.127954099 B 0.02400289 5.33 &lt;.0001 trt u8 0.000000000 B . . . thorax 1.203348424 0.09921873 12.13 &lt;.0001 NOTE: The X&#39;X matrix has been found to be singular, and a generalized inverse was used to solve the normal equations. Terms whose estimates are followed by the letter &#39;B&#39; are not uniquely estimable. The GLM Procedure Level of -----------loglife----------- ------------thorax----------- trt N Mean Std Dev Mean Std Dev a 25 1.78880000 0.11515642 0.83600000 0.08426150 m1 25 1.79880000 0.10763828 0.82560000 0.06988562 m8 25 1.79000000 0.11221260 0.80560000 0.08155162 u1 25 1.73680000 0.13145722 0.83760000 0.07055022 u8 25 1.56360000 0.15231218 0.80000000 0.07831560 For example, consider the treatment group with flies reared alone. In these data, it turns out that the average covariate value is \\(\\bar{x}_{++} = 0.821\\). Our calculation (using the log-transformed data) gives \\[\\begin{eqnarray*} \\hat{\\mu }_i &amp; = &amp; \\bar{y}_{i+} -\\hat{\\beta }(\\bar{x}_{i+} -\\bar{x}_{++} ) \\\\ &amp; = &amp; 1.789-(1.203) \\times (0.836-0.821) \\\\ &amp; = &amp; 1.771 \\end{eqnarray*}\\] Flies assigned to the “alone” treatment were slightly larger than the other flies in the experiment. Because larger flies tend to live longer, the adjusted treatment mean for the “alone” treatment is slightly smaller than the raw mean. 8.2 Separate-slopes model If the relationship between the covariate and the response differs across the treatment groups, then we need a model that allows the regression lines to be non-parallel. Non-parallel lines can be accommodated in an ANCOVA model by including an interaction between the covariate and the treatment factor: proc glm; class trt; model loglife = thorax | trt; run; The GLM Procedure Dependent Variable: loglife Sum of Source DF Squares Mean Square F Value Pr &gt; F Model 9 2.05753663 0.22861518 33.00 &lt;.0001 Error 115 0.79674337 0.00692820 Corrected Total 124 2.85428000 Source DF Type III SS Mean Square F Value Pr &gt; F thorax 1 1.00724047 1.00724047 145.38 &lt;.0001 trt 4 0.07751033 0.01937758 2.80 0.0293 thorax*trt 4 0.03956095 0.00989024 1.43 0.2293 In notation, the non-parallel slopes model can be written: \\[ y_{ij} =\\mu_i + \\beta_i( x_{ij} - \\bar{x}_{++}) +\\varepsilon_{ij} \\] The \\(F\\)-test associated with the interaction is a test of \\(H_0\\): \\(\\beta_1 = \\beta_2 = \\ldots = \\beta_g\\), that is, a test of null hypothesis that the covariate has the same effect on the response in every group. Here, the large \\(p\\)-value indicates that there is no evidence that the effect of size on fruitfly longevity differs among the 5 treatment groups. The common-slopes model is adequate for these data. Here is an example where the association between the covariate and the response differs among the treatment groups: Example (from Milliken and Johnson (2001)): An exercise physiologist is interested in studying the effectiveness of 3 types of exercise programs. 24 males between the ages of 28 and 35 are enrolled in the study. Each individual has his heart rate measured at rest. The 24 subjects are then randomly assigned to the 3 programs (a CRD). At the end of the 8 weeks on the exercise program, each subject has his heart rate measured again after a 6-minute run. proc glm; class program; model hrate = initrate | program; run; Sum of Source DF Squares Mean Square F Value Pr &gt; F Model 5 2432.463977 486.492795 29.50 &lt;.0001 Error 18 296.869356 16.492742 Corrected Total 23 2729.333333 Source DF Type III SS Mean Square F Value Pr &gt; F initrate 1 1539.535965 1539.535965 93.35 &lt;.0001 program 2 388.117289 194.058645 11.77 0.0005 initrate*program 2 381.126973 190.563487 11.55 0.0006 When there is a significant interaction between the covariate and the treatment, then a comparison of treatments depend on the value of the covariate being considered. We might still want to compare the adjusted treatment means at the average value of the covariate in the data set, or we might select a different value of the covariate. In the LSMEANS statement, we can specify the particular value of the covariate to calculate adjusted treatment means using the AT option, as illustrated below: proc glm; class program; model hrate = initrate|program; lsmeans program / at initrate=60 stderr pdiff adjust=tukey; lsmeans program / at initrate=80 stderr pdiff adjust=tukey; run; Least Squares Means at initrate=60 Adjustment for Multiple Comparisons: Tukey-Kramer Standard LSMEAN program hrate LSMEAN Error Pr &gt; |t| Number p1 141.472996 2.007926 &lt;.0001 1 p2 153.470778 2.254376 &lt;.0001 2 p3 153.310267 1.913754 &lt;.0001 3 Least Squares Means for effect program Pr &gt; |t| for H0: LSMean(i)=LSMean(j) i/j 1 2 3 1 0.0024 0.0013 2 0.0024 0.9984 3 0.0013 0.9984 Least Squares Means at initrate=80 Adjustment for Multiple Comparisons: Tukey-Kramer Standard LSMEAN program hrate LSMEAN Error Pr &gt; |t| Number p1 164.696418 1.960674 &lt;.0001 1 p2 172.230730 1.905086 &lt;.0001 2 p3 158.585366 2.055177 &lt;.0001 3 Least Squares Means for effect program Pr &gt; |t| for H0: LSMean(i)=LSMean(j) i/j 1 2 3 1 0.0332 0.1074 2 0.0332 0.0003 3 0.1074 0.0003 Interpretation: For subjects with an initial resting heart rate of 60 bpm, there is no significant difference between exercise programs 2 and 3. For subjects with an initial resting heart rate of 80 bpm, there is no significant difference between exercise programs 1 and 3. 8.3 Further reading In these notes, we have seen simple examples of a single covariate with a one-factor layout. Of course, things can get much more complicated. Experiments can include multiple covariates, or several treatment factors, or covariates with non-linear associations with the response. See Milliken and Johnson (2001) for a lengthier treatment. Bibliography Hanley, James A, and Stanley H Shapiro. 1994. “Sexual Activity and the Lifespan of Male Fruitflies: A Dataset That Gets Attention.” Journal of Statistics Education 2 (1). Milliken, George A, and Dallas E Johnson. 2001. Analysis of Messy Data, Volume III: Analysis of Covariance. Chapman; Hall/CRC. Partridge, Linda, and Marion Farquhar. 1981. “Sexual Activity Reduces Lifespan of Male Fruitflies.” Nature 294 (5841): 580–82. "],["random-effects.html", "Chapter 9 Random effects 9.1 Fixed vs. random effects: the big picture 9.2 Random-effects models 9.3 Subsampling 9.4 \\(^\\star\\)Mathematical foundations", " Chapter 9 Random effects 9.1 Fixed vs. random effects: the big picture In statistics, we regard each data point as a single observation from an underlying distribution, or population, of possible values. A statistical model characterizes the structure of the underlying distributions from which the observed data are drawn. Loosely, we think of a statistical model as combining two distinct components. One component of the statistical model describes the mean30 of the underlying distributions, and how the mean depends on (for example) predictors in a regression model, or on the particular levels of an experimental factor in an ANOVA. The other component of the model describes how the underlying distributions vary around their respective mean, both with respect to the underlying distribution for a single data point, and the mutual relationship among the underlying distributions for several data points (i.e., their correlations). In colloquial terms, this distinction is often referred to as the “signal” and the “noise” in a statistical model. To this point, we have spent nearly all of our effort thinking about how the mean, or “signal”, in a statistical model depends on predictors, treatment groups, etc. We have not yet given much thought to the noise component of the model, beyond asserting that the residual errors were iid draws from a normal distribution with a constant variance. The concept of “random effects” will give us our first tool for building richer models for the statistical noise. So far, although we have not used the term, all of the treatment effects in the ANOVA models that we have considered have been fixed effects. We use fixed effects when we are interested in specific levels of the corresponding treatment factor. For example, in the hot dog data, were are interested in the three different types of hot dogs included in the experiment. As a second example, in the reading data, we were interested in drawing inferences about the three different teaching methods. In contrast to fixed effects, we use random effects when the levels of a factor included in an experiment can be regarded as a representative (random) sample from a population of levels, and we are interested in drawing inferences about that population, not just the specific levels included in the experiment. The residual errors that we have included in all of our models so far is an example of a random effect. For example, in the reading data, we are not interested in drawing inferences about the particular 66 students included in the study, but instead we regard these students as a random sample from a larger population, and we want to learn about that population. Similarly, in the pine-resin data, we are not interested in limiting our inferences to the particular 24 trees included in the experiment, but we regard those particular trees as a representative sample from a population of trees to which we wish to extend our inferences. One (imperfect) way to determine whether a factor should be treated with fixed or random effects is to ask: if the experiment were repeated, would the same levels of the factor be included in the experiment, or would the experiment include a potentially different set of levels? If the experiment would include the same levels, then this suggests that the experimenter is interested in drawing inferences about these particular levels, and therefore fixed effects should be used. On the other hand, if the experiment could potentially include different levels, then this suggests that the levels have been drawn from some larger population of levels, and thus random effects should be used. Whether we treat an effect as a fixed effect or a random effect determines the parameters that we estimate to characterize that effect. For fixed effects, we estimate parameters that quantify the differences among the specific levels included in the experiment. For example, with the reading data, we use parameters that allow us to draw inferences about the differences among the three reading methods in the experiment. For random effects, we estimate a variance parameter that quantifies the spread in the distribution of the population from which the levels included in the experiment were drawn. Thus, for the reading data, we also estimated an error variance that quantifies the variation in responses among students who were taught with the same method. As a bit of terminology, statistical models can be classified by whether they contain just fixed effects, just random effects, or both fixed and random effects (not counting either the intercept or the residual error term, which all models have). A model with only fixed effects is called (sensibly enough) a fixed effects model, and a model with only random effects is called a random effects model. Models with both fixed and random effects are called mixed-effects models, or sometimes just ``mixed models’’ for short. To sum up, we draw inferences about the levels of the fixed effects included in the experiment, and the scope of those inferences extends to the population(s) from which the random effects were selected. 9.2 Random-effects models Models with no fixed effects (except for the intercept) are called random-effects models. Pure random-effects models are not frequently encountered in the life sciences, but they are occasionally useful for characterizing hierarchical variability. This example is taken from the on-line SAS documentation. The description given there reads In the following example from Snedecor and Cochran (1967), an experiment is conducted to study the variability of calcium concentration in turnip greens. Four plants are selected at random; then three leaves are randomly selected from each plant. Two 100-mg samples are taken from each leaf. The amount of calcium is determined by microchemical methods. A partial data listing is shown here: plant leaf y p1 l1 3.28 p1 l1 3.09 p1 l2 3.52 p1 l2 3.48 ... p2 l1 2.46 p2 l1 2.44 ... p4 l3 3.31 p4 l3 3.31 With these data, we may want to ask: how does sample-to-sample variation within a leaf compare to lea\\(F\\)-to-leaf variation within a plant, and how do each of these sources of variation compare to plant-to-plant variation? We can answer these questions with a random-effects model. Our goal here is to quantify the sources of variation that contribute to variation among the samples. We are not interested in quantifying the differences among these particular four plants, or among these particular leaves on these particular plants, or among these particular subsamples on these particular leaves. Instead, each factor in our experiment — plant, leaf and sample — contains levels that are regarded as a random sample from a population of levels. If we were to repeat the experiment, we might choose different plants, or different leaves, or different samples. Let \\(y_{ijk}\\) be the response for sample \\(k\\) from leaf \\(j\\) of pant \\(i\\). Let’s try the model: \\[ y_{ijk} = \\mu + P_i + L_{ij} + \\varepsilon_{ijk} \\] where \\(\\mu\\) is the reference level, \\(P_i\\) is a random effect associated with plant \\(i\\), \\(L_{ij}\\) is a random effect associated with leaf \\(j\\) of plant \\(i\\), and \\(\\varepsilon_{ijk}\\) is the residual error . Because the plants and leaves have been randomly selected from populations of plants and leaves, we assume \\[\\begin{eqnarray*} P_i &amp; \\sim &amp; \\mathcal{N}\\left(0, \\sigma^2_P \\right) \\\\ L_{ij} &amp; \\sim &amp; \\mathcal{N}\\left(0, \\sigma^2_L \\right) \\\\ \\varepsilon_{ijk} &amp; \\sim &amp; \\mathcal{N}\\left(0, \\sigma^2_{\\varepsilon} \\right) \\end{eqnarray*}\\] In words, each of the random effects are (independently) drawn from normal distributions with distinct variances. Note that we can think of the residual \\(\\varepsilon_{ijk}\\) as the random effect associated with sample \\(k\\) from leaf \\(j\\) of plant \\(i\\). There’s something else going on with this experiment. Namely, the “leaf” factor and the “plant” factor are not crossed. That is, each leaf is associated with one and only one plant. Thus, the “leaf” factor is nested within the “plant” factor. Similarly, each subsample is associated with one and only one leaf, so the subsample is nested within the leaf.31 In a random effects model, our goals are not to estimate the individual \\(P_i\\)’s or \\(L_{ij}\\)’s themselves. Instead, our goal is to estimate the variance parameters (or variance components) \\(\\sigma^2_P\\), \\(\\sigma^2_L\\) and \\(\\sigma^2_{\\varepsilon}\\) . These parameters quantify the plant-to-plant variability, the lea\\(F\\)-to-leaf variability within each plant, and the sample-to-sample variability within each leaf, respectively. Here is an analysis within PROC MIXED: proc mixed; class Plant Leaf; model Calcium = ; random Plant Leaf(Plant); run; Covariance Parameter Estimates Cov Parm Estimate Plant 0.3652 Leaf(Plant) 0.1611 Residual 0.006654 In PROC MIXED, we only list fixed effects on the right-hand side of the equals sign in the MODEL statement. In this model, there are no fixed effects, so nothing is placed on the right-hand side of the equals sign. (The reference level is a fixed effect, but it is included in every model, so it is never specified.) In the RANDOM statement, we list the random effects. Here, we include random effects for the individual plants and for the leaves nested within plants. Note that SAS uses the parenthetical notation to denote nesting as well, so that we write LEAF(PLANT) to tell SAS that the leaf factor is nested within the plant factor. We do not have to specify a random effect for the sample error, because the sample error is equivalent to the residual error, and this is always included in the model as well. Our parameter estimates are \\(\\hat{\\sigma}^2_P = 0.3652\\), \\(\\hat{\\sigma}^2_L = 0.1611\\), and \\(\\hat{\\sigma}^2_{\\varepsilon} = 0.0067\\). Thus, we conclude that plant-to-plant variation is roughly twice as large as lea\\(F\\)-to-leaf variation within plants, and that both of these are much larger than sample-to-sample variation within leaves. It is possible to generate an ANOVA table for the random-effects model. We could define sums-of-squares for each of the model terms in the following way: \\[\\begin{eqnarray*} SS(Plant) &amp; = &amp; \\sum_{i=1}^4 \\sum_{j=1}^3 \\sum_{k=1}^2 \\left( \\bar{y}_{i++} - \\bar{y}_{+++} \\right)^2 \\\\ SS(Leaf(Plant)) &amp; = &amp; \\sum_{i=1}^4 \\sum_{j=1}^3 \\sum_{k=1}^2 \\left( \\bar{y}_{ij+} - \\bar{y}_{i++} \\right)^2 \\\\ SS(Error) &amp; = &amp; \\sum_{i=1}^4 \\sum_{j=1}^3 \\sum_{k=1}^2 \\left( \\bar{y}_{ijk} - \\bar{y}_{ij+} \\right)^2 \\\\ \\end{eqnarray*}\\] Each sum-of-squares is associated with a certain number of df: source df Plant 3 (= 4 - 1) Leaf(Plant) 8 (= 12 - 1- 3) Error 12 (= 24 - 1- 3 - 8) Total 23 Something different has happened here. Because leaf is nested within plant, we have to deduct the df for the SS(Plant)from the df for SS(Leaf(Plant)). This is the general rule for nested factors. If factor ‘B’ is nested in factor ‘A’, then the df for SS(A) are subtracted from the df for SS(B(A)). (In fact, this explains why we’ve been deducting df for factorial effects from the df for error all along. The replicates are always nested within the factorial treatments, even though we haven’t thought of it in this way until now.) In models in which the residual error is the only random effect, we have learned that the estimate of that error, \\(\\hat{\\sigma}^2_{\\varepsilon}\\), is just equal to the MSE. Does this mean that we can equate MS(Plant) and MS(Leaf(Plant)) with \\(\\hat{\\sigma}^2_P\\) and \\(\\hat{\\sigma}^2_L\\), respectively? Unfortunately, in this case life isn’t that simple. As it turns out, there is a relationship between each of the mean squares and each of the variance components, but it’s complicated. One way to generate an ANOVA and learn about this relationship in SAS is to use the METHOD = TYPE3 option in PROC MIXED: proc mixed data = turnip method = type3; class Plant Leaf; model y = ; random Plant Leaf(Plant); run; Type 3 Analysis of Variance Sum of Source DF Squares Mean Square Expected Mean Square Plant 3 7.560346 2.520115 Var(Residual) + 2 Var(Leaf(Plant)) + 6 Var(Plant) Leaf(Plant) 8 2.630200 0.328775 Var(Residual) + 2 Var(Leaf(Plant)) Residual 12 0.079850 0.006654 Var(Residual) Covariance Parameter Estimates Cov Parm Estimate Plant 0.3652 Leaf(Plant) 0.1611 Residual 0.006654 Here, we see the ANOVA table with SS, MS and df for each variance component. We also see a column labeled “Expected Mean Square”. Briefly, the expected mean square is a formula that equates the “expectation” of the mean square for each variance component to each of the model parameters. (Remember that an “expectation” is the average value that you would observe over many runs of the same experiment.) The derivation of this formula is beyond the scope of ST512. With the formula in hand, though, it suggests one method of estimating the variance components. We can estimate the variance components by equating each MS with its expected mean square, and solving for the variance-component parameters. Sometimes, this “method of moments” approach to parameter estimation works well. Other times, however, it can generate negative estimates of variance parameters. (To see this, suppose that in the example above we had a data set where MS(Leaf) &lt; MS(Residual). Then we would have to have \\(\\hat{\\sigma}^2_L &lt; 0\\), which makes no sense.) Consequently, several other approaches have been developed to estimate variance-component parameters. The default in PROC MIXED is to use REML, which is an acronym for REstricted Maximum Likelihood. Although we will not discuss REML, do be aware that REML is a iterative numerical method. In some cases, the method can fail. The output to PROC MIXED presents an ‘Iteration History’ that gives you some information about the success or failure of REML. It is good practice to review the Iteration History and the Log file to make sure the procedure converged successfully. (We also saw a numerical method for estimating parameters when we considered non-linear regression.) For some nice data sets (such as the one above), the method-of-moments estimates of the variance components and the REML estimates are the same. In other cases, they may differ. General practice is to use the REML estimates unless a good reason exists to use another method of estimation. 9.3 Subsampling A second technique for reducing experimental error is subsampling. Subsampling refers to measuring the same experimental unit multiple times. This is the first occasion we have encountered in ST512 where the experimental unit (EU) and the measurement unit (MU) differ. Remember that the EU is the unit to which the experimental factor is applied, and the MU is the unit that is measured. Example: A former ST512 student from the Horticulture program studied the effect of 6 different vase solutions (floral preservatives) on the shelf lives of roses. She had 30 different vases at her disposal. The 6 different solutions were randomly assigned to the 30 vases in a balanced CRD with a one-way treatment structure. Three roses (or stems) were placed in each vase, and the shelf life of each rose was recorded.32 Here are some of the data for the cultivar “Freedom”: cultivar trt vase stem lifetime Freedom 1 1 1 13 Freedom 1 1 2 13 Freedom 1 1 3 11 Freedom 1 2 1 14 Freedom 1 2 2 11 Freedom 1 2 3 12 Freedom 1 3 1 14 ... Freedom 1 5 3 13 Freedom 2 1 1 16 Freedom 2 1 2 16 Freedom 2 1 3 12 Freedom 2 2 1 13 ... Here, the EU is the vase, and the MU is the stem. The stem is not the EU! The different stems are subsamples, in the sense that they are repeated samples of the same EU. Subsampling does not increase the number of EUs (and hence does not increase the number of df available to estimate the experimental error), but it does reduce the experimental error. 9.3.1 Equal subsamples per EU If the number of subsamples (stems) is the same for each EU (vase), we can average the subsample data (stem lifetimes) in each EU to generate 1 data point per EU, and then analyze the EU-averages with the usual one-way ANOVA. To calculate an average response for each EU, we can use whatever software package is most convenient (e.g., Excel). To calculate EU-level averages in SAS, we can use the following procedure. Suppose we had loaded the data from a file with the following DATA step: data freedom; infile &quot;.../rose.txt&quot; firstobs=2; input cultivar$ trt vase stem lifetime; run; We can then calculate average lifetimes per vase with PROC MEANS: /* This procedure averages the stems in each vase and produced a new data set named &#39;rosemeans&#39; that contains the newly calculated average lifetimes */ proc means data=freedom noprint; by cultivar trt vase; var lifetime; output out=rosemeans mean=avglife; run; We can use PROC PRINT to examine the newly created data set: proc print data=rosemeans; run; Obs cultivar trt vase _TYPE_ _FREQ_ avglife 1 Freedom 1 1 0 3 12.3333 2 Freedom 1 2 0 3 12.3333 3 Freedom 1 3 0 3 13.3333 4 Freedom 1 4 0 3 12.6667 5 Freedom 1 5 0 3 12.6667 6 Freedom 2 1 0 3 14.6667 7 Freedom 2 2 0 3 13.6667 ... 30 Freedom 6 5 0 3 12.0000 Note that there is now one observation per vase. Now we analyze the per-vase averages using a standard one-way ANOVA analysis in PROC GLM: proc glm data = rosemeans; class trt; model avglife = trt; means trt / tukey; run; Dependent Variable: avglife Sum of Source DF Squares Mean Square F Value Pr &gt; F Model 5 122.9185185 24.5837037 9.88 &lt;.0001 Error 24 59.6888889 2.4870370 Corrected Total 29 182.6074074 Source DF Type III SS Mean Square F Value Pr &gt; F trt 5 122.9185185 24.5837037 9.88 &lt;.0001 Tukey&#39;s Studentized Range (HSD) Test for avglife Means with the same letter are not significantly different. Tukey Grouping Mean N trt A 13.8667 5 2 A 13.6000 5 3 A 13.0667 5 6 A 12.6667 5 1 A 11.8667 5 5 B 7.8667 5 4 For comparison, suppose that instead only one rose stem had been included in each vase. The data set ‘rose1’ was formed by extracting the first rose stem listed for each vase from the original data set: cultivar trt vase stem lifetime Freedom 1 1 1 13 Freedom 1 2 1 14 Freedom 1 3 1 14 ... Freedom 2 1 1 16 Freedom 2 2 1 13 ... Here is the same one-factor ANOVA analysis for this smaller data set: proc glm data = freedom1; class trt; model lifetime = trt; run; Sum of Source DF Squares Mean Square F Value Pr &gt; F Model 5 84.5666667 16.9133333 2.30 0.0765 Error 24 176.4000000 7.3500000 Corrected Total 29 260.9666667 Source DF Type III SS Mean Square F Value Pr &gt; F trt 5 84.56666667 16.91333333 2.30 0.0765 Now the differences among the vase treatments are no longer significant! Note that the df’s have not changed: this is because both the full and reduced data sets contain the same number of EUs. What has changed is that the MSE for the reduced data set is much larger than the MSE for the full data set. This is because the `unexplained variability’ in the reduced data set includes both variability among vases assigned to the same treatment, and variability among individual rose stems. In the full data set, however, the unexplained variability in each data point consists of variability among vases assigned to the same treatment, and variability in the average lifetime of three individual rose stems. Because averaging reduces variability, the unexplained variability in the full data set is smaller than the unexplained variability in the reduced data set. Thus, subsampling reduces experimental error by averaging out the variability among individual measurements from the same EU. The wrong way to analyze these data is to treat the stem as the EU: /* This is the WRONG analysis */ proc glm data = freedom; class trt; model lifetime = trt; run; Sum of Source DF Squares Mean Square F Value Pr &gt; F Model 5 368.7555556 73.7511111 10.38 &lt;.0001 Error 84 597.0666667 7.1079365 Corrected Total 89 965.8222222 Source DF Type III SS Mean Square F Value Pr &gt; F trt 5 368.7555556 73.7511111 10.38 &lt;.0001 This analysis is wrong because it assumes that vase solutions were randomized to stems and not vases. This is pseudoreplication, because the analysis incorrectly inflates the amount of replication in the experiment. Pseudoreplication is a common and insidious mistake. 9.3.1.1 A second look: Analysis with a mixed effects model The method of averaging subsamples works well when each EU has the same number of subsamples. However, averaging subsamples does not work well when the number of subsamples per EU differs. This is because EUs with more subsamples will have less variability than EUs with fewer subsamples, and thus the equal variance assumption of ANOVA will be violated with averaged data. (As always, the seriousness of this violation is a matter of degree. If, for example, 29 vases had 3 stems and one vase had 2 stems, the unequal variance caused by averaging might be sufficiently mild that one could still justify averaging.) The best way to analyze data with unequal numbers of subsamples is to use a model that explicitly accounts for both the variation among EUs and the variation among MUs. To keep things from getting too complicated, we won’t look here at an example with an unequal number of subsamples per EU, but will instead consider an alternative analysis of the rose data above. This analysis can be easily modified to accommodate unequal numbers of subsamples per EU. Here is a model that we can use that models the data for the individual stems explicitly. This model will work regardless of whether we have the same number of stems in each vase. Let be the lifetime of the kth stem from the jth vase assigned to the ith treatment. Consider the model \\[ y_{ijk} = \\mu_i + V_{ij} + \\varepsilon_{ijk} \\] where \\(i = 1, 2, \\ldots, 6\\) is an index for the treatment \\(j = 1, 2, \\ldots, 5\\) is an index for the vase in each treatment \\(k = 1, 2, 3\\) is an index for the stem in each vase \\(\\mu_i\\) is the average response for treatment \\(i\\) \\(V_{ij}\\) is a random effect for vase \\(j\\) from treatment \\(i\\), \\(V_{ij} \\sim \\mathcal{N}\\left(0, \\sigma^2_V \\right)\\) \\(\\varepsilon_{ijk}\\) is subsample (residual) error for stem \\(k\\) from vase \\(j\\) of treatment \\(i\\), \\(\\varepsilon_{ijk} \\sim \\mathcal{N}\\left(0, \\sigma^2_{\\varepsilon} \\right)\\) The key feature of the model above is that it includes two random effects: one for the vase-to-vase error within treatments, and a second for the stem-to-stem error within vases. We assume that the random effects for the vases are drawn from a normal distribution with mean 0 and variance \\(\\sigma^2_V\\), and that the random effects for the stems are drawn from a normal distribution with mean 0 and variance \\(\\sigma^2_{\\varepsilon}\\). We can still think about building an ANOVA table for this analysis through a SS decomposition. The df for each SS will be: source df Treatment 5 Vase (EU error) 24 (= 30 - 1- 5) Stems (subsample error) 60 (= 90 - 1 - 5 - 24) Total 89 The key to this analysis is to realize that the treatment effects should be tested using the mean-squared error for the vases, not the stems. This is because the vases are the EUs (treatments were randomized to vases). Thus, any inferences that we draw about the treatments (i.e., \\(F\\)-tests, linear contrasts, or multiple comparisons) should be based on the MSE for the vases, not the MSE for the stems. In particular, we anticipate that the \\(F\\)-test for differences among the treatments will be based on an \\(F\\)-distribution with 5 ndf and 24 ddf. Here is PROC MIXED code and output for the Freedom rose data, using the default REML estimation: proc mixed data = freedom; class trt vase; model lifetime = trt; random vase(trt); run; Covariance Parameter Estimates Cov Parm Estimate vase(trt) 0.1648 Residual 6.9667 Type 3 Tests of Fixed Effects Num Den Effect DF DF F Value Pr &gt; F trt 5 24 9.88 &lt;.0001 Remarks: The MODEL statement in PROC MIXED only contains the fixed effects. In this model, the only fixed effects are the different treatments (vase solutions). The RANDOM statement specifies the random effects to be included in the model. The residual (= the error term for the subsample) is always included by default, so a separate term for “stem” did not need to be specified. We use the parenthetical syntax VASE(TRT) to tell SAS that VASE is nested within TRT. The portion of the output labeled “Covariance Parameter Estimates” provides the estimates for \\(\\sigma^2_V\\) and \\(\\sigma^2_{\\varepsilon}\\). In this case, they are \\(\\sigma^2_V = 0.165\\) and \\(\\sigma^2_{\\varepsilon} = 6.97\\). Although these estimates are not the primary focus of our analysis, they are still informative. In this case, they tell us that the stem-to-stem variability among stems in the same vase is much greater than the vase-to-vase variability among vases receiving the same treatment. The portion of the output labeled “Type 3 Tests of Fixed Effects” provides \\(F\\)-tests of the fixed effects. With REML estimation, PROC MIXED does not present the full ANOVA table. Note that the \\(F\\)-test for the treatment effect is identical to the \\(F\\)-test that we obtained by averaging subsamples within EUs. These two tests are identical because in this case we have the same number of stems per vase. Note that the \\(F\\)-test for the treatment effects has the correct ndf and ddf. Unless we specify otherwise, PROC MIXED uses the “containment” method for determining the appropriate error terms. In this case, the fixed-effects for TRT are contained within the random effect for VASE(TRT) (this is another way of saying that the vases are nested within the treatement), and so PROC MIXED uses the MSE for VASE(TRT) to test for the TRT fixed effects. The df for the \\(F\\)-tests can be used as a check to make sure that you’ve coded the model correctly. Here is an example of using linear contrasts and multiple comparisons to explore differences among the treatments. Note that throughout the df used for inference are the df that correspond to the vase error, not to the stem error. Linear contrasts: proc mixed data = freedom; class trt vase; model lifetime = trt; random vase(trt); estimate &#39;Difference b/w solutions 1 and 2&#39; trt 1 -1 0 0 0 0; run; Standard Label Estimate Error DF t Value Pr &gt; |t| Difference b/w solutions 1 and 2 -1.2000 0.9974 24 -1.20 0.2407 Multiple comparisons (there is no MEANS statement available in PROC MIXED, so we use LSMEANS instead): proc mixed data = freedom; class trt vase; model lifetime = trt; random vase(trt); lsmeans trt / pdiff adj=tukey; run; Least Squares Means Standard Effect trt Estimate Error DF t Value Pr &gt; |t| trt 1 12.6667 0.7053 24 17.96 &lt;.0001 trt 2 13.8667 0.7053 24 19.66 &lt;.0001 trt 3 13.6000 0.7053 24 19.28 &lt;.0001 trt 4 7.8667 0.7053 24 11.15 &lt;.0001 trt 5 11.8667 0.7053 24 16.83 &lt;.0001 trt 6 13.0667 0.7053 24 18.53 &lt;.0001 Differences of Least Squares Means Standard Effect trt _trt Estimate Error DF t Value Pr &gt; |t| Adjustment Adj P trt 1 2 -1.2000 0.9974 24 -1.20 0.2407 Tukey 0.8310 trt 1 3 -0.9333 0.9974 24 -0.94 0.3587 Tukey 0.9331 trt 1 4 4.8000 0.9974 24 4.81 &lt;.0001 Tukey 0.0008 trt 1 5 0.8000 0.9974 24 0.80 0.4304 Tukey 0.9644 trt 1 6 -0.4000 0.9974 24 -0.40 0.6919 Tukey 0.9985 trt 2 3 0.2667 0.9974 24 0.27 0.7915 Tukey 0.9998 trt 2 4 6.0000 0.9974 24 6.02 &lt;.0001 Tukey &lt;.0001 trt 2 5 2.0000 0.9974 24 2.01 0.0563 Tukey 0.3685 trt 2 6 0.8000 0.9974 24 0.80 0.4304 Tukey 0.9644 trt 3 4 5.7333 0.9974 24 5.75 &lt;.0001 Tukey &lt;.0001 trt 3 5 1.7333 0.9974 24 1.74 0.0951 Tukey 0.5217 trt 3 6 0.5333 0.9974 24 0.53 0.5978 Tukey 0.9941 trt 4 5 -4.0000 0.9974 24 -4.01 0.0005 Tukey 0.0060 trt 4 6 -5.2000 0.9974 24 -5.21 &lt;.0001 Tukey 0.0003 trt 5 6 -1.2000 0.9974 24 -1.20 0.2407 Tukey 0.8310 If we want to generate an ANOVA table, we can use the METHOD=TYPE3 option for method-of-moments estimation: proc mixed data = freedom method = type3; class trt vase; model lifetime = trt; random vase(trt); run; Type 3 Analysis of Variance Sum of Source DF Squares Mean Square Expected Mean Square Error Term trt 5 368.755556 73.751111 Var(Residual) + 3 Var(vase(trt)) MS(vase(trt)) + Q(trt) vase(trt) 24 179.066667 7.461111 Var(Residual) + 3 Var(vase(trt)) MS(Residual) Residual 60 418.000000 6.966667 Var(Residual) . Type 3 Analysis of Variance Error Source DF F Value Pr &gt; F trt 24 9.88 &lt;.0001 vase(trt) 60 1.07 0.4015 Covariance Parameter Estimates Cov Parm Estimate vase(trt) 0.1648 Residual 6.9667 Again, things work out nicely here because the data are balanced, but it is not guaranteed that the method-of-moments estimation and REML estimation will match up exactly. A final comment on subsampling: Often it is more expensive to have more EUs and is easier to have more subsamples. The extent to which increasing the number of subsamples will increase statistical power depends on the relative magnitudes of the variability among EUs and the variability among subsamples. If the variability among EUs is large relative to the variability among subsamples, then adding subsamples does little to increase statistical precision. But if the variability among subsamples is of the same or greater magnitude than the variability among EUs (as in the rose example), then adding subsamples can increase precision and statistical power. 9.4 \\(^\\star\\)Mathematical foundations In this section, we introduce formal mathematical arguments for how random effects induce correlations among grouped responses. This section makes heavy uses of the mathematics of probability. Before proceeding, we will need to review several key probability concepts 9.4.1 Probability refresher Let \\(X\\) denote a random variable. Recall that the expectation, or expected value, of \\(X\\) is one measure of the center of the probability distribution of \\(X\\). The expectation can also be thought of as the mean or average of \\(X\\). We denote the expectation of \\(X\\) as \\(\\mathrm{E}\\left[X\\right]\\), or sometimes \\(\\mu_X\\). (The subscript on \\(\\mu\\) is only used when we need it to clarify to which random variable the expectation refers.) The variance of \\(X\\), often denoted \\(\\mbox{Var}\\left(X\\right)\\) or \\(\\sigma^2_X\\), is defined as the expectation of \\((X - \\mu_X)^2\\). In other words, \\[ \\mbox{Var}\\left(X\\right) = \\mathrm{E}\\left[(X - \\mu_X)^2\\right]. \\] The variance of a random variable is a measure of its dispersion, or spread. Consider two random variables, \\(X\\) and \\(Y\\). The covariance of \\(X\\) and \\(Y\\), written as \\(\\mbox{Cov}\\left(X, Y\\right)\\), is defined as the expectation of the product \\((X - \\mu_X) (Y - \\mu_Y)\\). In other words, \\[ \\mbox{Cov}\\left(X, Y\\right) = \\mathrm{E}\\left[(X - \\mu_X)(Y - \\mu_Y)\\right]. \\] Note that the covariance of a random variable with itself is just its variance, that is, \\(\\mbox{Cov}\\left(X, X\\right) = \\mbox{Var}\\left(X\\right)\\). The covariance of two random variables is a measure of their association. If \\(X\\) and \\(Y\\) are positively associated, then values of \\(X\\) larger (smaller) than \\(\\mu_X\\) will tend to co-occur with values of \\(Y\\) larger (smaller) than \\(\\mu_Y\\), thus the product \\((X - \\mu_X) (Y - \\mu_Y)\\) will usually be positive, and so the expectation of that product — the covariance — will be positive as well. On the other hand, if the reverse is true, and values of \\(X\\) larger (smaller) than \\(\\mu_X\\) tend to co-occur with values of \\(Y\\) smaller (larger) than \\(\\mu_Y\\), then the product \\((X - \\mu_X) (Y - \\mu_Y)\\) will usually be negative, and so the covariance will be negative. When \\(X\\) and \\(Y\\) are independent, then their covariance is zero, i.e., \\(\\mbox{Cov}\\left(X, Y\\right) = 0\\). In general, the converse is not true: just because two random variables have a covariance of 0 does not necessarily imply that they are independent. However, if \\(X\\) and \\(Y\\) are two normally distributed random variables, then \\(\\mbox{Cov}\\left(X, Y\\right) = 0\\) does imply that \\(X\\) and \\(Y\\) are independent. Covariance is not just a measure of the association between two random variables. A covariance is also a measure of the dispersion or spread of those variables. Sometimes, we wish to factor out the dispersion component of covariance, and thus isolate the portion of covariance that measures the strength of association. We can do so by considering the correlation between \\(X\\) and \\(Y\\), written \\(\\mbox{Cor}\\left(X, Y\\right)\\) or \\(\\rho_{X,Y}\\), which scales the covariance by the dispersion of both \\(X\\) and \\(Y\\): \\[ \\mbox{Cor}\\left(X, Y\\right) = \\dfrac{\\mbox{Cov}\\left(X, Y\\right)}{\\sqrt{\\mbox{Var}\\left(X\\right)\\mbox{Var}\\left(Y\\right)}}. \\] Or, if we recognize that \\(\\sqrt{\\mbox{Var}\\left(X\\right)} = \\mbox{SD}\\left(X\\right)\\) (the standard deviation of \\(X\\)), then we can write \\[ \\mbox{Cor}\\left(X, Y\\right) = \\dfrac{\\mbox{Cov}\\left(X, Y\\right)}{\\mbox{SD}\\left(X\\right)\\mbox{SD}\\left(Y\\right)}. \\] Now here are some useful rules about the expectation and variance of a sum of random variables. First, the expectation of \\(X+Y\\) is simply \\[ \\mathrm{E}\\left[X+Y\\right] = \\mathrm{E}\\left[X\\right] + \\mathrm{E}\\left[Y\\right]. \\] The variance of \\(X+Y\\) is more complicated: \\[ \\mbox{Var}\\left(X+Y\\right) = \\mbox{Var}\\left(X\\right) + \\mbox{Var}\\left(Y\\right) + 2 \\mbox{Cov}\\left(X, Y\\right). \\] Thus, if \\(X\\) and \\(Y\\) are independent, then \\(\\mbox{Var}\\left(X+Y\\right) = \\mbox{Var}\\left(X\\right) + \\mbox{Var}\\left(Y\\right)\\). But, if \\(X\\) and \\(Y\\) are not independent, then we have to factor in the covariance. 9.4.2 Application to models with random effects Here is how this applies to models with random effects. Consider our model for the turnip-green data, but consider a simplified data set where there is only one sample from each leaf. Let’s write the model for these data as \\[ y_{ij} = \\mu + P_i + \\varepsilon_{ij} \\] where \\(P_i \\sim \\mathcal{N}\\left(0, \\sigma^2_P \\right)\\) represents the plant random effects, and \\(\\varepsilon_{ijk} \\sim \\mathcal{N}\\left(0, \\sigma^2_{\\varepsilon} \\right)\\) represents the leaf random effects (the residual error in this case). Let’s first find an expression for the variance of \\(y_{ij}\\): \\[\\begin{eqnarray*} \\mbox{Var}\\left(y_{ij}\\right) &amp; = &amp; \\mbox{Var}\\left(\\mu + P_i + \\varepsilon_{ij}\\right) \\\\ &amp; = &amp; \\mbox{Var}\\left(P_i\\right) + \\mbox{Var}\\left(\\varepsilon_{ij}\\right) + 2 \\mbox{Cov}\\left(P_i, \\varepsilon_{ij}\\right)\\\\ &amp; = &amp; \\sigma^2_P + \\sigma^2_{\\varepsilon}. \\end{eqnarray*}\\] Note that \\(\\mu\\) is a constant, and not a random variable, so its variance is 0. Note also that because the plant random effects and the leaf random effects are independent, then \\(\\mbox{Cov}\\left(P_i, \\varepsilon_{ij}\\right) = 0\\). Now, let’s try to find an expression for the correlation between two data points. First, we’ll find the covariance between two data points. To do so, we’ll need to use a rule for the covariance between two sums of random variables. That rule is \\[ \\mbox{Cov}\\left(X+Y, W+Z\\right) = \\mbox{Cov}\\left(X, W\\right) + \\mbox{Cov}\\left(X, Z\\right) + \\mbox{Cov}\\left(Y, W\\right) + \\mbox{Cov}\\left(Y, Z\\right). \\] (Note that we can apply this rule to obtain \\(\\mbox{Var}\\left(X+Y\\right) = \\mbox{Cov}\\left(X+Y, X+Y\\right) = \\mbox{Var}\\left(X\\right) + \\mbox{Var}\\left(Y\\right) + 2 \\mbox{Cov}\\left(X, Y\\right)\\).) Now, write two different data points as \\(y_{ij}\\) and \\(y_{kl}\\). Then, \\[\\begin{eqnarray*} \\mbox{Cov}\\left(y_{ij}, y_{kl}\\right) &amp; = &amp; \\mbox{Cov}\\left(\\mu + P_i + \\varepsilon_{ij}, \\mu + P_k + \\varepsilon_{kl}\\right) \\\\ &amp; = &amp; \\mbox{Cov}\\left(P_i, P_k\\right) + \\mbox{Cov}\\left(P_i, \\varepsilon_{kl}\\right) + \\mbox{Cov}\\left(\\varepsilon_{ij}, P_k\\right) + \\mbox{Cov}\\left(\\varepsilon_{ij}, \\varepsilon_{kl}\\right). \\end{eqnarray*}\\] Again, the \\(\\mu\\)’s drop out because they are constants and have no variance. Next, because the plant random effects are independent of the leaf random effects, \\(\\mbox{Cov}\\left(P_i, \\varepsilon_{kl}\\right) = \\mbox{Cov}\\left(\\varepsilon_{ij}, P_k\\right) = 0\\). Next, assuming that are two data points are from different leaves, then \\(\\mbox{Cov}\\left(\\varepsilon_{ij}, \\varepsilon_{kl}\\right) = 0\\) (because the lea\\(F\\)-level random effects are independent of one another). What about \\(\\mbox{Cov}\\left(P_i, P_k\\right)\\)? If the leaves come from the same plant, that is, if \\(i=k\\), then we have \\(\\mbox{Cov}\\left(P_i, P_k\\right) = \\mbox{Cov}\\left(P_i, P_i\\right) = \\mbox{Var}\\left(P_i\\right) = \\sigma^2_P\\). However, if the leaves come from different plants (\\(i \\neq k\\)), then the two plant random effects are independent, and thus \\(\\mbox{Cov}\\left(P_i, P_k\\right) = 0\\). Putting it all together, we have \\[ \\mbox{Cov}\\left(y_{ij}, y_{kl}\\right) = \\begin{cases} \\sigma^2_P &amp; i = k \\\\ 0 &amp; i \\neq k \\end{cases}. \\] Finally, to convert a covariance to a correlation, we just have to divide the covariance by \\(\\sqrt{\\mbox{Var}\\left(y_{ij}\\right) \\mbox{Var}\\left(y_{kl}\\right)} = \\sigma^2_P + \\sigma^2_{\\varepsilon}\\), giving \\[ \\mbox{Cor}\\left(y_{ij}, y_{kl}\\right) = \\begin{cases} \\dfrac{\\sigma^2_P}{\\sigma^2_P + \\sigma^2_{\\varepsilon}} &amp; i = k \\\\ 0 &amp; i \\neq k \\end{cases}. \\] Thus, we see that including the plant-level random effect induces a positive correlation between leaves from the same plant. (Note that correlation must be positive because \\(\\sigma^2_P\\) and \\(\\sigma^2_{\\varepsilon}\\) must both be positive.) Bibliography Snedecor, G. W., and W. G. Cochran. 1967. Statistical Methods. 6th ed. Ames: Iowa State University Press. The mean of a distribution is such an important and central concept in statistics that we have many different synonyms for it. Although the following terms are not perfect synonyms (their technical definitions are all slightly different), all of the following tend to be used as loose synonyms for a mean: average, center of mass, expected value, and expectation.↩︎ In fact, it is always true that the random effect associated within the individual data point is nested within another term in the model. Because this is always true, we usually don’t call attention to it.↩︎ A subsequent horticulture student provided helpful background information. This student explains: “Roses are often shipped from Colombia and the students running the experiment cannot control which rose comes off a healthy plant or not so healthy plant. Additionally, the flowers are shipped by air freight to Miami where they are then offloaded to be delivered. There can be expected variation of the roses by the placement of the package as well (cold / heat damage). Thus, once they arrive the student tries to randomize the flowers in the shipment to account for these variations. Once the jars are filled with floral preservatives and random flowers are placed the jars are randomly distributed on the tables in a lab. However, due to the windows in the lab and the nonuniform airflow, these factors can result in variation at the jar level (those that are placed near a window often reach 50% wilt faster than those placed near an AC vent).”↩︎ "],["blocked-designs.html", "Chapter 10 Blocked designs 10.1 Randomized complete block designs 10.2 Latin-squares designs 10.3 Split-plot designs 10.4 Repeated measures", " Chapter 10 Blocked designs 10.1 Randomized complete block designs The strength of people’s grip in their dominant or off-dominant hand was measured for \\(n=145\\) people. Data are below, with strength measured in Newtons of force. Suppose we wanted to use these data to ask if there was a difference in strength between people’s dominant and off-dominant hands. One (wrong) way to analyze these data is with a two-sample \\(t\\)-test: &gt; with(grip, t.test(x = Dominant, y = Off, var.equal = TRUE)) Two Sample t-test data: Dominant and Off t = 1.7844, df = 288, p-value = 0.07542 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -1.812768 36.998975 sample estimates: mean of x mean of y 298.6552 281.0621 The two-sample \\(t\\)-test is the wrong test for these data because it fails to account for the fact that the data are paired. The correct way to analyze these data is to calculate the difference in grip strength for each individual, and then to treat the sample of \\(n=145\\) differences as a single random sample from a population of differences. We then test the null hypothesis that the average of this population of differences is equals 0 against the alternative that the average difference is different from 0. &gt; with(grip, t.test(x = Dominant - Off)) One Sample t-test data: Dominant - Off t = 6.4883, df = 144, p-value = 1.306e-09 alternative hypothesis: true mean is not equal to 0 95 percent confidence interval: 12.23358 22.95263 sample estimates: mean of x 17.5931 The paired analysis estimates the average difference in grip strength more precisely, resulting in a narrower CI and a smaller \\(p\\)-value. The paired analysis is more precise because the experimental error has been reduced by removing person-to-person variability. Taking differences only works when we are comparing two treatments. When we are comparing more than two treatments, we need a more general strategy. Blocking is a technique that reduces experimental error (and hence increases precision) by grouping heterogeneous experimental units into blocks of homogeneous EUs. The canonical example of blocking comes from agriculture. Suppose we want to compare the effects of 3 different fertilizers (call them A, B, and C) on crop yield. Suppose a field can be divided into 12 plots (EUs), and there is a known east-west elevation slope to the field. In a CRD, treatments are randomly assigned to EUs. CRDs are appropriate when EUs are homogeneous. Here, we know that there is a difference between EUs based on where they lie along the east-west gradient. Consequently, a better design is to block against slope by grouping EUs into blocks of plots at similar locations along the east-west gradient. Treatments are then randomly assigned within blocks. This is a randomized complete block design (RCBD). A RCBD consists of several blocks, each of which is divided into several homogeneous EUs. Treatments are randomly assigned to EUs within each block. A blocked design is “complete” when each level of the experimental treatment is represented at least once in every block. In contrast, a blocked design is “incomplete” when some experimental treatments are missing from one or more blocks.33 A standard RCBD is one in which every treatment (or every treatment combination) is assigned to exactly one EU in each block. The classical way to analyze data from an RCBD is to include the block as an additional factor in the ANOVA, albeit a factor that is not engaged in interactions with the treatment factors. For the grip strength data, we would treat “subject” (the person) as a blocking factor. grip &lt;- read.table(&quot;data/gripall-long.txt&quot;, head = T, stringsAsFactors = T) summary(grip) ## subject hand strength ## AA : 2 dominant:145 Min. :119.0 ## AB : 2 off :145 1st Qu.:216.2 ## AC : 2 Median :288.5 ## AD : 2 Mean :289.9 ## AE : 2 3rd Qu.:362.5 ## AF : 2 Max. :503.0 ## (Other):278 fm1 &lt;- lm(strength ~ subject + hand, data = grip) anova(fm1) ## Analysis of Variance Table ## ## Response: strength ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## subject 144 1952993 13562 25.443 &lt; 2.2e-16 *** ## hand 1 22440 22440 42.098 1.306e-09 *** ## Residuals 144 76758 533 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Observe that the \\(p\\)-value for testing an effect of “hand” is exactly the same as the \\(p\\)-value from the paired \\(t\\)-test. This is not a coincidence. These are alternative ways of writing the same model. Remarks: We have fit an additive model that does not include a person-by-hand interaction. In a RCBD, it is common to assume that there is no block-by-treatment interaction. If we were interested in inspecting for an interaction, then this strongly suggests that the blocking factor is really another experimental treatment, and we should revise the design accordingly. In such a case, we would typically need to replicate each treatment within each block so that we have enough information to characterize the possible interaction. Typically we are not interested in the \\(F\\)-test associated with the block effect. Even if the block effect is not significant, we do not remove it from the model. Doing so is tantamount to treating the design as a CRD, which does not account for the restricted randomization. Here is an example of a standard RCBD with one experimental factor, described in the SAS documentation for PROC GLM: Stenstrom (1940) investigated how \\(p = 7\\) soil types affect the growth of snapdragons. Snapdragons were grown in 2.5-by-4 foot containers arranged along a greenhouse bench. To control for variation in light and temperature along the long axis of the bench, containers were blocked into three groups of seven based on their location on the bench. Average stem length of the snapdragons in each EU was recorded. Here is a diagram and a photo from Stenstrom’s MS thesis (Stenstrom (1940)) that illustrates the design: The data are: Block Type A B C Clarion 32.7 32.3 31.5 Clinton 32.1 29.7 29.1 Knox 35.7 35.9 33.1 O&#39;Neill 36.0 34.2 31.2 Compost 31.8 28.0 29.2 Wabash 38.2 37.8 31.9 Webster 32.5 31.1 29.7 Here is an analysis with PROC GLM: proc glm; class Block Type; model StemLength = Block Type; means Type / tukey; run; Dependent Variable: stemlength Sum of Source DF Squares Mean Square F Value Pr &gt; F Model 8 142.1885714 17.7735714 10.80 0.0002 Error 12 19.7428571 1.6452381 Corrected Total 20 161.9314286 Source DF Type III SS Mean Square F Value Pr &gt; F blk 2 39.0371429 19.5185714 11.86 0.0014 type 6 103.1514286 17.1919048 10.45 0.0004 Tukey&#39;s Studentized Range (HSD) Test for StemLength Alpha 0.05 Error Degrees of Freedom 12 Error Mean Square 1.645238 Critical Value of Studentized Range 4.94945 Minimum Significant Difference 3.6653 Means with the same letter are not significantly different. Tukey Grouping Mean N Type A 35.967 3 Wabash B A 34.900 3 Knox B A C 33.800 3 O&#39;Neill B D C 32.167 3 Clarion D C 31.100 3 Webster D C 30.300 3 Clinton D 29.667 3 Compost Here is an example of a standard RCBD with a 2 \\(\\times\\) 2 factorial treatment structure, described in the on-line SAS documentation. The description of the data there reads: The data, from Neter, Wasserman, and Kutner (1990, p. 941), are from an experiment examining the effects of codeine and acupuncture on post-operative dental pain in male subjects. Both treatment factors have two levels. The codeine levels are a codeine capsule or a sugar capsule. The acupuncture levels are two inactive acupuncture points or two active acupuncture points. There are four distinct treatment combinations due to the factorial treatment structure. The 32 subjects are assigned to eight blocks of four subjects each based on an assessment of pain tolerance. As with any factorial experiment, it is helpful to inspect an interaction plot of the treatment means first: Here is how we might analyze these data with SAS PROC GLM: proc glm data=dental; class block codeine acupuncture; model relief = block codeine|acupuncture; means codeine acupuncture / tukey; run; Sum of Source DF Squares Mean Square F Value Pr &gt; F Model 10 11.33500000 1.13350000 78.37 &lt;.0001 Error 21 0.30375000 0.01446429 Corrected Total 31 11.63875000 Source DF Type III SS Mean Square F Value Pr &gt; F block 7 5.59875000 0.79982143 55.30 &lt;.0001 codeine 1 2.31125000 2.31125000 159.79 &lt;.0001 acupuncture 1 3.38000000 3.38000000 233.68 &lt;.0001 codeine*acupuncture 1 0.04500000 0.04500000 3.11 0.0923 Tukey&#39;s Studentized Range (HSD) Test for relief Minimum Significant Difference 0.0884 Means with the same letter are not significantly different. Tukey grouping Mean N codeine A 1.42500 16 trt B 0.88750 16 placebo Tukey grouping Mean N acupuncture A 1.48125 16 yes B 0.83125 16 no 10.1.1 *Should a blocking factor be a fixed or random effect? When analyzing a blocked design, the analyst may want to consider using a random effect to characterize the differences among the blocks. To treat the block as a random effect, the analyst has to be able to justify two additional assumptions. First, there must be a larger population of blocks from which the blocks in the experiment were selected.34 Second, even if there is a larger population of blocks, the analyst must also be willing to assume that the blocks included in the experiment constitute a representative sample from this larger population. If the number of blocks in the experiment is small, it becomes less tenable to assert that the blocks in the experiment constitute a random sample. Whether the block effects are modeled as fixed or random impacts the scope of inferences for the experimental treatments. When the block effects are treated as fixed, then the resulting inferences about the experimental treatments pertain only the particular blocks in the experiment. When the block effects are treated as random, then the inferences about the experimental treatments extend to a population of blocks from which the blocks in the experiment have been selected. Of course, the latter (broader) inference is stronger, which is why it requires additional justification from the analyst. The broader inferences also reduce the precision of some of the estimates of the treatment effects, which makes sense if these effects are interpreted as applying to a broader population of blocks. For the snapdragon data, the design of the experiment strongly suggests that the block effects should be treated as fixed. The blocks are locations on the long axis of the greenhouse bench; it seems hard to argue that these three blocks are a sample from a larger population of blocks. Moreover, there are only three blocks in the experiment, which would hardly constitute a representative sample even if it were possible to conceptualize a larger population of blocks. Nevertheless, for the sake of illustration, we can still see how the inferences change if we were to treat the block as a random effect. Here is such an analysis in PROC MIXED. proc mixed; class blk type; model stemlength = type; random blk; lsmeans type; run; Covariance Parameter Estimates Cov Parm Estimate blk 2.5533 Residual 1.6452 Type 3 Tests of Fixed Effects Num Den Effect DF DF F Value Pr &gt; F type 6 12 10.45 0.0004 Least Squares Means Standard Effect type Estimate Error DF t Value Pr &gt; |t| type Clarion 32.1667 1.1830 12 27.19 &lt;.0001 type Clinton 30.3000 1.1830 12 25.61 &lt;.0001 type Compost 29.6667 1.1830 12 25.08 &lt;.0001 type Knox 34.9000 1.1830 12 29.50 &lt;.0001 type O&#39;Neill 33.8000 1.1830 12 28.57 &lt;.0001 type Wabash 35.9667 1.1830 12 30.40 &lt;.0001 type Webster 31.1000 1.1830 12 26.29 &lt;.0001 Remarks: The Type III \\(F\\)-tests for the differences among the soil types are the same in both analyses. This is true because the data are balanced. If the data are not balanced, treating the block as a random vs. fixed effect can have a small effect on the Type III \\(F\\)-tests. The standard error of the LSMEAN for each type is larger when we treat the block as a random effect. This makes sense, because if we want to expand our scope of inference to the population of blocks from which these blocks were selected (instead of restricting focus to these particular blocks), we incur a cost of greater uncertainty. If we wanted to write out an equation for this model, our model equation could be written as \\[ y_{ij} = \\mu_i + B_j + \\varepsilon_{ij} \\] where \\(\\mu_i\\) is the average response for soil type \\(i\\), \\(B_j \\sim \\mathcal{N}\\left(0, \\sigma^2_B \\right)\\) are the random block effects, and \\(\\varepsilon_{ij} \\sim \\mathcal{N}\\left(0, \\sigma^2_\\varepsilon \\right)\\) are the residual errors. 10.2 Latin-squares designs Consider the fertilizer again, and suppose that in addition to an east-west slope, there is also a north-south gradient in soil pH. A Latin squares design is a double blocking design that blocks against both east-west and north-south directions. To analyze these data, we need to include a separate factor in the ANOVA for each of the two blocking factors. Here is an example based on a real study. Taniguchi, Huntington, and Glenn (1995) studied the effects of infusion of casein (a protein) and cornstarch on net nutrient metabolism in beef steers. The investigators are interested in the effect of the location of the infusion (in the rumen vs. in the abomasum, both compartments of the ruminant stomach) for both the starch and the protein. The 4 unique treatment combinations are: starch and protein both delivered to the abomasum (henceforth treatment “A”), starch delivered to the abomasum and protein delivered to the rumen (treatment “B”), starch delivered to the rumen and protein delivered to the abomasum (treatment “C”), and starch and protein both delivered to the rumen (treatment “D”). Four beef steer were available for the study. Each steer was given each treatment over the course of 4 two-week periods. During each period, the steers were given the infusions during days 1–11, and removed from the infusions during days 12–14. Nutrient metabolism was measured on days 8–11 of each period. Thus, the data consist of a 2 \\(\\times\\) 2 factorial treatment structure with two blocking factors: steer and period. Treatments are assigned to each steer and period using a Latin squares design. Here is an interaction plot of the treatment means: Here is an analysis with PROC GLM: proc glm data=steer; class steer period starch protein; model nitrogen = starch|protein steer period; run; Dependent Variable: nitrogen Sum of Source DF Squares Mean Square F Value Pr &gt; F Model 9 1278.855625 142.095069 166.72 &lt;.0001 Error 6 5.113750 0.852292 Corrected Total 15 1283.969375 Source DF Type III SS Mean Square F Value Pr &gt; F starch 1 328.5156250 328.5156250 385.45 &lt;.0001 protein 1 8.8506250 8.8506250 10.38 0.0181 starch*protein 1 9.7656250 9.7656250 11.46 0.0148 steer 3 906.9968750 302.3322917 354.73 &lt;.0001 period 3 24.7268750 8.2422917 9.67 0.0103 The significant starch*protein interaction suggests that we should inspect the simple effects of the starch and protein treatments. The interaction plot above suggests that the effect of the protein location is small when starch is delivered to the rumen, but that the protein delivery site has a larger effect when starch is delivered to the abomasum. As a remark, note that the model above has only 6 df available to estimate the experimental error. While blocked designs are powerful, block effects do tend to gobble up lots of df, leaving few df available to estimate the experimental error. This phenomenon is particularly pronounced in Latin squares designs. 10.3 Split-plot designs A split-plot experiment combines the ideas of blocking and subsampling in a clever way. The essential feature of a split-plot experiment is that there are at least two experimental factors and the factors are randomized to different EUs, one of which nests the other. For example, consider a hypothetical experiment to evaluate the effect of 3 bacterial inoculation treatments applied to 2 grass cultivars (A vs. B). The response variable is dry-weight yield. Eight fields are available for the experiment. Suppose that the different inoculation treatments can be applied to different regions of the same field, but it is impossible to plant different cultivars in the same field. Consequently, cultivars are randomly assigned to fields as a balanced CRD. Fields are then split into 3 thirds, and inoculation treatments are randomly assigned to each third. Dry weight yield is then measured from each third for a total of 24 data points. Here is an interaction plot of the means for each of the 6 treatment combinations: In this experiment, the EU is different for the two experimental factors. For the cultivar treatment, the field is the EU, and the thirds are subsamples. For the bacterial inoculation treatment, however, the thirds are the EUs and the fields are blocks. Split-plot experiments are advantageous when it is more convenient to apply one factor or factors to large EUs and other factor(s) to smaller EUs. Split-plot experiments come with their own conventional terminology. In the example above, the cultivar is the whole-plot factor, and the fields are the whole-plot EU. The inoculation treatment is the split-plot factor, and the thirds are the split-plot EUs (subsamples of the whole-plot EUs). Because the design contains two different EUs, the analysis must contain two error terms. The critical piece of the analysis is to make sure that the statistical inferences about the whole-plot factor are drawn using the whole-plot error, and conversely inferences about the split-plot factor are drawn using the split-plot error. Here is an ANOVA model for this experiment: \\[ y_{ijk} = \\mu_{ij} + W_{ik} + \\varepsilon_{ijk} \\] where \\(i = 1, 2\\) is an index for the grass cultivars \\(j = 1, 2, 3\\) is an index for the inoculation treatment \\(k = 1, 2, \\ldots, 4\\) is an index for the replicate fields assigned to each cultivar \\(\\mu_{ij}\\) is average response when cultivar \\(i\\) is combined with bacterial inoculation \\(j\\) \\(W_{ik}\\) is a random effect for the whole-plot error, \\(W_{ik} \\sim \\mathcal{N}\\left(0, \\sigma^2_W \\right)\\) \\(\\varepsilon_{ijk}\\) is residual (split-plot) error, \\(\\varepsilon_{ijk} \\sim \\mathcal{N}\\left(0, \\sigma^2_{\\varepsilon} \\right)\\) As before, we can conduct an ANOVA sum-of-squares decomposition that will lead to \\(F\\)-tests of the factorial effects. We’ll prepare for this ANOVA by first partitioning the df among the various sums of squares. How do we partition the df? The random effect for the field will serve as the whole-plot error term. Because each field is associated with one and only one cultivar, then field is nested within cultivar. The random effect for the third will serve as the split-plot error term. Because each third is associated with one and only one cultivar, with one and only one field, with one and only one inoculation treatment, and with one and only one cultivar-inoculation treatment combination, then the third is nested in all these other model terms. The df for each term in the ANOVA will be: source df Cultivar 1 Whole-plot error 6 (=8 - 1 - 1) Inocuolation 2 Cultivar*Inoculation 2 Residual error 12 (= 24 - 1 - 1 - 6 - 2 - 2) Total 23 Because this model has multiple random effects, we’ll use PROC MIXED for our calculations. As before, we can use the METHOD = TYPE3 option to see the ANOVA table, but we’ll use METHOD = REML (the default) to obtain our estimates and \\(F\\)-tests of the fixed effects. (With a balanced data set, both the TYPE3 and REML methods of estimation will yield identical results.) Here is the TYPE3 analysis first: proc mixed method = type3; class field cult inoc; model drywt = cult|inoc; random field(cult); run; Type 3 Analysis of Variance Sum of Source DF Squares Mean Square Expected Mean Square cult 1 2.406667 2.406667 Var(Residual) + 3 Var(field(cult)) + Q(cult,cult*inoc) inoc 2 118.175833 59.087917 Var(Residual) + Q(inoc,cult*inoc) cult*inoc 2 1.825833 0.912917 Var(Residual) + Q(cult*inoc) field(cult) 6 34.800000 5.800000 Var(Residual) + 3 Var(field(cult)) Residual 12 8.465000 0.705417 Var(Residual) Note that we’ve coded the random-effect for the fields as FIELD(CULT), because field is nested within the cultivar treatment. Now, here is the REML analysis (the default): proc mixed; class field cult inoc; model drywt = cult|inoc; random field(cult); run; Covariance Parameter Estimates Cov Parm Estimate field(cult) 1.6982 Residual 0.7054 Type 3 Tests of Fixed Effects Num Den Effect DF DF F Value Pr &gt; F cult 1 6 0.41 0.5433 inoc 2 12 83.76 &lt;.0001 cult*inoc 2 12 1.29 0.3098 We see that there is evidence of a difference among the inoculation treatments, but no evidence of an interaction between the inoculation treatments and the cultivar, and no evidence of a difference among the cultivars. To explore the differences among the inoculation treatments, we’ll use an LSMEANS statement using Tukey’s HSD procedure: proc mixed; class field cult inoc; model drywt = cult|inoc / ddfm=satterthwaite; random field(cult); lsmeans inoc / pdiff adjust=tukey; run; Least Squares Means Standard Effect inoc Estimate Error DF t Value Pr &gt; |t| inoc con 27.9625 0.5481 9.01 51.01 &lt;.0001 inoc dea 29.9500 0.5481 9.01 54.64 &lt;.0001 inoc liv 33.3375 0.5481 9.01 60.82 &lt;.0001 Differences of Least Squares Means Standard Effect inoc _inoc Estimate Error DF t Value Pr &gt; |t| Adjustment Adj P inoc con dea -1.9875 0.4199 12 -4.73 0.0005 Tukey-Kramer 0.0013 inoc con liv -5.3750 0.4199 12 -12.80 &lt;.0001 Tukey-Kramer &lt;.0001 inoc dea liv -3.3875 0.4199 12 -8.07 &lt;.0001 Tukey-Kramer &lt;.0001 Remarks: The DDFM = SATTERTHWAITE option tells SAS to calculate df using a Satterthwaite approximation. See below. The LSMEANS statement calculates the average response for each inoculation treatment (the split-plot factor) averaging over the levels of the whole-plot factor, cultivar. The PDIFF option provides pairwise comparisons, and the ADJUST=TUKEY option adjusts these p-values for multiple comparisons. Analysis and interpretation: There is no evidence of an interaction between cultivar and inoculation treatment (\\(F_{2,12} = 1.29\\), \\(p= 0.31\\)). There is no evidence of a main effect of cultivar (\\(F_{1,6} = 0.41\\), \\(p= 0.54\\)). There is strong evidence of a main effect of inoculation treatment (\\(F_{2,12} = 83.76\\), \\(p&lt;.0001\\)). Pairwise comparisons of LSMEANS suggest that all inoculation treatments are significantly different from one another. Variance component estimates suggest that the experimental error associated with the whole-plot is roughly twice as great as the experimental error associated with the split-plot (1.70 vs. 0.70). 10.3.1 Denominator degrees of freedom in split-plot designs and the Satterthwaite approximation Standard errors of the LSMEANS for the split-plot factor (here, inoculation treatment) are functions of both the whole-plot error and the split-plot error. Consequently, the df associated with these standard errors are intermediate between the df available for the whole-plot error (here, 6) and the df available for the split-plot error (12). The Satterthwaite approximation is a computationally intensive method for approximating the appropriate df. (It’s computationally intensive because it entails inverting a big matrix.) With big models or small computers, calculating the Satterthwaite approximation may not be feasible. See section 11.4 of Oehlert for a more detailed discussion of the Satterthwaite approximation. You may have already encountered the Satterthwaite approximation in the context of Welch’s two-sample \\(t\\)-test. (Recall that Welch’s two-sample \\(t\\)-test can be used to compare the means of two samples when the variances of those two samples are not equal. In that context, the standard error of the difference between the means blends two different variance estimates, and thus the Satterthwaite approximation is needed to calculate the associated df. The key feature to both of these calculations is the presence of multiple variance estimates in the calculation of a standard error.) The Kenward-Roger approximation is an (even more computationally intensive) alternative to the Satterthwaite approximation (Kenward and Roger (1997)), and can be implemented in PROC MIXED by using the DDFM=KENWARDROGER option. To my knowledge, the guidance in the literature is fairly sparse regarding when these (or other) methods for approximating the denominator df perform best, though see Schaalje, McBride, and Fellingham (2002) for at least one study that seems to suggest the Kenward-Roger approximation is preferable when the computation time is not prohibitive. For classic experimental designs with balanced data and nested random effects (such as the split-plot examples encountered in these notes) the denominator df calculations for \\(F\\)-tests of the experimental treatment effects are beyond dispute. For unbalanced data or more complicated designs, it is not clear that the usual \\(F\\)-statistics calculated to test the fixed effects actually have \\(F\\)-distributions. That is to say, it is not clear that the sampling distribution of the test statistics actually takes an \\(F\\)-distribution when the experimental treatment has no effect. Notably, the authors of the PROC MIXED routine in SAS and the author of the program for fitting linear mixed models in R (the lmer routine in the lme4 library) took very different approaches to navigating this ambiguity. PROC MIXED will approximate the denominator df and provide corresponding \\(p\\)-values even when the quality of the approximation is unknown. In R, the lmer program does not quote denominator df or corresponding \\(p\\)-values under any circumstance, because the author of the routine was not convinced that any method for computing these \\(p\\)-values (including the methods implemented in PROC MIXED) could be justified. While the \\(p\\)-values reported by PROC MIXED are clearly valid for balanced designs with nested random effects, their validity for models with complicated arrangements of random effects is less clear. 10.4 Repeated measures Consider the following study that involved 27 children. On each child, the distance (mm) from the center of the pituitary to the pterygomaxillary fissure was made at ages 8, 10, 12, and 14 years of age. The data also specify the children’s genders, in this case as either boys or girls. The data are shown below, as they were originally reported in Potthoff and Roy (1964). In this figure, the panels with blue plot symbols show the girls and the panels with pink plot symbols show the boys. We would like to answer the usual questions that one poses for a factorial design: Do the marginal means differ between boys and girls? Do the marginal means differ with age? Do the differences between boys and girls depend on age? This is an example of a repeated-measures study. A repeated-measures study is one in which the response variable is measured multiple times on the same EU. Other examples include an experiment that measures the changes in plant community composition along a transect from an urban center into the countryside, or an experiment that measures changes in health status over time in response to different diets. Repeated-measures data share important similarities with a split-plot design. The EU in a repeated-measures design is analogous to the whole-plot in a split-plot design. The repeated measures on a single EU are analogous to the split-plots, where the split-plot factor is time (or age). Repeated-measures designs can also involve repeated measures in space, in which case the split-plot factor is spatial location. There is one critical difference between split-plot and repeated measures designs. In split-plot designs, the split-plot treatment is assigned randomly to the split-plot EUs. In repeated-measures designs, however, the “split-plot” treatment is usually time or space, and hence cannot be randomly assigned. This has important consequences for the statistical analysis. Because of their historical roots, repeated measures designs have their own terminology. The “whole-plot” units are usually called subjects (think of people enrolled in a longitudinal study). The experimental treatment is the between-subject factor, and time (age, spatial location) is the within-subject factor. Before proceeding, let’s perform a df accounting. This is the same df accounting that we would obtain if we treated these data as a split-plot experiment: source df Gender 1 Age 3 Gender * Age 3 Whole-plot error 25 (=27 - 1 - 1) Residual error 75 (=108 - 1 - 1 - 25 - 3 - 3) Total 107 Let’s also write down a model equation: \\[ y_{ijk} = \\mu_{ij} + \\varepsilon_{ijk} \\] where \\(i = 1, 2\\) indexes the two genders in this study \\(j = 1, 2, 3, 4\\) indexes the four ages \\(k = 1, 2, \\ldots, n_i\\) indexes for the replicate subjects for each gender \\(\\mu_{ij}\\) is the average response for a child of gender \\(i\\) at age \\(j\\). \\(\\varepsilon_{ijk}\\) is residual (within-subject) error, \\(\\varepsilon_{ijk} \\sim \\mathcal{N}\\left(0, \\sigma^2_{\\varepsilon} \\right)\\) As written, the model above doesn’t seem too different from the standard CRD. The key is that the within-subject errors are not independent. Instead, they may be correlated. In a split-plot analysis, we accommodated positively correlated errors among observations from the same whole plot by introducing a separate random effect for the whole-plot. Recall that a random whole-plot error is essentially a way of smuggling into the model a positive correlation among observations from the same whole plot. However, in the split-plot model, any two measurements from the same whole plot are assumed to be have the same correlation. In a repeated-measures design, we expect that the strength of the correlation of two measurements from the same subject will depend on how closely spaced the measurements are. Thus a repeated-measures analysis usually requires a different model for the correlation of the within-subject errors than the split-plot model provides. For each subject, let \\(\\rho_{mn}\\) denote the correlation between measurements \\(m\\) and \\(n\\). For the dental data, both \\(m\\) and \\(n\\) take the values from 1 to 4. With the dental data, there are 6 unique pairs of measurements on each subject, and hence we need to find six distinct values of \\(\\rho_{mn}\\) to fully characterize the correlations among the several measurements on each subject. While finding six values doesn’t seem so bad, in other contexts when there are more measurements on each subject, the number of unique correlations that we need to estimate can get large quickly. Thus it is often worthwhile to try to develop a parsimonious model for the within-subject correlations that allows us to characterize these correlations with a small number of parameters. We will examine three common models for the within-subject correlations. In the PROC MIXED documentation, you’ll find descriptions of nearly two dozen possible models for the within-subject correlations. When discussing within-subject correlations, it is common to arrange these correlations in a correlation matrix. A correlation matrix is a square matrix in which the element in row \\(i\\) and column \\(j\\) corresponds to the correlation \\(\\rho_{ij}\\). For our purposes, the correlation matrix is just a handy bookkeeping device. In other contexts, the matrix representation is useful to derive additional properties using matrix algebra, although we won’t be using any of those results here. Bear in mind that the correlation matrix has a lot of redundancy built into it. First, the elements on the main diagonal of the matrix (that is, the diagonal running from the upper-left corner to the lower-right corner) must equal 1, because these values give the correlation of a number with itself, and any random variable is always perfectly correlated with itself. Second, because correlations are symmetric (that is, \\(\\rho_{mn} = \\rho_{nm}\\)), each correlation appears in the correlation matrix twice: once in the “upper triangle” above the main diagonal and once in the “lower triangle” below the main diagonal. Correlation structure 1: Compound symmetry (CS) In this structure, all pairs of observations from the same subject have the same correlation. In an equation, we’d write \\[ \\rho_{mn} = \\rho. \\] The CS model is equivalent to treating the data as if they came from a split-plot design. Correlation structure 2: First-order autoregressive (AR(1)) In this structure, correlations decay exponentially with time. This is often the case for repeated measures, as two repeated measures occurring close together in time are apt to be more strongly associated than two repeated measures occurring far apart in time. In an equation, we’d write \\[ \\rho_{mn} = \\rho^{|m-n|}. \\] Here, we’ve made the additional assumtion that the repeated measures are equally spaced in time. The AR(1) model can easily be adapted to repeated-measures data that are not equally spaced. To do so, let \\(t(m)\\) be the time at which measurement \\(m\\) is made. Then the AR(1) correlation model writes as \\(\\rho_{mn} = \\rho^{|t(m)-t(n)|}\\). Correlation structure 3: Unstructured correlation matrix (UN) In this model, each correlation parameter is estimated separately. This is the most flexible and parameter-rich correlation structure, although not necessarily the best. Here is an illustration of how to implement these models in PROC MIXED. We’ll start by assuming a CS correlation structure. proc print data=growth; run; Obs Person Gender y Age 1 1 F 21.0 8 2 1 F 20.0 10 3 1 F 21.5 12 4 1 F 23.0 14 5 2 F 21.0 8 6 2 F 21.5 10 7 2 F 24.0 12 8 2 F 25.5 14 9 3 F 20.5 8 ... proc mixed data=growth; class Person Gender Age; model y=Gender | Age; repeated / type=cs subject=Person(Gender) rcorr; run; Type 3 Tests of Fixed Effects Num Den Effect DF DF F Value Pr &gt; F Gender 1 25 9.29 0.0054 Age 3 75 35.35 &lt;.0001 Gender*Age 3 75 2.36 0.0781 Estimated R Correlation Matrix for Person(Gender) 1 F Row Col1 Col2 Col3 Col4 1 1.0000 0.6245 0.6245 0.6245 2 0.6245 1.0000 0.6245 0.6245 3 0.6245 0.6245 1.0000 0.6245 4 0.6245 0.6245 0.6245 1.0000 Covariance Parameter Estimates Cov Parm Subject Estimate CS Person(Gender) 3.2854 Residual 1.9750 Fit Statistics -2 Res Log Likelihood 423.4 AIC (smaller is better) 427.4 AICC (smaller is better) 427.5 BIC (smaller is better) 430.0 Remarks: The REPEATED statement tells PROC MIXED that the data are repeated measures data, and asks SAS to model the correlation among data points shared by the same subject. The SUBJECT = option specifies a unique indicator for each subject. In this data set, the variable PERSON is nested within GENDER. The TYPE = CS option tells SAS to use a compound symmetry correlation structure for the observations from the same subject. Other options include TYPE = AR(1) and TYPE = UN. The RCORR option asks SAS to output the estimated correlation matrix for the first subject in the data set. (Remember that this is the same correlation matrix for all subjects.) In the data set itself, the data must be ordered for each subject. That is, the first measurement (at age 8) must appear as the first data record for each person, the second measurement must appear as the second data record, etc. Now let’s try an AR(1) correlation structure. To do so, we’ll modify the TYPE argument in the REPEATED statement. proc mixed data=growth; class Person Gender Age; model y = Gender | Age; repeated / type = ar(1) subject = Person(Gender) rcorr; run; The Mixed Procedure Estimated R Correlation Matrix for Person(Gender) 1 F Row Col1 Col2 Col3 Col4 1 1.0000 0.6153 0.3786 0.2329 2 0.6153 1.0000 0.6153 0.3786 3 0.3786 0.6153 1.0000 0.6153 4 0.2329 0.3786 0.6153 1.0000 Covariance Parameter Estimates Cov Parm Subject Estimate AR(1) Person(Gender) 0.6153 Residual 5.2467 Fit Statistics -2 Res Log Likelihood 434.5 AIC (Smaller is Better) 438.5 AICC (Smaller is Better) 438.7 BIC (Smaller is Better) 441.1 Type 3 Tests of Fixed Effects Num Den Effect DF DF F Value Pr &gt; F Gender 1 25 11.07 0.0027 Age 3 75 15.48 &lt;.0001 Gender*Age 3 75 1.12 0.3445 Finally, here’s the fit with a UN correlation structure. proc mixed data=growth; class Person Gender Age; model y=Gender|Age; repeated / type=un subject=Person(Gender) rcorr; run; The Mixed Procedure Estimated R Correlation Matrix for Person(Gender) 1 F Row Col1 Col2 Col3 Col4 1 1.0000 0.5707 0.6613 0.5216 2 0.5707 1.0000 0.5632 0.7262 3 0.6613 0.5632 1.0000 0.7281 4 0.5216 0.7262 0.7281 1.0000 Covariance Parameter Estimates Cov Parm Subject Estimate UN(1,1) Person(Gender) 5.4155 UN(2,1) Person(Gender) 2.7168 UN(2,2) Person(Gender) 4.1848 UN(3,1) Person(Gender) 3.9102 UN(3,2) Person(Gender) 2.9272 UN(3,3) Person(Gender) 6.4557 UN(4,1) Person(Gender) 2.7102 UN(4,2) Person(Gender) 3.3172 UN(4,3) Person(Gender) 4.1307 UN(4,4) Person(Gender) 4.9857 Fit Statistics -2 Res Log Likelihood 414.0 AIC (smaller is better) 434.0 AICC (smaller is better) 436.5 BIC (smaller is better) 447.0 Type 3 Tests of Fixed Effects Num Den Effect DF DF F Value Pr &gt; F Gender 1 25 9.29 0.0054 Age 3 25 34.45 &lt;.0001 Gender*Age 3 25 2.93 0.0532 How do we choose among correlation structures? The usual approach is to arbitrate among correlation structures by using AIC, or it’s small-sample variant AICc. (Recall that AIC is Akaike’s Information Criterion, and is a penalized measure of model fit. AICc is a modification of AIC that is more appropriate for small sample sizes.) In PROC MIXED, smaller (i.e., further to the left on the number line) values of AICc are better. Covariance structure AICc CS 427.5 AR(1) 438.7 UN 436.5 Thus, we use the CS correlation structure and base our inference of the fixed gender and age effects based on that Type III analysis. So, here we would conclude that there is marginal evidence of an interaction between age and gender (\\(F_{3,75} = 2.36\\), \\(p= 0.078\\)). If we disregard the interaction, there is strong evidence of both a main effect of age (\\(F_{3,75} = 35.35\\), \\(p&lt;.0001\\)) and a main effect of gender (\\(F_{1,25} = 9.29\\), \\(p=.0054\\)). We could dig deeper and compare LSMEANS for both age and gender, using a Satterthwaite approximation: proc mixed data=growth; class Person Gender Age; model y=Gender|Age / ddfm=satterth; repeated / type=cs subject=Person(Gender) rcorr; lsmeans Gender / pdiff; lsmeans Age / pdiff adjust=tukey; run; Least Squares Means Standard Effect Gender Age Estimate Error DF t Value Pr &gt; |t| Gender F 22.6477 0.5861 25 38.64 &lt;.0001 Gender M 24.9687 0.4860 25 51.38 &lt;.0001 Age 8 22.0284 0.4492 46.1 49.04 &lt;.0001 Age 10 23.0199 0.4492 46.1 51.25 &lt;.0001 Age 12 24.4048 0.4492 46.1 54.33 &lt;.0001 Age 14 25.7798 0.4492 46.1 57.39 &lt;.0001 Differences of Least Squares Means Standard Effect Gender Age _Gender _Age Estimate Error DF t Value Pr &gt; |t| Gender F M -2.3210 0.7614 25 -3.05 0.0054 Age 8 10 -0.9915 0.3892 75 -2.55 0.0129 Age 8 12 -2.3764 0.3892 75 -6.11 &lt;.0001 Age 8 14 -3.7514 0.3892 75 -9.64 &lt;.0001 Age 10 12 -1.3849 0.3892 75 -3.56 0.0007 Age 10 14 -2.7599 0.3892 75 -7.09 &lt;.0001 Age 12 14 -1.3750 0.3892 75 -3.53 0.0007 Differences of Least Squares Means Effect Gender Age _Gender _Age Adjustment Adj P Gender F M . Age 8 10 Tukey-Kramer 0.0608 Age 8 12 Tukey-Kramer &lt;.0001 Age 8 14 Tukey-Kramer &lt;.0001 Age 10 12 Tukey-Kramer 0.0036 Age 10 14 Tukey-Kramer &lt;.0001 Age 12 14 Tukey-Kramer 0.0039 In the example above, age is treated as a categorical variable. It might make sense to treat it instead as a numerical variable, akin to a covariate in ANCOVA, and to look for a linear effect of age on size. The SAS code is identical, except that now we do not declare age as a categorical variable: proc mixed data=growth; class Person Gender; model y=Gender|Age / solution; repeated / type=cs subject=Person(Gender) rcorr; run; Estimated R Correlation Matrix for Person(Gender) 1 F Row Col1 Col2 Col3 Col4 1 1.0000 0.6318 0.6318 0.6318 2 0.6318 1.0000 0.6318 0.6318 3 0.6318 0.6318 1.0000 0.6318 4 0.6318 0.6318 0.6318 1.0000 Type 3 Tests of Fixed Effects Num Den Effect DF DF F Value Pr &gt; F Gender 1 25 0.45 0.5082 Age 1 79 108.36 &lt;.0001 Age*Gender 1 79 6.30 0.0141 If we repeat the competition among correlation structures with age considered as a numerical predictor, the CS correlation structure emerges as the best correlation structure. Note that treating age as a continuous predictor leads us to a different conclusion about the significance of the age-by-gender interaction. Which model should we use? Answer: Use AICC again to compete models against one another. WARNING! AIC / AICC values based on REML fits are not comparable among models with different fixed effects. With the REML estimation scheme, the information provided in the “Fit Statistics” component of the PROC MIXED output can only be used to compare models with the same fixed-effect structure. To compare models with different fixed-effect structures, PROC MIXED must be told to find parameter estimates using maximum likelihood (ML). This is achieved by adding a METHOD = ML option to the initial call to PROC MIXED. It’s helpful at this point to know a bit more about the distinction between REML and ML estimation. Maximum likelihood (ML) estimation is a general scheme for drawing inferences in parameterized models. An appealing feature of ML estimation is that the bias of a ML estimate (MLE) is guaranteed to shrink to zero as the data set becomes arbitrarily large. (While this may seem like a weak result, it reassures us that our estimates are going to tend to get better as we collect more data. Certainly we’d be concerned if this were not true!) However, MLEs can be, and often are, biased for small data sets. This is especially true with respect to estimating the variance of a Gaussian distribution. For example, in simple regression, the MLE of the residual variance is \\(\\hat{\\sigma}^2_{\\varepsilon} = \\dfrac{SSE}{n} = \\dfrac{n-2}{n}\\dfrac{SSE}{n-2} = \\dfrac{n-2}{n}s^2_{\\varepsilon}\\). We favor our usual estimate of the residual variance (\\(s^2_\\varepsilon=SSE/(n-2)\\)) because it is unbiased. If \\(s^2_\\varepsilon\\) is unbiased (it is the “right size”, on average), then the MLE is too small, although the amount by which the MLE is too small itself gets small as \\(n\\) gets large. This is the basic problem: ML underestimates the variance of a Gaussian distribution. Underestimating the error variance is bad, because our estimates of the error variance feed into all of our other inference procedures. Thus, if we use the ML estimates of the variance, then our confidence intervals and \\(p\\)-values imply more precision in our estimates of the experimental effects than the data warrant. Restricted maximum likelihood, or REML, is a modification of ML that provides appropriate estimates of the variance. Indeed, our estimate of the residual variance in SLR — \\(s^2_\\varepsilon=SSE/(n-2)\\) — is exactly the REML estimate of the residual variance, we just didn’t identify it as such when we first encountered it. REML acts by “restricting” the data and then applying ML to these restricted data. However, in a mixed model, the “restriction” that REML uses differs depending on the fixed-effects component of the model. This is why we can’t compare mixed models with different fixed-effects components using AIC. Remember that AIC values can only be compared among models fit to the same data. Suppose we use REML to fit several mixed models with different fixed-effects components to the same original data. Because the models have different fixed-effects components, they each restrict the original data in different ways, and hence the competing models are fit to different restricted data sets. Thus the AIC values for the fits can’t be compared. Re-fit using a CS covariance structure and ML estimation. Age as a categorical variable: proc mixed method=ml data=growth; class Person Gender Age; model y=Gender|Age; repeated / type=cs subject=Person(Gender) rcorr; run; Fit Statistics -2 Log Likelihood 426.6 AIC (smaller is better) 446.6 AICC (smaller is better) 448.9 BIC (smaller is better) 459.6 Type 3 Tests of Fixed Effects Num Den Effect DF DF F Value Pr &gt; F Gender 1 25 10.04 0.0040 Age 3 75 38.18 &lt;.0001 Gender*Age 3 75 2.55 0.0620 Age as a continuous variable: proc mixed method=ml data=growth; class Person Gender; model y=Gender|Age / solution; repeated / type=cs subject=Person(Gender) rcorr; run; Fit Statistics -2 Log Likelihood 428.6 AIC (smaller is better) 440.6 AICC (smaller is better) 441.5 BIC (smaller is better) 448.4 Solution for Fixed Effects Standard Effect Gender Estimate Error DF t Value Pr &gt; |t| Intercept 16.3406 0.9631 25 16.97 &lt;.0001 Gender F 1.0321 1.5089 25 0.68 0.5003 Gender M 0 . . . . Age 0.7844 0.07654 79 10.25 &lt;.0001 Age*Gender F -0.3048 0.1199 79 -2.54 0.0130 Age*Gender M 0 . . . . Type 3 Tests of Fixed Effects Num Den Effect DF DF F Value Pr &gt; F Gender 1 25 0.47 0.5003 Age 1 79 111.10 &lt;.0001 Age*Gender 1 79 6.46 0.0130 The model with age as a continuous variable is AIC-best. Thus we conclude that there is a significant age-by-gender interaction (\\(F_{1,79} = 6.46\\), \\(p= 0.013\\)). In light of the significant interaction, we do not consider the Type III \\(F\\)-tests of the main effects. We can use the table of parameter solutions to determine the best-fitting linear relationships between size and age for boys and girls: For girls: size = (16.34 + 1.03) + (0.78 - 0.30) * age = 17.37 + 0.48 * age For boys: size = (16.34 + 0) + (0.78 + 0) * age = 16.34 + 0.78 * age It’s surprising that the CS correlation structure is AICc-best for the dental data. We’ve argued that the AR(1) correlation structure seems to make the most sense based on first principles, so why is it that the CS correlation structure provides a better fit to these data? The answer turns out to have a lot to do with the one subject whose response decreases from age 8 to age 10, increases dramatically from age 10 to age 12, and then decreases again from age 12 to age 14. (This subject is easily spotted in the data display above.) If this subject is removed from the data set, then the AR(1) correlation structure emerges as AIC-best. Without knowing more about these data, it’s hard to know what to make of the subject with the unusual data pattern. For the present purposes, we’ll take the data at face value. Doing so leads us to favor the CS model. Bibliography Kenward, Michael G, and James H Roger. 1997. “Small Sample Inference for Fixed Effects from Restricted Maximum Likelihood.” Biometrics, 983–97. Potthoff, Richard F, and SN Roy. 1964. “A Generalized Multivariate Analysis of Variance Model Useful Especially for Growth Curve Problems.” Biometrika 51 (3-4): 313–26. Schaalje, G Bruce, Justin B McBride, and Gilbert W Fellingham. 2002. “Adequacy of Approximations to Distributions of Test Statistics in Complex Mixed Linear Models.” Journal of Agricultural, Biological, and Environmental Statistics 7: 512–24. Stenstrom, F. H. 1940. “The Growth of Snapdragons, Stocks, Cinerarias, and Carnations on Six Iowa Soils.” Master’s thesis, Iowa State College. Taniguchi, K, GB Huntington, and BP Glenn. 1995. “Net Nutrient Flux by Visceral Tissues of Beef Steers Given Abomasal and Ruminal Infusions of Casein and Starch.” Journal of Animal Science 73 (1): 236–49. Incomplete block designs arise when the number of EUs per block is smaller than the number of experimental treatments. If the number of EUs in each block is at least as large as the number of experimental treatments, one would typically opt for a complete blocked design instead of an incomplete one.↩︎ As before, a useful way to approach this question is to ask whether a hypothetical repeat of the experiment would have necessarily included the same blocks as the actual experiment. If the answer to this question is “yes”, then this argues against the existence of a larger population of blocks. If “no”, then there is a larger population of blocks at hand, and it might be reasonable to treat the blocks as a random effect.↩︎ "],["bibliography.html", "Bibliography", " Bibliography Bates, Douglas M, and Donald G Watts. 1988. Nonlinear Regression Analysis and Its Applications. Wiley. Benjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” Journal of the Royal Statistical Society: Series B (Methodological) 57 (1): 289–300. Berry, Donald A. 2016. “P-Values Are Not What They’re Cracked up to Be.” Online Commentary to ASA Statement on Statistical Significance and \\(p\\)-Values.(doi: 10.1080/00031305.2016. 1154108). Bowerman, Bruce L, and Richard T O’Connell. 1990. Linear Statistical Models: An Applied Approach. Brooks/Cole. Breiman, Leo. 2001. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” Statistical Science 16 (3): 199–231. Dennis, Brian, and Mark L Taper. 1994. “Density Dependence in Time Series Observations of Natural Populations: Estimation and Testing.” Ecological Monographs 64 (2): 205–24. Galton, Francis. 1886. “Regression Towards Mediocrity in Hereditary Stature.” The Journal of the Anthropological Institute of Great Britain and Ireland 15: 246–63. Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press. Gillibrand, EJV, P Bagley, A Jamieson, PJ Herring, JC Partridge, MA Collins, R Milne, and Imants George Priede. 2007. “Deep Sea Benthic Bioluminescence at Artificial Food Falls, 1,000–4,800 m Depth, in the Porcupine Seabight and Abyssal Plain, North East Atlantic Ocean.” Marine Biology 150 (6): 1053–60. Gotelli, Nicholas J, Aaron M Ellison, et al. 2004. A Primer of Ecological Statistics. Vol. 1. Sinauer Associates Sunderland. Grayson, Donald K. 1993. “Differential Mortality and the Donner Party Disaster.” Evolutionary Anthropology: Issues, News, and Reviews 2 (5): 151–59. Hanley, James A, and Stanley H Shapiro. 1994. “Sexual Activity and the Lifespan of Male Fruitflies: A Dataset That Gets Attention.” Journal of Statistics Education 2 (1). Holm, Sture. 1979. “A Simple Sequentially Rejective Multiple Test Procedure.” Scandinavian Journal of Statistics, 65–70. Kahneman, Daniel. 2011. Thinking, Fast and Slow. Macmillan. Kenward, Michael G, and James H Roger. 1997. “Small Sample Inference for Fixed Effects from Restricted Maximum Likelihood.” Biometrics, 983–97. Lehmann, Erich L. 1993. “The Fisher, Neyman-Pearson Theories of Testing Hypotheses: One Theory or Two?” Journal of the American Statistical Association 88 (424): 1242–49. Low, C. K., and A. R. Bin Mohd. Ali. 1985. “Experimental Tapping of Pine Oleoresin.” The Malaysian Forester 48: 248–53. Martin, Ryan. 2017. “A Statistical Inference Course Based on p-Values.” The American Statistician 71 (2): 128–36. McIntyre, Lauren. 1994. “Using Cigarette Data for an Introduction to Multiple Regression.” Journal of Statistics Education 2 (1). Mendenhall, William M, and Terry L Sincich. 2012. Statistics for Engineering and the Sciences. New York: Dellen Publishing Co. Milliken, George A, and Dallas E Johnson. 2001. Analysis of Messy Data, Volume III: Analysis of Covariance. Chapman; Hall/CRC. Monahan, John F. 2008. A Primer on Linear Models. CRC Press. Moore, David S, and George P McCabe. 1989. Introduction to the Practice of Statistics. WH Freeman. Oehlert, Gary W. 2010. A First Course in Design and Analysis of Experiments. Partridge, Linda, and Marion Farquhar. 1981. “Sexual Activity Reduces Lifespan of Male Fruitflies.” Nature 294 (5841): 580–82. Pearson, Egon S. 1955. “Statistical Concepts in the Relation to Reality.” Journal of the Royal Statistical Society. Series B (Methodological), 204–7. Poole, Joyce H. 1989. “Mate Guarding, Reproductive Success and Female Choice in African Elephants.” Animal Behaviour 37: 842–49. Potthoff, Richard F, and SN Roy. 1964. “A Generalized Multivariate Analysis of Variance Model Useful Especially for Growth Curve Problems.” Biometrika 51 (3-4): 313–26. Quinn, Gerry P, and Michael J Keough. 2002. Experimental Design and Data Analysis for Biologists. Cambridge university press. R Core Team. 2021. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Ramsey, Fred, and Daniel Schafer. 2012. The Statistical Sleuth: A Course in Methods of Data Analysis. Pacific Grove, CA: Duxbury. Rao, Pejaver Vishwamber. 1998. Statistical Research Methods in the Life Sciences. Duxbury Press. Rawlings, John O, Sastry G Pantula, and David A Dickey. 1998. Applied Regression Analysis: A Research Tool. Springer. Rubin, Donald B. 2005. “Causal Inference Using Potential Outcomes: Design, Modeling, Decisions.” Journal of the American Statistical Association 100 (469): 322–31. Sackett, Dana K, W Gregory Cope, James A Rice, and D Derek Aday. 2013. “The Influence of Fish Length on Tissue Mercury Dynamics: Implications for Natural Resource Management and Human Health Risk.” International Journal of Environmental Research and Public Health 10 (2): 638–59. Schaalje, G Bruce, Justin B McBride, and Gilbert W Fellingham. 2002. “Adequacy of Approximations to Distributions of Test Statistics in Complex Mixed Linear Models.” Journal of Agricultural, Biological, and Environmental Statistics 7: 512–24. Sen, Ashish, and Muni Srivastava. 1997. Regression Analysis: Theory, Methods, and Applications. Springer Science &amp; Business Media. Snedecor, G. W., and W. G. Cochran. 1967. Statistical Methods. 6th ed. Ames: Iowa State University Press. Sokal, Robert R, and F James Rohlf. 1995. Biometry. 3rd ed. New York: W.H. Freeman. Steel, R. G. D., J. H. Torrie, and D. A. Dickey. 1997. “Principles and Procedures of Statistics: A Biometrical Approach.” Stenstrom, F. H. 1940. “The Growth of Snapdragons, Stocks, Cinerarias, and Carnations on Six Iowa Soils.” Master’s thesis, Iowa State College. Taniguchi, K, GB Huntington, and BP Glenn. 1995. “Net Nutrient Flux by Visceral Tissues of Beef Steers Given Abomasal and Ruminal Infusions of Casein and Starch.” Journal of Animal Science 73 (1): 236–49. Vicente, Joaquı́n, Ursula Höfle, Joseba Garrido, Isabel G Fernández-De-Mera, Ramón Juste, Marta Barral, and Christian Gortazar. 2006. “Wild Boar and Red Deer Display High Prevalences of Tuberculosis-Like Lesions in Spain.” Veterinary Research 37 (1): 107–19. Wasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA Statement on p-Values: Context, Process, and Purpose.” The American Statistician 70 (2): 129–33. Wilkinson, WS. 1954. “Influence of Diethylstilbestrol on Feeding Digestibility and on Blood and Liver Composition of Lambs.” PhD thesis, University of Wisconsin-Madison. Xie, Yihui. 2022. Bookdown: Authoring Books and Technical Documents with r Markdown. https://github.com/rstudio/bookdown. Zuur, Alain F, Elena N Ieno, Neil J Walker, Anatoly A Saveliev, Graham M Smith, et al. 2009. Mixed Effects Models and Extensions in Ecology with R. New York: Springer. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
