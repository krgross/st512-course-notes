[["index.html", "ST 512 course notes Preface", " ST 512 course notes Kevin Gross 2023-02-09 Preface The primary purpose of this document is to serve as a set of course notes for ST 512, Statistical Methods for Researchers II, taught at North Carolina State University. ST 512 is the second semester of a traditional two-semester graduate-level sequence in statistical methods. I eventually hope to convert these notes into an on-line text, but this is an ongoing process. In the meanwhile, these notes are available for use by anyone associated with NCSU. Philosophy These notes take the following perspectives. Statistics is nonintuitive. When it comes to statistics, researchers cannot necessarily rely on their common sense to lead them towards correct answers. Statistical reasoning is non-intuitive (Kahneman (2011)), and the foundational ideas of statistics are elusive. Therefore statistical literacy must be learned. The primary goal of this course is to sharpen students’ statistical literacy so that they may become more effective researchers. The route to conceptual understanding is the detailed study of basic methods. However, one does not develop a deep conceptual understanding merely by discussing concepts. Instead, conceptual understanding is honed in part by studying the details of particular methods to understand why those details matter. When we study the details of a method, the expectation is not that the student will remember those details in perpetuity. Indeed, practicing scientists are unlikely anyway to remember details about statistical methods that they do not use routinely. (This is not a knock on practicing scientists, but is instead simply a statement about the limitations of human memory.) Instead, the point of studying statistical methods in detail is to strengthen conceptual understanding by exploring statistical methods at a reasonably deep level. Examining details now will also make those details more recognizable if and when one faces a future data-analysis task that requires re-engaging those details. That said, the ultimate emphasis of this course is not on the details of the methods, but is instead on the ideas, concepts, and reasoning that underlie statistical thinking. I hope that these notes will deepen readers’ conceptual understanding of statistics and by doing so strengthen their effectiveness as scientists. Except when it comes to sums-of-squares decompositions in ANOVA. The exception to the statement above is ANOVA and the associated sums-of-squares decompositions. At this mathemtical level — that is, without assuming a familiarity with linear algebra — sums-of-squares decompositions and the associated ANOVA tables are poor vehicles for developing conceptual understanding. Instead, they fill texts with inscrutable tables tethered to formulas that lack a compelling underlying logic.1 ANOVA is still worth learning — especially for the analysis of designed experiments — but unless the underlying linear algebra is engaged, ANOVA is more usefully approached as a special case of a regression model. For this reason, these notes introduce regression modeling first and ANOVA second, reversing the path taken by most texts. Confidence intervals deserve more emphasis, and hypothesis tests less. Hypothesis tests have become the primary route to inference in contemporary science, likely because they are the de facto evidentiary standard for announcing results in the scientific literature. This is unfortunate, because statistical significance provides only the thinnest of summaries of pattern in data. Confidence intervals, on the other hand (or even standard errors), are often relegated to a secondary role, even though they provide a richer basis for characterizing pattern and uncertainty. In the fullness of time, these notes will seek to promote the reporting of confidence intervals or standard errors as opposed to hypothesis tests as the primary vehicle for inference. Simplicity in statistical analysis is a virtue. Contemporary statistical software allows anyone to fit complex statistical models. However, just because one can fit a complex model does not mean that one should. For a statistical analysis to have scientific value, it must be understood by both the analyst and the analyst’s audience. Unfortunately, many contemporary statistical methods produce opaque analyses that are impossible for the informed reader to understand without a substantial and unusual investment of time and energy. (It doesn’t help that, in the scientific literature, the details of contemporary analyses are buried in supplementary material that escapes the scrutiny of most reviewers, but that’s another matter.) As a result, informed and well-intentioned readers of the scientific literature have little choice but to accept the analysis at face value without understanding the genesis of the announced results. This state of affairs does not serve science well. It is high time for the scientific community to ask whether complex but opaque statistical analyses are in the best interest of science. For most scientific studies, a simple and transparent analysis provides a more trustworthy vehicle to understanding for both the analyst and the analyst’s audience. These notes will emphasize methods that, when studied, are transparent enough to promote such an understanding. Everyone is a visual learner. No one is convinced by a \\(p\\)-value in isolation, Or at least no one should be, given the ease of making errors in statistical analysis. When an analysis suggests a pattern in data, the best way to understand the pattern is to visualize it with a thoughtfully constructed graphic. Unfortunately, these notes in their current state are not as richly illustrated as they should be. Eventually, I hope the notes will contain a full set of graphics that exemplify how to visualize patterns revealed by statistical analysis. Scope and coverage These notes form the basis for an intermediate course in statistical analysis. These notes do not start from the very beginning, and they presume a basic knowledge of the fundamentals of (frequentist) statistical inference, similar to what one might see in an introductory statistics course. Breiman (2001) wrote a provocative paper some twenty years ago describing two statistical cultures: one of data modeling and a second of algorithmic modeling, thus anticipating the era of machine learning and artificial intelligence in which we increasingly seem to reside. These notes fall squarely within (and even celebrate!) the former culture of data modeling as a path to scientific understanding. Moreover, it seems to me that the data-modeling culture contains at least two different subcultures that map to disciplines that either learn from designed experiments vs. disciplines that rely primarily on so-called observational data.2 The experimental-data culture tends to involve disciplines in the life sciences, favors ANOVA modeling, uses frequentist inference, and codes its models in SAS. The observational-data culture tends to involve disciplines in the social and environmental sciences, favors regression modeling, increasingly embraces Bayesian analysis, and codes its models in R or Python. These notes aim to serve both the experimental and observational subcultures. The saving grace that allows us to do so is that the core statistical model in both subcultures is the linear statistical model, which encompasses both regression and ANOVA. Of course, most graduate students will need to learn specialized statistical methods that are popular in their own field of study. These notes are not meant to cover these specialized methods, and thus they are not meant to embody the whole of statistics. However, study of regression and ANOVA provides an opportunity to master core tools and provides a springboard to the study of more specialized and possibly discipline-specific methods. These notes also deal exclusively with so-called “frequentist” statistical inference. We do not engage Bayesian methods yet, although some Bayesian coverage is eventually forthcoming. This class is also firmly situated in the study of low-dimensional statistical models. We value parsimony, and we take the view that well constructed models are worthy objects of study in their own right. More concretely, we seek to construct statistical models with parameters that correspond to natural phenomena of interest. Algorithmic modeling (that is, machine learning) is outside the scope of these notes. Mathematical level These notes do not assume knowledge of or use any math beyond arithmetic and basic probability. This basic probability includes an understanding of random variables, standard distributions — primarily Gaussian (normal) distributions, but also binomial and Poisson — and basic properties of means and variances. Students who are willing to engage the math a bit more deeply will find that doing so provides a more satisfying path through the material and leads to a more durable understanding. Without knowing the math underneath, one can only learn statistical methods as different recipes in a vast cookbook, a tedious task that taxes the memory and gives statistics courses their reputation for drudgery. For those who are so inclined, learning a bit of the mathematical theory reveals how the methods we study connect with one another, and thus provides a scaffolding to organize the methods sensibly and coherently. Moreover, the underpinning mathematics can be understood with a minimum of calculus. Linear algebra, however, is more essential. Indeed, the linear models that we study are, ultimately, exercises in linear algebra. These notes assume no previous familiarity with linear algebra, and so we will not emphasize the linear algebra underpinnings of the methods. In the fullness of time, I hope that these notes will eventually include sidebars that present the linear algebra underneath the methods, for interested readers. In this day and age, one might ask why it’s necessary to understand the math at all. Indeed, the internet makes it easy to quickly find code for any standard analysis.3 In such a world, the primary task facing an analyst is not so much to get the computer to give you an answer, but instead to confirm that the answer is in fact the one you want. Towards this end, knowing a bit about the math behind the methods makes it possible to determine whether the computer output you’ve obtained is indeed the analysis you hoped for. Throughout, we will try to emphasize simple, quick calculations that can be used to verify that computer output is correct, or indicate that something needs to be fixed. Computing The first portion of these notes (focused on regression) presents analyses in R, while the latter portion (focused on designed experiments) presents analyses in SAS. In the fullness of time, I hope that these notes will include complete code for conducting analyses in both R and SAS, but that is a work in progress. While the notes examine the R and SAS implementation of the methods that it presents, these notes are not intended as a complete guide for learning either R or SAS from square one. The internet abounds with resources for learning the basics of R, and I would not be able to improve on those resources here. In many cases I provide R code for the sake of illustration, but—especially when it comes to data wrangling and to graphics—the code is not meant to be authoritative. ST 512 students will receive instruction in R and SAS coding in the laboratory component fo the course. Readers interested in using R professionally would be well served by consulting Hadley Wickham’s tidyverse style guide. The ideas therein have helped me write substantially cleaner code, even if I haven’t had the discipline to adhere to those ideas in all the code in these notes. That said, the R code in these notes does not fully embrace the piping style of the tidyverse ecosystem and the associated graphical facilities of ggplot. I take this approach because the focus of these notes is on fitting and visualizing traditional statistical models, and it seems to me that the conventional style of R coding is still best suited for this purpose. The piping style of the tidyverse seems better suited to data-science tasks such as wrangling with and visualizing large data sets. As for ggplot, I prefer the style of coding in R’s native graphical facilities, although ggplot can certainly produce high-quality graphics with relatively few lines of code. As a practical matter, these notes are prepared in bookdown (Xie (2022)). While it is possible to compile both R and SAS code on the fly in bookdown, the extensive output produced by SAS does not serve these notes well. As a consequence, SAS output is condensed to show only the most salient portions of the output. Format of the notes Advanced sections are indicated by section titles that begin with stars (\\(^\\star\\)). Shorter sidebars for enrichment appear in gray text and are offset by horizontal rules (like the one following the acknowledgments). This material may be skipped without loss. Sections that are in an early and rougher stage of development are indicated with section titles shown in italics. Acknowledgments and license I am deeply indebted to the R community (R Core Team (2021)) for their project which has done no less than revolutionize data analysis in our times. I also thank the developers of bookdown for providing the platform for these notes (Xie (2022)). These notes are provided under version 3 of the GNU General Public License. Bibliography "],["simple-linear-regression.html", "Chapter 1 Simple linear regression 1.1 The basics of SLR 1.2 Least-squares estimation 1.3 Inference for the slope 1.4 Sums of squares decomposition and \\(R^2\\) 1.5 Fitting the SLR model in R 1.6 Diagnostic plots 1.7 Consequences of violating model assumptions, and possible fixes 1.8 Prediction with regression models 1.9 Regression design 1.10 \\(^\\star\\)Centering the predictor Appendix: Regression models in SAS PROC REG", " Chapter 1 Simple linear regression In statistics, regression models are those that relate the distribution of an output variable to the value(s) of one or several input variables. Characterizing the relationships among input and output variables is central to much of science, and regression methods form the foundation for much of data analysis. We’ll have a lot to say about regression, but we’ll begin with so-called simple linear regression (SLR). SLR models are “simple” in the sense that they contain only one predictor. Because these notes are meant for the intermediate analyst, I’ll assume that you’ve seen SLR before. The purpose of our study here is twofold. First, we’ll use SLR as a familiar venue to review many of the foundational concepts of (frequentist) statistical inference. These ideas are often elusive, so it’s worth reviewing them again to solidify our understanding. Along the way, we’ll encounter some new (!) ideas for interpreting the outcomes of statistical hypothesis test that may improve upon the traditional convoluted definitions. Second, we’ll also introduce some more advanced regression ideas. These ideas will carry over into our study of regression models with several predictors, so it is helpful to study them here in a less complicated setting. For such a fundamental technique, the name “regression” seems a bit odd. Why do we give this most central of tasks such an obscure moniker? The name traces back to Francis Galton’s late 19th century study of the relationship between the heights of individuals and their parents’ heights (Galton (1886)). Galton observed that while the children of taller-than-average or shorter-than-average parents also tend to be taller than average or shorter than average, respectively, the children’s heights tend to be closer to the average height than those of their parents. Galton termed this phenomenon “regression to mediocrity”; today, we don’t like to equate average-ness with mediocrity, so we now refer to the phenomenon as “regression to the mean”. In any case, to characterize this relationship statistically, Galton wrote down equations that were the precursors of the statistical model that we now know as regression. So, the statistical model of regression was first used to describe the empirical phenomenon of regression to the mean, even though statistical regression models are now used in a much broader variety of contexts. All that said, regression to the mean doesn’t just apply to people’s heights; instead, it appears in all sorts of everyday contexts. For example, it helps explains why students who do particularly well on a first exam tend not to do quite so well on a subsequent exam, or why repeating as a league champion is so rare in sports. For more coverage, see e.g. Ch. 17 of Kahneman (2011). 1.1 The basics of SLR Simple linear regression characterizes the relationship between two variables: a predictor variable and a response variable. We will begin with a simple example for context. Example: Individuals in this study consumed a certain number of beers, and their blood alcohol content (BAC) was measured. Data were obtained for \\(n=16\\) individuals.4 Here is a scatter plot of the data: Figure 1.1: BAC vs. beers consumed. To begin, let’s observe that the two variables that a regression model associates are not on equal footing. One variable is designated as the “predictor” and the other variable is designated as the “response”. The predictor variable is denoted by the symbol \\(x\\), and the response variable is denoted by \\(y\\). In plotting, we almost always show the predictor on the horizontal axis and the response on the vertical axis.5 The predictor is also called the “independent” variable because, in a designed experiment, its values are determined by the investigator. The response is also called the “dependent” variable because its distribution depends on the value of the predictor variable, in a way that is determined by nature. For the BAC data, we will identify the number of beers consumed as the predictor and BAC as the response. The regression model associates each value of the predictor variable with a distribution for the response variable. Indeed, the fact that the output of the model is a distribution is what makes this a statistical model, as opposed to some other flavor of mathematical model. A simple linear regression (SLR) is a simple statistical model in which the association between the value of the predictor and the distribution of the response takes a specific form. In particular, in a SLR, the distribution of the response variable is Gaussian (or normal) with a mean that depends linearly on the value of the predictor and a variance that is independent of the value of the predictor. When we plot the fit of a regression model, we typically only plot the regression line. However, the line merely shows how the average of the distribution of the response depends on the predictor. The model has more structure than a plot of the regression line suggests. In terms of an equation, we can write the model using the regression equation \\[\\begin{equation} y_i =\\beta_0 +\\beta_1 x_i +\\varepsilon_i \\tag{1.1}. \\end{equation}\\] In words, we might re-write the equation as \\[ \\mbox{response = intercept + slope} \\times \\mbox{predictor + error}. \\] In the mathematical equation above, the i subscript distinguishes individual data points. For example, \\(y_1\\) is the value of the response associated with the first observation in the data set. Usually, we use the notation \\(n\\) for the total number of data points, and so to be precise we might also write \\(i = 1, \\ldots, n\\). In words, we say that “\\(i\\) varies from 1 to \\(n\\)” or “\\(i\\) ranges from 1 to \\(n\\)”. We’ll suppress the \\(i\\) subscript when we don’t need it. In the SLR model, the equation \\(\\beta_0 + \\beta_1 x\\) shows how the average of the response depends on the predictor value. The parameter \\(\\beta_0\\) is called the intercept, and it gives the value of the regression line when the predictor \\(x = 0\\). As we will see, the value of the regression line at \\(x=0\\) often isn’t a scientifically meaningful quantity, even though we need to know the value to specify the model fully. The parameter \\(\\beta_1\\) is the slope. In SLR, the slope is a parameter tells us by how much regression line rises or falls as the predictor changes. Positive values of the slope indicate that the regression line increases as the predictor increases, and negative values of the slope indicate that the regression line decreases as the predictor increases. The regression line alone is not sufficient to fully specify the entire regression model. To the regression line we add a normally distributed error, denoted by \\(\\varepsilon\\). The error term is a catch-all that subsumes all the other factors that might influence the response that are not included in the predictors. In the context of the BAC example, these might include body weight, metabolism, and/or alcohol content of the beer (if it differed among subjects). Although they look similar, it is important to realize that \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\varepsilon\\) are different beasts. The quantities \\(\\beta_0\\) and \\(\\beta_1\\) are parameters. Recall that in statistics, parameters are quantities that characterize a population. We assume that true values of \\(\\beta_0\\) and \\(\\beta_1\\) exist; those values are just unknown to us. We will estimate these parameters and draw inferences about their values on the basis of data. In contrast, the error term \\(\\varepsilon\\) is a random variable. It does not have one single value, but instead takes a different value for every member of a population. We describe the distribution of the errors across the members of the population using a probability distribution. In simple linear regression, we assume that the random errors have a Gaussian (or normal, or bell-shaped) distribution with mean 0 and variance \\(\\sigma_{\\varepsilon}^{2}\\). We also assume that the random errors are independent among individuals in our sample. A succinct way of stating this is to state that the errors are Gaussian and “independent and identically distributed” (abbreviated “iid”). In notation, we write \\(\\varepsilon_{i} \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}\\left(0, \\sigma_{\\varepsilon }^2 \\right)\\), a statement which we would read as “the errors have a normal (or Gaussian) distribution with mean 0 and variance \\(\\sigma^2_\\varepsilon\\)”. The error variance \\(\\sigma_{\\varepsilon }^2\\) is a parameter, and it measure of the variability in the response that is not explained by the predictor. We will also discuss how to estimate \\(\\sigma_{\\varepsilon }^2\\). (It is also possible to draw statistical inferences for \\(\\sigma_{\\varepsilon }^2\\), although we will not discuss how to do so in these notes.) Before moving on to discussing how to estimate the model parameters, let’s reflect a bit on the slope, \\(\\beta_1\\), because this is the parameter that captures the linear association between the two variables. A particularly nice way to interpret the slope is due to Gelman, Hill, and Vehtari (2020). Their interpretation works like this. Consider two values of the response \\(y_1\\) and \\(y_2\\), associated respectively with two values of the predictor \\(x_1\\) and \\(x_2\\). The regression model says that, on average, the difference \\(y_1 - y_2\\) will equal \\(\\beta_1 \\times (x_1 - x_2)\\). The “on average” part of this interpretation is important because we realize that any two actual observations will also include their respective errors, and so we don’t expect these two observations to differ by exactly \\(\\beta_1 \\times (x_1 - x_2)\\). Second, this interpretation also makes it clear that the regression model predicts that the average difference between two responses will increase or decrease linearly as the difference between their two associated predictor values grows or shrinks. Thus, if the SLR model is appropriate for the BAC data (something we have yet to verify), then the model suggests that the average BAC difference between two individuals who have consumed 1 vs. 2 beers is the same as the average BAC difference between two individuals who have consumed 4 vs. 5 beers, and that both of these differences are one-half as big as the average BAC difference between two individuals who have drank 2.5 vs. 4.5 beers. Our assumption of normally distributed errors has a deeper justification than may meet the eye. If you’ve studied probability, you may have encountered an important result called the Central Limit Theorem. For our purposes, the Central Limit Theorem tells us that if the error results from the combined effect of many small factors added together, then the error’s distribution will be approximately normal. (We will see that regression models are not sensitive to moderate departures from normality, so approximately normal errors are good enough.) This result provides a strong justification for expecting normally distributed errors in many cases. The normality assumption begins to break down when the errors are dominated by only a few factors, or when the factors that contribute to the error combine multiplicitavely. This latter scenario — errors that result from the product of many small influences as opposed to their sum — frequently arises in biology when the response measures some form of population size. Populations grow or shrink multiplicitavely, and so population sizes tend to have right-skewed distributions. It’s also worth noting that we can write the regression model as the sum of the regression line (\\(\\beta_0 + \\beta_1 x\\)) and an error term with mean zero (\\(\\varepsilon\\)) because we have assumed that the errors have a normal distribution. A normal distribution has the special property that we can take a normally distributed quantity, add a constant to it, and the sum will still have a normal distribution. Most statistical distributions do not have this property; for example, a Poisson random variate plus a non-zero constant does not yield a Poisson distributed sum. Some authors find it more natural to write the SLR model as \\(y \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x, \\sigma^2)\\), to emphasize that the response has a Gaussian distribution and that the predictor only affects the mean of this distribution. We will use the style of eq. (1.1), because this style lends itself more readily to mixed-effects models with multiple variance terms. However, the two styles of notation denote the same model. Feel free to use whichever style makes the most sense to you. 1.2 Least-squares estimation The parameters of an SLR model are estimated by the method of least-squares. That is, we find the values of the parameters that minimize the sum of the squared differences between the data points themselves and the line. The estimates are denoted by “hats”, i.e., \\(\\hat{\\beta}_0\\) is the estimate of \\(\\beta_0\\). Other authors use \\(b\\)’s instead of \\(\\hat{\\beta}\\)’s for parameter estimates in regression. Both types of notation commonly appear in the scientific literature. If we were inventing SLR from scratch, we might imagine many possible criteria that we could use to determine the parameter values that provide the best fit of the SLR model. For example, we might contemplate fitting the line that minimized the average absolute difference between the data points and the line. The reason why we favor the least-squares criterion is a direct consequence of the assumption that the errors take a Gaussian distribution.6 In an introductory statistics course, you may have derived formulas for calculating the least-squares estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) by hand. Here, we will rely on software for the necessary computations, although one might note that one could derive the formulas for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) by using basic calculus tools to minimize the error sum-of squares. Using R, we obtain the least-squares fit of the regression model to the BAC data below.7 fm1 &lt;- with(beer, lm(BAC ~ Beers)) summary(fm1) ## ## Call: ## lm(formula = BAC ~ Beers) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.027118 -0.017350 0.001773 0.008623 0.041027 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.012701 0.012638 -1.005 0.332 ## Beers 0.017964 0.002402 7.480 2.97e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02044 on 14 degrees of freedom ## Multiple R-squared: 0.7998, Adjusted R-squared: 0.7855 ## F-statistic: 55.94 on 1 and 14 DF, p-value: 2.969e-06 The least-squares estimates of the intercept and slope for the BAC data are \\(\\hat{\\beta}_0 = -0.013\\) and \\(\\hat{\\beta}_1 = 0.018\\), respectively. Here’s a picture of the scatter-plot with the least-squares line: with(beer, plot(BAC ~ Beers, xlab = &quot;beers consumed&quot;)) abline(fm1) Figure 1.2: SLR fit of BAC vs. beers consumed. The best fitting line shows a positive relationship between BAC and beers consumed. Using the interpretation that we introduced in the previous section, we would say that if we measure the BAC of two people, one of whom has consumed one more beer than the other, on average the BAC of the person who drank more beers will be 0.018 higher than the BAC of the persion who drank fewer beers. Similarly, if we compare the BAC of two people, one of whom drank four more beers than the other, on average the BAC of the person who drank more beers will be \\(0.4 \\times 0.018 = 0.072\\) higher than the person who drank fewer beers, and so on. In a perfect world, we would always include units along with our parameter estimates. In the BAC data, the units of the predictor are perfectly clear (the units are the number of beers), but the units of the response are a bit trickier. The units of BAC are percent by volume, which is often just shortened to percent. So, in the BAC regression model, the units of the intercept are \\(\\hat{\\beta}_0 = -0.013\\%\\), and the units of the slope are \\(\\hat{\\beta}_1 = 0.018\\) percent per beer consumed. Quoting units can get a bit repetitive, so we’ll omit them on occasion, but identifying the units of parameters in your own analysis is a good way to deepen your understanding of what the numbers in the analysis mean. Evaluating the fitted regression line for a given value of the predictor generates a fitted value for each data point. Fitted values are denoted \\(\\hat{y}_i\\). In notation, \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\). (What are the units of fitted values? And why did the error term vanish in the equation for \\(\\hat{y}_i\\)?) The residual for observation \\(i\\), denoted \\(e_i\\), is the difference between the actual observation and the fitted value. In notation, we write \\(e_i = y_i -\\hat{y}_i\\). (What are the units of residuals?) In terms of the data plot, the residuals can be thought of as the vertical differences between the actual data points and the fitted line. In the figure below, the vertical line represents the residual for the individual who consumed 9 beers. Example: The first individual in the data set drank \\(x_1 = 5\\) beers and had a BAC of \\(y_1 = 0.1\\%\\). Find the fitted value and residual for this data point. Answer: \\(\\hat{y}_1 = 0.077\\%\\), \\(e_1 = 0.023\\%\\). The error sum of squares (SSE) is the sum of the squared residuals. Written as a formula, we would write \\[ SSE = \\sum_{i=1}^{n} e_i^{2} = \\sum_{i=1}^{n} \\left(y_{i} - \\hat{y}_i \\right)^{2}. \\] The SSE is a measure of the unexplained variability in the response. The least squares estimates, \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), are called the least squares estimates because they minimize the SSE. We can use the SSE to find an estimate of the error variance parameter by using the formula \\[ s_\\varepsilon^2 = \\dfrac{SSE}{n-2} = MSE \\] We divide by \\(n - 2\\) because there are \\(n - 2\\) degrees of freedom (df) associated with the SSE. When we divide an error sum-of-squares by its degrees of freedom, the resulting quotient is called the “mean-squared error” (MSE). For the BAC data, the SSE is 0.0058, yielding a MSE of \\(0.0058/(16-2) \\approx 0.0004\\). See the gray text at the end of this section for an explanation of why the number of degrees of freedom is \\(n-2\\). Variances are difficult to understand because they are on a squared scale. Thus, the units of the error variance are the units of the response, squared. To place this estimate on a more meaningful scale, we take the square root to obtain the estimate of the residual standard deviation \\(s_{\\varepsilon}\\): \\[ s_{\\varepsilon} =\\sqrt{\\dfrac{SSE}{n-2}} = \\sqrt{MSE} \\] For the BAC data, \\(s_{\\varepsilon} = \\sqrt{0.0004} = 0.020\\). This is a more useful number, as it suggests that a typical deviation between an observed BAC and the corresponding fitted value is 0.020%. (Take a look again at the magnitude of the residuals in the scatterplot of the BAC data, and convince yourself that 0.020% is a reasonable guide to the magnitude of a typical residual.) In the R summary of our model fit, the value of \\(s_{\\varepsilon}\\) is given by the portion of the output labeled “Residual standard error”.8 Degrees of freedom appear frequently in statistical modeling. We will spend quite a bit of effort in these notes keeping track of degrees of freedom, so it’s helpful to understand this concept well. We’ll look carefully at df in the simple case of SLR to build intuition that will carry over into more complicated models. Most error terms, like the SLR error variance \\(\\sigma_\\varepsilon^2\\), are estimated by sums of squares. The concept of degrees of freedom quantifies how many “free differences” are available to compute a sum of squares. Consider the following thought experiment. Suppose that, bizarrely, we knew the values of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in an SLR, and only needed to estimate the error variance \\(\\sigma_\\varepsilon^2\\). We could do so using the sum of squares \\(\\sum_{i=1}^{n}\\left(y_{i} -\\left[\\beta_0 +\\beta_1 x_i \\right]\\right)^2\\). In this case, each of our \\(n\\) data points would contribute a “free difference” to the summation above, and so there would be \\(n\\) free differences with which we could estimate the error variance \\(\\sigma_\\varepsilon^2\\). However, we never know the values of \\(\\beta_0\\) and \\(\\beta_1\\) in advance. Instead, we have to use the data to estimate both \\(\\beta_0\\) and \\(\\beta_1\\). Now, because we have to estimate both \\(\\beta_0\\) and \\(\\beta_1\\), there are only \\(n - 2\\) free differences in the sum of squares \\(\\sum_{i=1}^{n}\\left(y_{i} -\\left[\\hat{\\beta}_0 +\\hat{\\beta}_1 x_i \\right]\\right)^{2}\\). One way to visualize this is to imagine fitting a line to a data set with only \\(n = 2\\) data points (with different \\(x\\) values). The line would be guaranteed to pass through both points, and consequently both residuals would equal 0. Because both residuals equal 0, the SSE would also equal 0. However, the SSE doesn’t equal 0 because the actual value of \\(\\sigma_\\varepsilon^2\\) equals 0. Instead, the SSE equals 0 because there is no information remaining to estimate the residual variance. In general, when we have to use the same data set to estimate the parameters that determine the average value of the response and to estimate the residual variance, then each parameter that we have to estimate in the mean component of the model eliminates a free difference from the sum of squares \\(\\sum_{i=1}^{n}\\left(y_{i} -\\hat{y}_{i} \\right)^{2}\\). To convert the sum of squares into an estimate of an error variance, we need to count the number of free differences (or degrees of freedom) correctly, and divide the sum of squares by the appropriate number of df to make sure we get a good estimate of the variance. 1.3 Inference for the slope To draw statistical inferences about the slope parameter \\(\\beta_1\\), we make the following assumptions: Linearity: The average value of the response is a linear function of the predictor. Equal variance (“homoscedasticity”): The variance of the error terms (the \\(\\varepsilon_i\\)’s) is the same for all observations. Independence: The error terms are independent of one another. Normality. The errors have a normal (i.e., bell-shaped, or Gaussian) distribution. Note that assumption number 1 deals with the mean component of the model, while assumptions 2–4 deal with the error component of the model. 1.3.1 Standard errors As intelligent scientists, we realize that estimates are not exactly equal to the parameters that they seek to estimate. We can characterize the uncertainty in parameter estimates in different ways. One tool that we have for quantifying uncertainty in parameter estimates is to calculate a standard error. In general, a standard error quantifies the variability in an estimate that is attributable to random sampling. Most parameter estimates that we will encounter have known formulas for their standard errors. In most cases, these formulas are complicated, and we will rely on computers to calculate standard errors for us. However, the formula for the standard error of the slope parameter in SLR is interesting to examine because it contains a valuable insight that we can use when collecting data for a regression study. The standard error of \\(\\hat{\\beta}_1\\), denoted \\(s_{\\hat{\\beta}_1}\\), is given by the formula \\[\\begin{equation} s_{\\hat{\\beta}_1} = \\dfrac{s_{\\varepsilon}}{\\sqrt{S_{xx} } } \\tag{1.2} \\end{equation}\\] where \\(S_{xx} =\\sum_{i}\\left(x_{i} -\\bar{x}\\right)^2\\) quantifies the dispersion in the predictor variables. Although this formula looks a bit daunting, there’s some intuition to be gained here, and a lesson for experimental design. Suppose we had designed a regression experiment in which all of the individuals were assigned similar values of the predictor. In this case, \\(S_{xx}\\) would be small, and consequently the standard error \\(s_{\\hat{\\beta}_1}\\) would be large. Conversely, if the values of the predictor were very different among individuals in the study, then \\(S_{xx}\\) would be large and the standard error \\(s_{\\hat{\\beta}_1}\\) would be small. Thus, if we want a precise estimate of the slope, we should choose predictor values that span the range over which we want to learn. Thought question: Following this line of reasoning, is it a good idea to design a study so that half the individuals are assigned a very large value of the predictor, and the other half are assigned a very small value? Why or why not? For the BAC example, \\(s_{\\hat{\\beta}_1} = 0.0024\\). (The units are the same units as the slope, or percent per beer consumed for the BAC data.) This tells us that, over many hypothetical repetitions of this same experiment, a typical difference between our estimate of the slope and its true value is 0.0024. This information sharpens our understanding of the precision of the estimate.9 1.3.2 Confidence intervals A second way in which we can measure the uncertainty in a parameter estimate is to calculate a confidence interval (CI). Recall that the general formula for a confidence interval associated with a statistic is: \\[ \\mathrm{estimate} \\pm \\mathrm{critical\\ value} \\times \\mathrm{standard\\ error} \\] Critical values are found either by consulting a table (and re-living the good old days) or using the internet or a computer program. Critical values depend on the confidence level that you want to associate with the CI. Although it seems a bit backwards, we typically denote the confidence level of a CI as \\(100 \\times \\left(1-\\alpha \\right)\\%\\). Thus, for a 95% confidence interval (a common choice), \\(\\alpha = 0.05\\). Alternatively, we might seek a 99% CI, in which case \\(\\alpha = 0.01\\). To construct a CI for \\(\\beta_1\\) , we find the appropriate critical values from a \\(t\\)-distribution with \\(n - 2\\) df. For a \\(100\\times \\left(1-\\alpha \\right)\\%\\) CI, the critical value is the value that “cuts-off” an upper tail of \\(\\alpha / 2\\) %. For example, to calculate a 99% CI for \\(\\beta_{1}\\), we need to find the critical value of a \\(t\\)-distribution with 14 df that cuts-off an upper 0.5%-tail. Using an online calculator, or another tool, we find that this critical value is 2.977. Thus, a 99% CI is 0.018 \\(\\pm\\) 2.977 \\(\\times\\) 0.0024 = (0.011, 0.025). Recall that the appropriate interpretation of the confidence level a CI is fairly tricky. A proper interpretation is that, if we were to repeat this experiment a large number of times, and calculate a 99% CI for each experiment, in the long run 99% of those CIs would contain the true value of \\(\\beta_1\\). Of course, in real life, we’ll only do the experiment once, and we don’t know if our experiment is one of the 99% in which the CI contains the true parameter value or not. It is often tempting to abbreviate this interpretation by saying that ``there is a 99% chance that \\(\\beta_1\\) is in the CI’’, although technically this interpretation is incorrect (because any single CI either contains the parameter or it doesn’t).10 Note also that there is a trade-off between the confidence level and the width of the interval. If we wanted greater confidence that our interval contained the true parameter value, we could increase the confidence level. However, increasing the confidence level increases the width of the interval, and thus provides less information about the true parameter value in some sense. If we follow this argument to its (il)logical extreme, a 100% CI for \\(\\beta_1\\) covers the entire number line. Now we are fully confident that our interval contains \\(\\beta_1\\), but at the cost of having no information whatsoever about the actual value of \\(\\beta_1\\). 1.3.3 Statistical hypothesis tests Finally, a third way to characterize the statistical uncertainty in \\(\\hat{\\beta}_1\\) is to conduct a statistical hypothesis test. Recall that statistical hypotheses are statements about the values of unknown parameters, and a statistical hypothesis test is a way to measure the strength of evidence against a “null hypothesis”. In the context of SLR, we are almost always interested in testing the null hypothesis that the true value of the slope parameter is equal to zero. In notation, we write this as \\(H_0: \\beta_1 = 0\\). Evidence against this null hypothesis is taken as evidence that the predictor is linearly related to the response. Recall that in statistical hypothesis testing, we must also specify an alternative hypothesis. In SLR, we are almost always interested in testing \\(H_0: \\beta_1 = 0\\) vs. the two-sided alternative \\(H_a: \\beta_1 \\ne 0\\). We conduct a statistical hypothesis test by first calculating a test statistic. In general, formulas for test statistics take the form: \\[ \\mbox{test statistic} = \\dfrac{\\mbox{parameter estimate} - \\mbox{value of parameter under }H_0} {\\mbox{standard error}} \\] Test statistics have the property that if the null hypothesis is true, then the test statistic has a known sampling distribution. In the case of testing \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\) in SLR, if the null hypothesis is true, then the test statistic will have a \\(t\\)-distribution with \\(n-2\\) df. In notation, the test statistic is \\[ t=\\frac{\\hat{\\beta}_{1} -0}{s_{\\hat{\\beta }_{1} } } =\\frac{\\hat{\\beta }_{1} }{s_{\\hat{\\beta }_{1} } } \\] In SLR, this test is so common that the value of the \\(t\\)-statistic is provided automatically by most statistical software packages, including R. For the BAC data, the \\(t\\)-statistic associated with the test of \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\) is \\(t = 7.48\\). Values of the test statistic by themselves are not terribly enlightening. Instead, we use the test statistic to find a \\(p\\)-value. \\(P\\)-values are famously difficult to interpret, and those difficulties in interpretation have impeded their proper use. In 2016, a blue-ribbon panel of experts were convened by the American Statistical Association (the leading professional organization for statisticians in the US) to take the remarkable step of issuing a policy statement regarding the use of \\(p\\)-values. That statement (Wasserstein and Lazar (2016)) defines a \\(p\\)-value as follows: &gt; Informally, a \\(p\\)-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value. (Bear in mind that this definition is the work of two dozen of the world’s leading statisticians.) In the context of the test of \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\) in SLR, this means finding the probability that a \\(t\\)-statistic with \\(n-2\\) df is at least as different from zero as the value observed. For a two-sided alternative hypothesis, we say “different from zero’ because the sign (positive vs. negative) of the \\(t\\)-statistic is irrelevant. Be careful, though: for a one-sided alternative hypothesis, the sign of the observed \\(t\\)-statistic is critical! For the BAC data, we find the area under the tail of a \\(t\\)-distribution with 14 df that is greater than 7.48, and then (because the \\(t\\)-distribution is symmetric) multiply by 2. That is, \\[\\begin{align*} p &amp; = \\mathrm{Pr}\\!\\left\\{ t_{14} &lt; -7.48\\right\\} +\\mathrm{Pr}\\!\\left\\{ t_{14} &gt; 7.48\\right\\} \\\\ &amp; = 2 \\times \\mathrm{Pr}\\!\\left\\{ t_{14} &gt;7.48 \\right\\} \\\\ &amp; = 3\\times 10^{-6} \\end{align*}\\] Thus, there is exceedingly strong evidence that BAC is related to the number of beers consumed. My NCSU colleague Ryan Martin suggests that we interpret the \\(p\\)-value as the plausibility of the null hypothesis (Martin (2017)). Thus, small \\(p\\) values correspond to null hypotheses that are not plausible in light of the data, and large \\(p\\) values (those nearer to 1) indicate that null hypothesis is plausible in light of the data. The good news here is that “plausibility” in this context has a rigorous mathematical meaning, and that meaning is more or less exactly what the everyday definition of “plausibility” suggests. The less good news is that understanding this meaning exactly requires the mathematics imprecise probability, which is beyond the scope of this and most statistics courses today. Nevertheless, it strikes me as the best available option for interpreting \\(p\\)-values. Continuing in this vein, we can plot the \\(p\\)-value associated with the test of \\(H_0: \\beta_1 = \\beta_{1,0}\\) vs. \\(H_a: \\beta_1 \\ne \\beta_{1,0}\\) for any parameter value \\(\\beta_{1,0}\\) that we might assume under the null. This plot shows the \\(p\\)-value function, or, following along the lines of the interpretation above, what we might call the plausibility function. Here is a look at the plausibility function for \\(\\beta_1\\) for the BAC data: b1.hat &lt;- 0.017964 b1.se &lt;- 0.002402 p_val &lt;- function(b1) { t.stat &lt;- (b1.hat - b1) / b1.se pt(-abs(t.stat), df = 14, lower.tail = TRUE) + pt(abs(t.stat), df = 14, lower.tail = FALSE) } curve(p_val, from = 0, to = 0.03, xlab = expression(beta[1]), ylab = &quot;plausibility&quot;, yaxt = &quot;n&quot;) axis(2, at = c(0, 0.5, 1), las = 1) abline(v = b1.hat, lty = &quot;dotted&quot;) axis(3, at = b1.hat, lab = expression(hat(beta)[1])) Another nice feature of the \\(p\\)-value function is that we can find a \\(100 \\times (1 - \\alpha)\\%\\) confidence interval (or, more generally, a confidence region) by taking all those parameter values that are individually at least \\(\\alpha\\%\\) plausible. So, for example, a 90% confidence interval consists of all those parameter values that are at least 10% plausible. For the BAC data, we can show this confidence interval as: curve(p_val, from = 0, to = 0.03, xlab = expression(beta[1]), ylab = &quot;plausibility&quot;, yaxt = &quot;n&quot;) axis(2, at = c(0, 0.5, 1), las = 1) axis(2, at = 0.1, col = &quot;red&quot;, las = 1) abline(h = 0.1, col = &quot;red&quot;, lty = &quot;dashed&quot;) (conf.limits &lt;- confint(fm1, level = 0.9)) ## 5 % 95 % ## (Intercept) -0.03495916 0.009557957 ## Beers 0.01373362 0.022193906 abline(v = conf.limits[2, ], col = &quot;red&quot;, lty = &quot;dotted&quot;) This graph nicely illustrates the tight connection between confidence intervals and hypothesis tests. For example, a 90% confidence interval consists of all those parameter values that we would fail to reject at the 10% significance level. While \\(p\\)-value curves have been around a long time, statisticians have never agreed on what to do with them. For that reason, they don’t appear commonly in statistics texts. The values above could be found by consulting a table, or by using statistical software such as R. Because the test of \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\) is sufficiently common in SLR, most computer packages will do this calculation for us. We’ll sweep a lot of acrimonious debate about statistical hypothesis testing under the rug and simply say that some scientists like to make a decision about whether or not to “reject” the null hypothesis. In contemporary practice, most scientists make these “reject” or “do not reject” decisions by comparing the \\(p\\)-value to the test’s significance level, which is usually denoted by \\(\\alpha\\). The significance level (or size) of a test is the frequency with which one would erroneously reject a true null hypothesis; you might also think of it as the allowable false-positive rate. Consequently, tests with more exacting thresholds for statistical signficance require more evidence against the null to reject it. Most scientists conventionally make reject / do not reject decisions with a significance level of \\(\\alpha = .05\\), but you are free to use whatever significance level you deem appropriate. If \\(p \\le \\alpha\\), we reject the null hypothesis; otherwise, we fail to reject it. (Remember that we never `accept’ the null hypothesis. We only fail to reject it.) Although it is rare, we can also entertain so-called ‘one-sided’ alternative hypotheses. For example, suppose that we were uninterested in the (somewhat nonsensical) possibility that the numbers of beers consumed decreased BAC, and only were interested in measuring the evidence that the numbers of beers consumed increases BAC. To do so, we might test the same null hypothesis \\(H_0: \\beta_1 \\leq 0\\) vs. the one-sided alternative \\(H_a: \\beta_1 &gt; 0\\). To conduct this test, the test statistic is still \\[ t=\\dfrac{\\hat{\\beta }_{1} -0}{s_{\\hat{\\beta }_{1} } } =\\dfrac{0.0180}{0.0024} =7.48. \\] However, because the alternative hypothesis is one-sided, to calculate a \\(p\\)-value, we interpret “equal to or more extreme than its observed value” as the probability of observing a test statistic greater than 7.48, i.e., \\[ p=\\mathrm{Pr}\\!\\left\\{ t_{14} &gt;7.48\\right\\} =1.5\\times 10^{-6} \\] We would then reject \\(H_0: \\beta_1 = 0\\) in favor of the one-sided alternative \\(H_a: \\beta_1 &gt; 0\\) at the \\(\\alpha = .05\\) significance level. Finally, although it doesn’t make much sense in terms of what we know about alcohol, we could consider testing \\(H_0: \\beta_1 \\geq 0\\) vs. the one-sided alternative \\(H_a: \\beta_1 &lt; 0\\). Again, the test statistic is the same (\\(t\\) = 7.48), but now evidence against the null and in favor of the alternative is provided by negative values of the test statistic, so the p-value is the probability of observing a test statistic less than 7.48, i.e., \\[ p=\\mathrm{Pr}\\!\\left\\{ t_{14} &lt; 7.48\\right\\} = 1 - \\mathrm{Pr}\\!\\left\\{ t_{14} &gt; 7.48\\right\\} \\approx 0.9999. \\] Thus, there is no evidence that would allow us to reject \\(H_0: \\beta_1 = 0\\) in favor of the one-sided alternative \\(H_a: \\beta_1 &lt; 0\\). One final note: Although it is rarely done, there is no reason why we must restrict ourselves to testing \\(H_0: \\beta_1 = 0\\). We could in fact test any null hypothesis. For example, suppose conventional wisdom held that each additional beer consumed increased BAC by 0.02, and we were interested in asking if these data contain evidence that the conventional wisdom is false. Then we could test \\(H_0: \\beta_1 = 0.02\\) vs. \\(H_a: \\beta_1 \\ne 0.02\\), although we have to calculate the test statistic and \\(p\\)-value manually instead of relying on computer output: \\[\\begin{align*} t &amp; = \\dfrac{\\hat{\\beta}_1 -0.02}{s_{\\hat{\\beta}_1 }} \\\\ &amp; = \\dfrac{0.0180-0.02}{0.0024} \\\\ &amp; =-0.83 \\\\ \\\\ p &amp; = \\mathrm{Pr}\\!\\left\\{t_{14} &lt;-0.83\\right\\} +\\mathrm{Pr}\\!\\left\\{t_{14} &gt;0.83\\right\\} \\\\ &amp; = 2 \\times \\mathrm{Pr}\\!\\left\\{ t_{14} &gt;0.83\\right\\}\\\\ &amp; = 0.421. \\end{align*}\\] Thus, \\(H_0: \\beta_1 = 0.02\\) is reasonably plausible in light of these data. Do be mindful of the distinction between a statistical hypothesis and a scientific hypothesis. The following excerpt from an article by B. Dennis and M.L. Taper (Dennis and Taper (1994)) puts it nicely: A statistical hypothesis is an assumption about the form of a probability model, and a statistical hypothesis test is the use of data to make a decision between two probability models. A scientific hypothesis, on the other hand, is an explanatory assertion about some aspect of nature. Thus, while a statistical hypothesis can often embody a scientific hypothesis, a scientific hypothesis does not always boil down to a statistical hypothesis. When the ideas of hypothesis testing were first being developed, there was stark disagreement about what the output of a hypothesis test should be. R.A. Fisher argued that a hypothesis test should quantify how compatible the data are with the null hypothesis, relative to the universe of alternatives contained in the alternative hypothesis. Fisher argued that the appropriate tool for this purpose was the \\(p\\)-value. In contrast, Jerzy Neyman and Egon Pearson argued that the result of a hypothesis test should be a decision about whether or not to reject the null. Of course, the two approaches can often be combined by specifying the rejection region (the set of outcomes that would cause the analyst to “reject” the null) in terms of the \\(p\\)-value. While Fisher, Neyman, and Pearson argued vehemently, contemporary practice typically reports both a \\(p\\)-value and a reject / fail-to-reject decision, even if it may be difficult to articulate an entirely coherent rationale for doing so. 1.3.4 Inference for the intercept Most statistical packages automatically provide the standard errors for the intercept, \\(s_{\\hat{\\beta}_0}\\), as well as a test of \\(H_0: \\beta_0 = 0\\) vs. \\(H_a: \\beta_0 \\ne 0\\). Sometimes this is a meaningful test, but usually it isn’t. The scientific context of the problem will determine whether or not it makes sense to pay attention to this test. There is a special type of regression called ``regression through the origin’’ that is appropriate when we can assume \\(\\beta_0 = 0\\) automatically. Should we use regression through the origin for the BAC example? 1.4 Sums of squares decomposition and \\(R^2\\) We have already seen that the SSE measures the unexplained variability in the response. \\[ {\\rm SSE}=\\sum _{i=1}^{n}e_{i}^{2} = \\sum _{i=1}^{n}\\left(y_{i} -\\hat{y}_{i} \\right)^{2} \\] We can also define the total sum of squares, SS(Total): \\[ {\\rm SS(Total)}=\\sum _{i=1}^{n}\\left(y_{i} -\\bar{y}\\right)^{2} \\] SS(Total) is a measure of the total variability in the response. Finally, we can define the regression sum of squares, SS(Regression), as \\[ {\\rm SS(Regression)}=\\sum _{i=1}^{n}\\left(\\hat{y}_{i} -\\bar{y}\\right)^{2} \\] SS(Regression) measures the variability in the response that is explained by the regression. The regression sum of squares is also called the model sum of squares, or SS(Model). By a small miracle (actually, by the Pythagorean Theorem), it happens to be true that: \\[ {\\rm SS(Total)=SS(Regression)+SSE} \\] The coefficient of determination, or \\(R^2\\), is the proportion of the variability in the response explained by the regression model. The formula for \\(R^2\\) is \\[ R^2 = \\dfrac{{\\rm SS(Regression)}}{{\\rm SS(Total)}} = 1-\\frac{{\\rm SSE}}{{\\rm SS(Total)}} . \\] \\(R^2\\) is a nice metric because it quantifies how much of the variability in the response is explained by the predictor. Values of \\(R^2\\) close to 1 indicate that the regression model explains much of the variability in the response, while values of \\(R^2\\) close to 0 suggest the regression model explains little of the variability in the response. We’ll also see that \\(R^2\\) is not limited to SLR and in fact has the same interpretation for more complicated regression models that we will examine later. For the BAC example, \\(R^2\\) = 0.80, suggesting that variation in beers consumed explains roughly 80% of the variation in BAC. Mathematically, \\(R^2\\) can also be computed as square of the (sample) correlation coefficient between the fitted values and the response. In SLR, the fitted values and the predictor are perfectly correlated with one another, so \\(R^2\\) is also the square of the sample correlation coefficient between the predictor and the response. 1.5 Fitting the SLR model in R The basic command in R for fitting a regression model is the function lm, short for [l]inear [m]odel. (As the name suggests, the `lm’ function can be used for more than just SLR.) The basic syntax is &gt; lm(response ~ predictor) where “response” and “predictor” would be replaced by the appropriate variable names. The &gt; is the R prompt, and is meant to show what you could type at the command line. Although the above command would work, it would fit the SLR and then forget the model fit. We want to keep the model fit around to analyze it, so we’ll store it in memory under a name of our choosing. Here, we’ll choose the name fm1, although any name would work. Anything proceeded by a pound sign (#) is a comment in R. We’ll assume that the BAC data have already been read into R and reside in memory, and that the variables in the BAC data are named BAC and Beers. Here is code for fitting a SLR model to these data: fm1 &lt;- lm(BAC ~ Beers, data = beer) # The &#39;&lt;-&#39; is the assignment operator. # Here, the output produced by the call to &#39;lm&#39; is stored in memory under # the name &#39;fm1&#39;. We can learn about &#39;fm1&#39; by asking for a summary. summary(fm1) ## ## Call: ## lm(formula = BAC ~ Beers, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.027118 -0.017350 0.001773 0.008623 0.041027 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.012701 0.012638 -1.005 0.332 ## Beers 0.017964 0.002402 7.480 2.97e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02044 on 14 degrees of freedom ## Multiple R-squared: 0.7998, Adjusted R-squared: 0.7855 ## F-statistic: 55.94 on 1 and 14 DF, p-value: 2.969e-06 Let’s examine each portion of the R output above. The portion labeled Call simply tells us what command was used to generate the model. The portion labeled Residuals tells us a five-number summary (minimum, first quartile, median, third quartile, and maximum) of the residuals. The portion labeled Coefficients gives us a table of parameter estimates and standard errors. Each row of the table corresponds to a single parameter. The row labeled (Intercept) obviously corresponds to the intercept. The row labeled with the name of the predictor gives information about the slope parameter. In addition to parameter estimates and standard errors, R (like many computer packages) also automatically generates hypothesis tests of \\(H_0: \\beta_0 = 0\\) vs. \\(H_a: \\beta_0 \\ne 0\\) and \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\). It is up to you, the user, to determine whether or not these tests are informative. Finally, the last block of output provides a variety of additional information. The “residual standard error” (perhaps not the best term) is the estimate of the residual standard deviation, \\(s_{\\varepsilon}\\). R also provides two different \\(R^2\\) values; the \\(R^2\\) that we discussed above is labeled as the “Multiple R-squared”. We will discuss adjusted R-squared later. Finally, the \\(F\\)-statistic corresponds to a `model utility test’, which we will discuss in the context of multiple regression. For now, you might notice that in SLR the p-value of the model-utility test is always equal to the p-value for the test of \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\). We will explain why this is so later. The SS decomposition for a regression model is also referred to as the analysis of variance for the regression model. We can use the `anova’ command in R to obtain the SS decomposition: anova(fm1) ## Analysis of Variance Table ## ## Response: BAC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Beers 1 0.0233753 0.0233753 55.944 2.969e-06 *** ## Residuals 14 0.0058497 0.0004178 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The \\(F\\)-statistic is the model utility test, which we will examine in more detail when we study multiple regression. 1.6 Diagnostic plots We have seen that, in order to draw statistical inferences from a simple linear regression, we need to make several assumptions. Although in everyday life assumptions can get a bad rap, assumptions in statistics are necessary and appropriate. The statistician Don Rubin puts it nicely (Rubin (2005)): Nothing is wrong with making assumptions … they are the strands that link statistics to science. It is the scientific quality of the assumptions, not their existence, that is critical. In regression, we can use diagnostic plots to investigate the scientific quality of our assumptions. The main idea of diagnostic plots is that if the assumptions are appropriate, then residuals should be independent draws from a normal distribution with constant variance (what some might more colorfully describe as “white noise”). Any structure in the residuals indicates a violation of at least one assumption. We list commonly used diagnostic plots below. Although some types of plots are more useful for examining some assumptions than others, there isn’t a strict correspondence between plot types and assumptions. Any plot can reveal a departure from any one of our assumptions. Examples of each for the BAC data and the R code used to generate the plots are provided as examples. 1.Residuals vs. fitted values. Check for non-constant variance (trumpeting). The BAC data shown here don’t show an obvious increase or decrease in variance as the fitted values increase, although the fact that the largest residual is associated with the largest fitted value is notable. We might want to go back and check that data point out. plot(resid(fm1) ~ fitted(fm1), xlab = &quot;Fitted Values&quot;, ylab = &quot;Residuals&quot;) abline(h = 0, lty = &quot;dotted&quot;) Perhaps the most common violation of the regression assumption occurs when the variance of a response increases as the fitted values increase. The tell-tale signature of this violation is a “trumpeting” pattern in the plot of the residuals vs. the fitted values. Indeed, an increasing variance is perhaps more the rule than the exception in some sciences, especially the life sciences. To illustrate, here is a data set that we will study more closely when we study [ANCOVA]. For now, it suffices to say that this is a data set in which the response is the lifespan of a fruitfly, and there are several predictors. Here is a residual plot of the ANCOVA model: Fruitflies that live longer clearly have more variable lifespans. Recall that the Central Limit Theorem gives us a good reason to expect normally distributed residuals when the many small influences that comprise the residual error add together. There’s a related explanation for why increasing variance is so common. When the many small influences that comprise the residual error multiply together instead of adding together, then we tend to observe more variance in the response when the fitted value is larger. Indeed, this is the usual explanation offered for why increasing variance is common in the life sciences, where many processes involve some form of multiplicative growth or decay. This explanation also helps us understand why a log transformation is usually helpful as a remedy for increasing variance, because when we take a log of the response we are converting a multiplicative process into an additive one. Residuals vs. predictor. We can use this plot to check for non-linear trends. If we see a non-linear trend, like a hump-shaped pattern, it might suggest that the true relationship between predictor and response is actually non-linear. For the BAC data, you’ll note that the plot below looks exactly like the plot of residuals vs. fitted values above. This isn’t just coincidence; in fact, residuals vs. fitted values and residuals vs. predictor will always generate exactly the same patterns in SLR. (The reason is because in SLR the fitted value is just a linear function of the predictor.) We want to get in the habit of checking both types of plots, however, because when we start entertaining multiple predictor variables in multiple regression, the plots will no longer be identical. plot(resid(fm1) ~ beer$Beers, xlab = &quot;Beers&quot;, ylab = &quot;Residuals&quot;) abline(h = 0, lty = &quot;dotted&quot;) Residuals vs. variables not in the model, e.g., other predictors, observer, order of observation. In the BAC data, the only other variable we have (for now at least) is the order in which the observations appear in the data set. Without knowing how the data were collected or recorded, it’s impossible to say whether this variable is meaningful. However, the plot suggests a distinctive downward trend – data points that appear early in the data set are associated with positive residuals, and data points that appear later in the data set are associated with negative residuals. What do you think might have caused this trend? plot(resid(fm1), xlab = &quot;Order&quot;, ylab = &quot;Residuals&quot;) abline(h = 0, lty = &quot;dotted&quot;) An obvious way to check the normality assumption is to plot a histogram of the residuals. While this is a straightforward idea, it suffers from the fact that the shape of the histogram depends strongly on how the residuals are grouped into bins. Note how the two histograms below of the BAC residuals provide different impressions about the suitability of the normality assumption. hist(resid(fm1), main = &quot;Bin width = 0.01&quot;, xlab = &quot;Residuals&quot;) hist(resid(fm1), main = &quot;Bin width = 0.02&quot;, xlab = &quot;Residuals&quot;, breaks = 4) An alternative to histograms is a normal probability plot of residuals, also known as a quantile-quantile, or Q-Q, plot. Q-Q plots calculate the empirical quantile of each residual, and compare this to the theoretical quantile from a normal distribution. If the normality assumption is appropriate, the empirical and theoretical quantiles will change at the same rate, so when plotted against one another, they’ll fall on a line. If the normality assumption is not appropriate, the plot of empirical vs. theoretical quantiles will bend. As we’ll see below, the normality assumption is the critical of the assumptions in regression. Thus, unless the Q-Q plot shows big and dramatic bends, we won’t concern ourselves with small bumps and wiggles. The Q-Q plot for the BAC data below doesn’t seem terribly problematic. qqnorm(resid(fm1)) qqline(resid(fm1)) 1.7 Consequences of violating model assumptions, and possible fixes Linearity When the linearity assumption is violated, the model has little worth. What’s the point of fitting a linear model to data when the relationship between predictor and response is clearly not linear? The best fix is to fit a non-linear model using non-linear regression. (We will discuss non-linear regression later.) A second-best option is to transform the predictor and / or the response to make the relationship linear. Independence Inference about regression parameters using naive standard errors is not trustworthy when errors are correlated (there is more uncertainty in the estimates than the naive standard errors suggest). The most common sources of non-independence is either temporal or spatial structure in the data, or if the data are grouped in some way that has been accounted for in the analysis. Arguably, we have seen this with the BAC data, where one way to think about the downward trend of residuals vs. the order of observation is that residuals close together in time tend to be positively correlated. The best, and easiest, way to accommodate this type of dependence is to include (an)other predictor(s) in the model for time or space, or to account for a group structure. A second-best solution is to use specific methods for time-series data or spatial data, which doable, but is fairly involved, and will require considerable additional study. Constant variance Like violations of the independence assumption, violations of the constant-variance assumption cause inference about regression parameters is not trustworthy. Non-constant variance causes there to be more uncertainty in the parameters estimates than the default CIs or \\(t\\)-tests suggest. There are two possible fixes for non-constant variance. If the non-constant variance arises because the response variable has a known, non-normal distribution, then one can use generalized linear models (such as logistic regression for binary data, or Poisson regression for count data). We will touch on generalized linear models briefly at the end of ST 512. Alternatively, if there is no obvious alternative distribution for the response, the usual approach is to transform the response variable to “stabilize” the variance. For better or worse, there used to be a bit of a cottage industry in statistics in developing variance-stabilizing transformations. Remember that transformations come with a cost of diminished interpretability, and be wary of exotic transformations. It is not uncommon to observe data where the variance increases as the mean response increases. Good transformations for this situation are either a log transformation or a square-root transformation.11 Another common non-constant variance problem arises when the response is a percentage or a proportion. In this case, the standard and appropriate transformation is the arcsin-square root transformation, i.e., if the observed response is 10%, the transformed response is \\(\\sin^{-1}(\\sqrt{.1})=0.322\\). Normality Perhaps surprisingly, the consequences of violating the normality assumption are minimal, unless departures from normality are severe (e.g., binary data).12 When one encounters decidedly non-normal data, the usual remedy is to entertain a so-called generalized linear models, i.e., logistic regression for binary data; Poisson regression for count data. Here’s an example of another data set with residuals that are a bit more problematic. These data give the box office take (in millions of US$) vs. a composite rating score from critics’ reviews: movie &lt;- read.table(&quot;data/movie.txt&quot;, head = T, stringsAsFactors = T) with(movie, plot(BoxOffice ~ Score, xlab = &quot;Average rating&quot;, ylab = &quot;Box office take&quot;)) fm1 &lt;- lm(BoxOffice ~ Score, data = movie) abline(fm1) The plots of residuals vs. fitted value show clear evidence of non-constant variance. The Q-Q plot indicates right-skew. Taking a square-root transformation of the response stabilizes the variance nicely: plot(resid(fm1) ~ fitted(fm1), xlab = &quot;Fitted value&quot;, ylab = &quot;Residual&quot;) abline(h = 0,lty = &quot;dashed&quot;) qqnorm(resid(fm1),main = &quot;QQ plot, movie data&quot;) qqline(resid(fm1)) Let’s try a square-root transformation of the response: fm2 &lt;- lm(sqrt(BoxOffice) ~ Score, data = movie) summary(fm2) ## ## Call: ## lm(formula = sqrt(BoxOffice) ~ Score, data = movie) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.60533 -0.17889 -0.07339 0.17983 0.92065 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.114102 0.106000 1.076 0.284 ## Score 0.010497 0.001834 5.722 6.27e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3109 on 138 degrees of freedom ## Multiple R-squared: 0.1918, Adjusted R-squared: 0.1859 ## F-statistic: 32.74 on 1 and 138 DF, p-value: 6.272e-08 plot(fm2) Another commonly used transformation for right-skewed data is the log transformation. Here are residual plots and model output for log-transformed data: fm3 &lt;- lm(log(BoxOffice) ~ Score, data = movie) summary(fm3) ## ## Call: ## lm(formula = log(BoxOffice) ~ Score, data = movie) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.99268 -0.43135 0.00783 0.67263 1.81413 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.634451 0.323390 -8.146 2.01e-13 *** ## Score 0.029984 0.005596 5.358 3.44e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9484 on 138 degrees of freedom ## Multiple R-squared: 0.1722, Adjusted R-squared: 0.1662 ## F-statistic: 28.71 on 1 and 138 DF, p-value: 3.438e-07 plot(fm3) Which transformation do you think is more appropriate? Do the different transformations lead to different qualitative conclusions regarding the statistical significance of the relationship between reviewer rating and box office take? Here’s a second example using a data set that gives the highway fuel efficiency (in mpg) and vehicle weight of 1999 model cars: cars &lt;- read.table(&quot;data/cars.txt&quot;, head = T) with(cars, plot(mpghw ~ weight, xlab = &quot;Vehicle weight (lbs)&quot;, ylab = &quot;Highway mpg&quot;)) fm1 &lt;- lm(mpghw ~ weight, data = cars) abline(fm1) plot(resid(fm1) ~ fitted(fm1), xlab = &quot;Fitted value&quot;, ylab = &quot;Residual&quot;) abline(h = 0,lty = &quot;dashed&quot;) The relationship between highway mpg and vehicle weight is clearly non-linear, although that is seen most clearly from the plot of residuals vs. fitted values. We will discuss modeling non-linear relationships later. Here are some additional comments: What about outliers? The famous statistician George Box was fond of saying that outliers can be the most informative points in the data set. If you have an outlier, try to figure out why that point is an outlier. Discard outliers only if a good reason exists for doing so – resist the temptation to ``scrub’’ your data. Doing so is tantamount to cheating. If you absolutely must remove an outlier, at least report the model fits both with and without the outliers included. Be particularly wary of data points associated with extreme \\(x\\)-values. These points can be unduly influential. (See discussion in the multiple-regression installment of the notes on leverage, standardized residuals, and Cook’s distance.) What about transforming the \\(x\\)-variable? Remember that there are no assumptions about the distribution of the \\(x\\)-variable. However, transformations of the \\(x\\)-variable can also make non-linear relationships into linear ones. Remember though that transformations tend to lessen interpretability. Don’t extrapolate the regression line beyond the range of the \\(x\\)-variable observed in the data. Remember that statistical models are only valid to the extent that data exist to support them. Although it’s often overlooked, remember that the standard regression model also assumes that the predictor is measured without error. If there’s error in the predictor as well as the response, then the estimated slope will be biased towards 0. If the error in the predictor is comparable to the error in the response, then consider a regression model that allows for variability in the predictor. These models go by multiple names, but they are most often called ``major-axis regression’’. 1.8 Prediction with regression models Regression models are regularly used for prediction. Consider a new value of the predictor \\(x^\\star\\). There are two different types of predictions we could make: What is the average response of the population at \\(x^\\star\\)? What is the value of a single future observation at \\(x^\\star\\)? Point estimates (i.e., single best guesses) are the same for both predictions. They are found by simply plugging \\(x^\\star\\) into the fitted regression equation. Example. Suppose every grad student at NCSU drinks 2.5 beers. What do we predict the average BAC of this population to be? \\[\\begin{align*} \\hat{y}^\\star &amp; = \\hat{\\beta }_{0} +\\hat{\\beta }_{1} x^\\star \\\\ &amp; = -0.013 + 0.018 \\times 2.5\\\\ &amp; = 0.032 \\end{align*}\\] Suppose Danny drinks 2.5 beers. What do we predict Danny’s BAC to be? \\[ \\hat{y}^\\star = 0.032 \\] However, the uncertainty in these two predictions is different. Predictions of single future observations are more uncertain than predictions of population averages (why?). We quantify the uncertainty in prediction 1 with a confidence interval. We quantify the uncertainty in prediction 2 with a prediction interval. A prediction interval (PI) is just like a confidence interval in the sense that you get to choose the coverage level. i.e., a 95% prediction interval will contain a single new prediction 95% of the time, while a 99% prediction interval will contain a single new prediction 99% of the time. All else being equal, a 99% prediction interval will be wider than a 95% prediction interval. Both confidence intervals and prediction intervals follow the same general prescription of \\[ \\mbox{estimate} \\pm \\mbox{critical value} \\times \\mbox{standard error} \\] Both also use the same point estimate, \\(\\hat{y}^\\star\\), and the same critical value (taken from a \\(t\\)-distribution with \\(n-2\\) df). However, the standard errors differ depending on whether we are predicting an average response or a single future observation. If you find formulas helpful, you might derive some insight from the formulas for these two standard errors. For an average population response, the standard error is \\[ s_{\\varepsilon} \\sqrt{\\frac{1}{n} +\\frac{\\left(x^\\star -\\bar{x}\\right)^{2} }{S_{xx} } } \\] while for a single future observation, the standard error is \\[ s_{\\varepsilon} \\sqrt{1+\\frac{1}{n} +\\frac{\\left(x^\\star -\\bar{x}\\right)^{2} }{S_{xx} } } \\] Thus, the width of a CI or PI depends on the following: The type of interval (all else being equal, a PI is wider than a CI; note the extra ‘1’ in the formula for the standard error of a single future observation). The coverage level (all else being equal, higher coverage requires a wider interval). The unexplained variability in the data (all else being equal, larger MSEs yield wider intervals). The distance between \\(x^\\star\\) and the average predictor value, \\(\\bar{x}\\) (all else being equal, predictions are more uncertain further away from \\(\\bar{x}\\)). The function predict can be used to calculate these intervals in R: fm1 &lt;- lm(BAC ~ Beers, data = beer) new.data &lt;- data.frame(Beers = 2.5) predict(fm1, interval = &quot;confidence&quot;, newdata = new.data) ## fit lwr upr ## 1 0.0322088 0.01602159 0.04839601 predict(fm1, interval = &quot;prediction&quot;, newdata = new.data) ## fit lwr upr ## 1 0.0322088 -0.01452557 0.07894317 predict(fm1, interval = &quot;prediction&quot;, newdata = new.data, level = 0.90) ## fit lwr upr ## 1 0.0322088 -0.006169709 0.07058731 Regression (solid line), 95% confidence intervals (dashed lines), and 95% prediction intervals (dotted lines) for the beer data. Note that both confidence and prediction intervals widen near the edges of the range of the predictor. 1.9 Regression design Regression models can be used both for observational and experimental data. In some experiments, the experimenter has control over the values of the predictor included in the experiment. Gotelli, Ellison, et al. (2004) (pp. 167-9) give the following guidelines for a regression design with a single predictor: Ensure that the range of values sampled for the predictor variable is large enough to capture the full range of responses by the response variable. Ensure that the distribution of predictor values is approximately uniform within the sampled range. Once the values of the predictor to be included in the experiment have been chosen, these values should be randomly assigned to the experimental units. Note that randomization does not require randomly choosing the values of the predictor to be included in the experiment! 1.10 \\(^\\star\\)Centering the predictor While it isn’t essential, it can be useful to redefine the predictor in a regression as the difference between the observed value and the average value of the predictor. For example, in the BAC data, we can define a centered version of the number of beers consumed by \\[ x^{ctr} =x -\\bar{x} \\] Let’s try regressing the response against the centered version of the predictor: beer$beers.c &lt;- beer$Beers - mean(beer$Beers) head(beer) ## Beers BAC beers.c ## 1 5 0.100 0.1875 ## 2 2 0.030 -2.8125 ## 3 9 0.190 4.1875 ## 4 8 0.120 3.1875 ## 5 3 0.040 -1.8125 ## 6 7 0.095 2.1875 beer_slr_ctr &lt;- lm(BAC ~ beers.c, data = beer) summary(beer_slr_ctr) ## ## Call: ## lm(formula = BAC ~ beers.c, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.027118 -0.017350 0.001773 0.008623 0.041027 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.073750 0.005110 14.43 8.47e-10 *** ## beers.c 0.017964 0.002402 7.48 2.97e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02044 on 14 degrees of freedom ## Multiple R-squared: 0.7998, Adjusted R-squared: 0.7855 ## F-statistic: 55.94 on 1 and 14 DF, p-value: 2.969e-06 The main advantage of centering the predictors is that the intercept now has a nice interpretation. Namely, the intercept is now the value of the regression line when \\(x = x^{ctr}\\), which happens to equal the average value of \\(y\\) in the data set. Importantly, we have accomplished this without changing anything about the linear association between the predictor and the response, so our inference for the slope remains unchanged. This is perhaps only a small victory, but it’s a nice touch. Centering the predictor also eases the interpretation of regression parameters in more complicated models with interactions, as we will see later. Appendix: Regression models in SAS PROC REG There are two main procedures (‘PROCs’ for short) that can be used to fit regression models: PROC REG (for REGression) and PROC GLM (for General Linear Model). As the names suggest, GLM is more versatile, but both can be used for regression. Let’s assume the BAC data have already been loaded into memory in a data set called ‘beer’, and the pertinent variables reside under the variable names ‘bac’ and ‘beers’. Here is sample code for fitting an SLR using PROC REG, and some edited output: proc reg data = beer; model bac = beers; run; The SAS System The REG Procedure Root MSE 0.02044 R-Square 0.7998 Dependent Mean 0.07375 Adj R-Sq 0.7855 Coeff Var 27.71654 Parameter Estimates Parameter Standard Variable DF Estimate Error t Value Pr&gt;|t| Intercept 1 -0.01270 0.01264 -1.00 0.3320 beers 1 0.01796 0.00240 7.48 &lt;.0001 Note that even though the output is arranged differently, the parameter estimates and inference provided are exactly the same, regardless of whether one uses PROC REG, PROC GLM, or R. Bibliography "],["multiple-regression.html", "Chapter 2 Multiple regression 2.1 Multiple regression basics 2.2 \\(F\\)-tests for several regression coefficients 2.3 Categorical predictors 2.4 Interactions between predictors 2.5 (Multi-)Collinearity 2.6 Variable selection: Choosing the best model 2.7 Leverage, influential points, and standardized residuals Appendix: Regression as a linear algebra problem", " Chapter 2 Multiple regression In this chapter, we examine regression models that contain several predictor variables. Thankfully, many of the ideas from the simple linear regression also apply to regression models with several predictors. After an overview of the basics, this chapter will focus on the new aspects of regression modeling that arise when considering several predictors. 2.1 Multiple regression basics 2.1.1 The multiple regression model Just as SLR was used to characterize the relationship between a single predictor and a response, multiple regression can be used to characterize the relationship between several predictors and a response. Example. In the BAC data, we also know each individual’s weight and gender: beer &lt;- read.csv(&quot;data/beer2.csv&quot;, head = T, stringsAsFactors = T) head(beer) ## BAC weight beers ## 1 0.100 132 5 ## 2 0.030 128 2 ## 3 0.190 110 9 ## 4 0.120 192 8 ## 5 0.040 172 3 ## 6 0.095 250 7 A plot of the residuals from the BAC vs. beers consumed model against weight strongly suggests that some of the variation in BAC is attributable to differences in weight: fm1 &lt;- with(beer, lm(BAC ~ beers)) plot(x = beer$weight, y = resid(fm1), xlab = &quot;weight&quot;, ylab = &quot;residual&quot;) abline(h = 0, lty = &quot;dotted&quot;) Figure 1.1: SLR residuals vs. weight. To simultaneously characterize the effect that the variables “beers” and “weight” have on BAC, we might want to entertain a model with both predictors. In words, the model is \\[ \\mbox{BAC} = \\mbox{intercept} + \\mbox{(parameter associated with beers)} \\times \\mbox{beers} + \\mbox{(parameter associated with weight)} \\times \\mbox{weight} + \\mbox{error} \\] where (for the moment) we are intentionally vague about what we mean by “parameter associated with beers”. As in SLR, the error term can be interpreted as a catch-all term that includes all the variation not accounted for by the linear associations betwen the response and the predictors “beers” and “weight”. In mathematical notation, we can write the model as \\[\\begin{equation} y = \\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +\\varepsilon \\tag{2.1} \\end{equation}\\] We use subscripts to distinguish different predictors. In this case, \\(x_1\\) is the number of beers consumed and \\(x_2\\) is the individual’s weight. Of course, the order in which we designate the predictors is arbitrary. There are a variety of ways to think about this model. As in SLR, we can separate this model into a mean or signal component \\(\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2\\) and an error component \\(\\varepsilon\\). Note that the mean component is now a function of two variables, and suggests that the relationship between the average response and either predictor is linear. If we wish to make statistical inferences about the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\beta_2\\) (which we do), then we need to place the standard assumptions on the error component: independence, constant variance, and normality. In notation, \\(\\varepsilon \\sim \\mathcal{N}\\left(0,\\sigma_{\\varepsilon}^2 \\right)\\). We can also think about this model geometrically. Recall that in SLR, we could interpret the SLR model as a line passing through a cloud of data points. With 2 predictors, we are now fitting a plane to data points that “exist” in a three- dimensional data cloud. As in SLR, we use the least squares criteria to find the best-fitting parameter estimates. That is to say, we will agree that the best estimates of the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\beta_2\\) are the values that minimize \\[\\begin{eqnarray*} SSE &amp; = &amp; \\sum_{i=1}^n e_i^2 \\\\ &amp; = &amp; \\sum_{i=1}^n \\left(y_i -\\hat{y}_i \\right)^2 \\\\ &amp; = &amp; \\sum_{i=1}^n\\left(y_i -\\left[\\hat{\\beta}_0 +\\hat{\\beta}_1 x_{i1} +\\hat{\\beta}_{2} x_{i2} \\right]\\right)^2 \\end{eqnarray*}\\] In R, we can fit this model by adding a new term to the right-hand side of the model formula in the call to the function ‘lm’: fm2 &lt;- lm(BAC ~ beers + weight, data = beer) summary(fm2) ## ## Call: ## lm(formula = BAC ~ beers + weight, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.0162968 -0.0067796 0.0003985 0.0085287 0.0155621 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.986e-02 1.043e-02 3.821 0.00212 ** ## beers 1.998e-02 1.263e-03 15.817 7.16e-10 *** ## weight -3.628e-04 5.668e-05 -6.401 2.34e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.01041 on 13 degrees of freedom ## Multiple R-squared: 0.9518, Adjusted R-squared: 0.9444 ## F-statistic: 128.3 on 2 and 13 DF, p-value: 2.756e-09 Thus, we see that the LSEs are \\(\\hat{\\beta}_0 = 0.040\\%\\), \\(\\hat{\\beta}_1 = 0.020\\%\\) per beer consumed, and \\(\\hat{\\beta}_{2} = -0.0003\\%\\) per pound of body weight. As in SLR, we can define the fitted value associated with the \\(i\\)th data point as \\(\\hat{y}_i =\\hat{\\beta}_0 +\\hat{\\beta}_1 x_{i1} +\\hat{\\beta}_{2} x_{i2}\\), and the residual associated with the \\(i\\)th data point as \\(e_i =y_i -\\hat{y}_i\\). Here, we require a double subscripting of the \\(x\\)’s, with the first subscript is used to distinguish the individual observations and the second subscript is used to distinguish different predictors. For example, \\(x_{i2}\\) is the value of the second predictor for the \\(i\\)th data point. Example. Find the fitted value and residual for the first observation in the data set, a \\(x_2=132\\) lb person who drank \\(x_1=5\\) beers and had a BAC of \\(y=0.1\\). Answer: \\(\\hat{y}_1 =0.092\\) and \\(e_1 =0.008\\). We can define the error sum of squares as \\(SSE=\\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n \\left(y_i -\\hat{y}_i \\right)^2\\). How many df are associated with the SSE? In this model, there are \\(n-3\\) df associated with the SSE, because 3 parameters are needed to determine the mean component of the model. As in SLR, we can estimate the error variance \\(\\sigma_{\\varepsilon}^2\\) with the MSE, although now we must be careful to divide by the appropriate df: \\[ s_\\varepsilon^2 = MSE = \\frac{SSE}{n-3}. \\] For the model above, \\(s_\\varepsilon = 0.010\\%\\). In general, the equation for an MLR model with any number of predictors can be written: \\[ y_i =\\beta_0 +\\beta_1 x_{i1} +\\beta_2 x_{i2} +\\ldots +\\beta_k x_{ik} +\\varepsilon_i \\] The error term is subject to the standard assumptions of independence, constant variance, and normality. We will use the notation that \\(k\\) is the number of parameters that need to be estimated in the mean component of the model excluding the intercept. (When counting parameters, some texts include the intercept, while others do not. If you consult a text, check to make sure you know what definition is being used.) The SSE will be associated with \\(n - (k + 1)\\) df. Thus, the estimate of \\(\\sigma_{\\varepsilon}^2\\) will be \\[ s_\\varepsilon^2 = MSE = \\frac{SSE}{n-(k+1)}. \\] 2.1.1.1 Sums of squares decomposition and \\(R^2\\). The sums-of-squares decomposition also carries over from SLR. We still have \\({\\rm SS(Total)} = \\sum_{i=1}^n \\left(y_i -\\bar{y}\\right)^2\\), \\({\\rm SS(Regression)} = \\sum_{i=1}^n \\left(\\hat{y}_i -\\bar{y}\\right)^2\\), and \\({\\rm SS(Total) = SS(Regression) + SSE}\\). Thus, we can define \\(R^2\\) using the same formula: \\[ R^2 = \\frac{{\\rm SS(Regression)}}{{\\rm SS(Total)}} = 1- \\frac{{\\rm SSE}}{{\\rm SS(Total)}} \\] We still interpret \\(R^2\\) as a measure of the proportion of variability in the response that is explained by the regression model. In the BAC example, \\(R^2=0.952\\). 2.1.2 Interpreting partial regression coefficients. The \\(\\hat{\\beta}\\)’s in a MLR model are called partial regression coefficients (or partial regression slopes). Their interpretation is subtly different from SLR regression slopes. Misinterpretation of partial regression coefficients is one of the most common sources of statistical confusion in the scientific literature. We can interpret the partial regression coefficients geometrically. In this interpretation, \\(\\beta_j\\) is the slope of the regression plane in the direction of the predictor \\(x_j\\). Imagine taking a “slice” of the regression plane. In the terminology of calculus, \\(\\beta_j\\) is also the partial derivative of the regression plane with respect to the predictor \\(x_j\\) (hence the term “partial regression coefficient”). Another ways to express this same idea in everyday language is that \\(\\beta_j\\) quantifies the linear association between predictor \\(j\\) and the response when the other predictors are held constant, or while controlling for the effects of the other predictors. This is different from an SLR slope, which we can interpret as the slope of the linear association between \\(y\\) and \\(x_j\\) while ignoring all other predictors. Thus, the interpretation of a partial regression coefficient depends on the model in which the parameter is found. Compare the estimated regression coefficients for the number of beers consumed in the SLR model and the MLR model that includes weight. Estimated SLR coefficient for no. of beers consumed: 0.018 Estimated MLR coefficient for no. of beers consumed: 0.020 The coefficients differ because they estimate different parameters that mean different things. The SLR coefficient estimates a slope that does not account for the effect of weight, while the MLR coefficient estimates a slope that does account for the effect of weight. Gelman, Hill, and Vehtari (2020)‘s interpretation of regression coefficients extends nicely here. Recall that we interpreted the SLR slope as saying that if we compare two individuals who consume different numbers of beers (call those values \\(x_1\\) and \\(x_2\\)), then the average difference in the individuals’ BAC equals \\(0.018 \\times (x_1 - x_2)\\). In a multiple regression context, we would now say that if we compare two individuals who weigh the same but who consume different numbers of beers, then the average difference in the individuals’ BAC equals \\(0.020 \\times (x_1 - x_2)\\). Thus, the partial regression coefficient tells us something different than the simple regression coefficient. Therefore, we are not surprised that the values differ. As a final point, note that our interpretation of the partial regression coefficient 0.020 above does not depend on the particular weight of the two individuals that we are comparing; all that matters is that the two individuals compared weigh the same. Thus, if we are comparing two 100-pound individuals, one of whom has comsumed 1 beer and the other who has consumed 3 beers, then we estimate that the person who drank 3 beers would have a BAC 0.040 larger than the person who drank 1 beer. Under the current model, this would also be true if we compared two 250-point individuals, one of whom had comsumed 1 beer and the other who had consumed 3 beers. All that matters, so far, is that the individuals to be compared weigh the same. That said, knowing what we do about human physiology, we might that this value should differ depending on whether we are comparing two 100-pound individuals or two 250-pound individuals. This idea — that the association between one predictor and the response depends on the value of another predictor — is the idea of a statistical interaction. We will encounter interactions soon. Here’s another example from the world of food science. As cheese ages, various chemical processes take place that determine the taste of the final product. These data are from the Moore and McCabe (1989). The response variable is the taste scores averaged from several tasters. There are three predictors that describe the chemical content of the cheese. They are: acetic: the natural log of acetic acid concentration h2s: the natural log of hydrogen sulfide concentration lactic: the concentration of lactic acid Here is a “pairs plot” of the data. In this plot, each panel is a scatterplot showing the relationship between two of the four variables in the model. Pairs plots are useful ways to gain a quick grasp of the structure in the data and how the constituent variables are related to one another. cheese &lt;- read.table(&quot;data/cheese.txt&quot;, head = T, stringsAsFactors = T) pairs(cheese) Let’s entertain a model that uses all three predictors. cheese_regression &lt;- lm(taste ~ Acetic + H2S + Lactic, data = cheese) summary(cheese_regression) ## ## Call: ## lm(formula = taste ~ Acetic + H2S + Lactic, data = cheese) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.390 -6.612 -1.009 4.908 25.449 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -28.8768 19.7354 -1.463 0.15540 ## Acetic 0.3277 4.4598 0.073 0.94198 ## H2S 3.9118 1.2484 3.133 0.00425 ** ## Lactic 19.6705 8.6291 2.280 0.03108 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.13 on 26 degrees of freedom ## Multiple R-squared: 0.6518, Adjusted R-squared: 0.6116 ## F-statistic: 16.22 on 3 and 26 DF, p-value: 3.81e-06 Compare this MLR model with each of the three possible SLR models: slr1 &lt;- lm(taste ~ Acetic, data = cheese) summary(slr1) ## ## Call: ## lm(formula = taste ~ Acetic, data = cheese) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.642 -7.443 2.082 6.597 26.581 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -61.499 24.846 -2.475 0.01964 * ## Acetic 15.648 4.496 3.481 0.00166 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.82 on 28 degrees of freedom ## Multiple R-squared: 0.302, Adjusted R-squared: 0.2771 ## F-statistic: 12.11 on 1 and 28 DF, p-value: 0.001658 slr2 &lt;- lm(taste ~ H2S, data = cheese) summary(slr2) ## ## Call: ## lm(formula = taste ~ H2S, data = cheese) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.426 -7.611 -3.491 6.420 25.687 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.7868 5.9579 -1.643 0.112 ## H2S 5.7761 0.9458 6.107 1.37e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.83 on 28 degrees of freedom ## Multiple R-squared: 0.5712, Adjusted R-squared: 0.5558 ## F-statistic: 37.29 on 1 and 28 DF, p-value: 1.374e-06 slr3 &lt;- lm(taste ~ Lactic, data = cheese) summary(slr3) ## ## Call: ## lm(formula = taste ~ Lactic, data = cheese) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.9439 -8.6839 -0.1095 8.9998 27.4245 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -29.859 10.582 -2.822 0.00869 ** ## Lactic 37.720 7.186 5.249 1.41e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.75 on 28 degrees of freedom ## Multiple R-squared: 0.4959, Adjusted R-squared: 0.4779 ## F-statistic: 27.55 on 1 and 28 DF, p-value: 1.405e-05 What do you make of the fact that an SLR analysis suggests that there is a (statistically significant) positive relationship between acetic acid concentration and taste, yet the partial regression coefficient associated with acetic acid concentration is not statistically significant in the MLR model? The fact that the interpretation of regression parameters depends on the context of the model in which they are found cannot be overemphasized. I speculate that much of the confusion surrounding this point flows from the arguably deficient notation that we use to write regression models. Consider the beer data. When we compare the simple regression model \\(y = \\beta_0 + \\beta_1 x_1 + \\varepsilon\\) and the multiple regression model \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon\\), it seems natural to conclude that the parameter \\(\\beta_1\\) means the same thing in both models. But the parameters differ, as we have seen. In a different world, we might imagine notation that makes this context-dependence explicit, by writing something like \\(\\beta_1(x_1)\\) for the regression coefficient associated with \\(x_1\\) in a model that includes only \\(x_1\\), and \\(\\beta_1(x_1, x_2)\\) for the regression coefficient associated with \\(x_1\\) in a model that includes both \\(x_1\\) and \\(x_2\\). But such notation would quickly become unwieldy. So we are stuck with the notation that we have, and must avoid the confusion that it can create. 2.1.3 Visualizing a multiple regression model How do you visualize a multiple regression model? With two predictors (as in our current BAC model), the regression fit is a (rigid) plane in three-dimensional space. So, we have a fighting chance of making a picture of the model fit if we have good graphics software handy. However, once we enocunter models with more than two predictors, the regression fits become increasingly high-dimensional objects that we humans won’t be able to visualize in full. So the best we can do in general is to think about the bivariate relationship implied between each predictor and the response. We’ll continue to use the BAC model as an example. For starters, we might consider plotting the implied relationship between the predicted response and a predictor when the other predictors are set at their average value. In the BAC model, that means plotting the relationship between the predicted BAC and beers consumed for an individual of average weight, where by average weight we mean equal to the average weight of the individuals in the data set. We can couple this with a plot of predicted BAC vs weight for an individual who has consumed an average number of beers, where again we determine this average based on the average value that appears in the data set. Here are those plots for the BAC model: par(mfrow = c(1, 2)) b.hat &lt;- coefficients(fm2) # extract LSEs of regression coefficients avg.beers &lt;- mean(beer$beers) avg.weight &lt;- mean(beer$weight) plot(BAC ~ beers, type = &quot;n&quot;, xlab = &quot;beers consumed&quot;, data = beer) # set up axes abline(a = b.hat[1] + b.hat[3] * avg.weight, b = b.hat[2]) # predicted BAC for average weight individual legend(&quot;topleft&quot;, leg = paste(&quot;weight = &quot;, round(avg.weight, 0), &quot;lbs&quot;), bty = &quot;n&quot;) plot(BAC ~ weight, type = &quot;n&quot;, data = beer) # set up axes abline(a = b.hat[1] + b.hat[2] * avg.beers, b = b.hat[3]) # predicted BAC for average weight individual legend(&quot;topleft&quot;, leg = paste(&quot;beers consumed = &quot;, round(avg.beers, 1)), bty = &quot;n&quot;) It’s not clear whether we should overlay the data on these plots. Data are helpful, but showing the data points would suggest that the fitted line is a simple regression fit. We want to be clear that it isn’t. 2.1.4 Statistical inference for partial regression coefficients Statistical inference for partial regression coefficients proceeds in the same way as statistical inference for SLR slopes. Standard errors for both partial regression coefficients are provided in the R output: \\(s_{\\hat{\\beta}_1 }\\)= 0.0013, \\(s_{\\hat{\\beta}_{2} }\\)= 0.000057. Under the standard regression assumptions, the quantity \\(t=(\\hat{\\beta}_i -\\beta _i )/s_{\\hat{\\beta}_i }\\) has a \\(t\\)-distribution. The number of degrees of freedom is the number of df associated with the SSE. This fact can be used to construct confidence intervals and hypothesis tests. Most software will do the needed math for us. For example, to find 99% CIs for the partial regression coefficients in the BAC model, we can use confint(fm2, level = 0.99) ## 0.5 % 99.5 % ## (Intercept) 0.0084354279 0.0712912780 ## beers 0.0161715108 0.0237799132 ## weight -0.0005335564 -0.0001920855 Thus a 99% CI for the partial regression coefficient associated with weight is given by the interval from -0.00053 to -0.00019. Most statistical software will automatically provide tests of the null \\(H_0\\): \\(\\beta_i =0\\) vs. the alternative \\(H_a\\): \\(\\beta_i \\ne 0\\). For example, the R output above tells us that the \\(p\\)-value associated with this test for the partial regression coefficient associated with weight is \\(p = 0.000023\\). If we were reporting this analysis in scientific writing, we might say that when comparing people who have consumed the same number of beers, every 1-lb increase in weight is associated with an average BAC decrease of 3.6% \\(\\times\\) 10\\(^{-4}\\) (s.e. 5.7% \\(\\times\\) 10\\(^{-5}\\)). This association is statistically significant (\\(t_{13} =-6.40\\), \\(p &lt; .001\\)). 2.1.5 Prediction As in SLR, we distinguish between predictions of the average response of the population at a new value of the predictors vs. the value of a single future observation. The point estimates of the two predictions are identical, but the value of a single future observation is more uncertain. Therefore, we use a prediction interval for a single observation and a confidence interval for a population average. The width of a PI or CI is affected by the same factors as in SLR. In MLR, the width of the PI or CI depends on the distance between the new observation and the “center of mass” of the predictors in the data set. For example, if we now use the BAC model to predict the BAC of a 170-lb individual who consumes 4 beers: new.data &lt;- data.frame(weight = 170, beers = 4) predict(fm2, newdata = new.data, interval = &quot;prediction&quot;) ## fit lwr upr ## 1 0.05808664 0.03480226 0.08137103 predict(fm2, newdata = new.data, interval = &quot;confidence&quot;) ## fit lwr upr ## 1 0.05808664 0.05205732 0.06411596 2.2 \\(F\\)-tests for several regression coefficients Consider a general multiple regression model with \\(k\\) predictors: \\[ y=\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +...+\\beta_k x_k +\\varepsilon \\] So far, we’ve seen how to test whether any one individual partial regression coefficient is equal to zero, i.e., \\(H_0 :\\beta_j =0\\) vs. \\(H_a :\\beta_j \\ne 0\\). We will now discuss how to generalize this idea to test if multiple partial regression coefficients are simultaneously equal to zero. The statistical method that we will use is an \\(F\\)-test. Unfortunately, we don’t yet have a great context for motivating \\(F\\)-tests. We will see more compelling motivations for \\(F\\)-tests soon when we consider categorical predictors. For now, we’ll consider our multiple regression model for the BAC data with the two predictors “beers consumed” and “weight”. Recall that the equation for this model is given in eq. (2.1). Let’s suppose that we were interested in testing the null hypothesis that neither of the predictors have a linear association with the response, versus the alternative hypothesis that at least one predictor (and perhaps both) has a linear association with the response. In symbols, we would write the null hypothesis as \\(H_0 :\\beta_1 =\\beta_2 =0\\) and the alternative as \\(H_a :\\beta_1 \\ne 0{\\rm \\; or\\; }\\beta_2 \\ne 0\\). \\(F\\)-tests are essentially comparisons between models. The model that provides the context for the null hypothesis is the “full” model, in the sense that it will prove to be the more flexible of our two models to be compared. In the context of our present example, the full model is given by (2.1). The full model is then compared to a “reduced” model, which is the full model constrained by the null hypothesis. In the present example, when we constrain the full model by the null hypothesis \\(\\beta_1 =\\beta_2 =0\\), we are left with a reduced model that includes only the intercept: \\[\\begin{equation} y = \\beta_0 + \\varepsilon. \\end{equation}\\] Formally, it is important to note that the reduced model is a special case of the full model. The statistical jargon for this observation is that the reduced model is “nested” in the full model, or (to say it in the active voice) the full model “nests” the reduced model. Because the full model nests the reduced model, we know that the full model is guaranteed to explain at least as much of the variation in the response as the reduced model. The operative question is whether the improvement obtained by the full model is statistically significant, that is, whether it is more of an improvement than we would expect by random chance. An \\(F\\)-test proceeds by fitting both the full and reduced model, and for each model recording the SSE and the df associated with the SSE. An \\(F\\)-statistic is then calculated as \\[ F=\\frac{\\left[SSE_{reduced} -SSE_{full} \\right]/\\left[df_{reduced} -df_{full} \\right]}{{SSE_{full} / df_{full} }} \\] where by \\(df_{full}\\) and \\(df_{reduced}\\) we mean the df associated with the SSE of the full and reduced models, respectively. Roughly, the numerator of the \\(F\\)-statistic quantifies the improvement in fit that the full model provides relative to the reduced model. The denominator quantifies the variation in the response left unexplained by the full model. Both numerator and denominator are standardized by their associated df to create an apples-to-apples comparison. The larger the numerator is relative to the denominator, the less likely it is that the improvement in fit was strictly due to random chance. For the BAC data, we could compute the \\(F\\)-statistic “manually” by first fitting both models and extracting the SSE: full.model &lt;- lm(BAC ~ beers + weight, data = beer) (sse.full &lt;- sum(resid(full.model)^2)) ## [1] 0.001408883 reduced.model &lt;- lm(BAC ~ 1, data = beer) (sse.full &lt;- sum(resid(reduced.model)^2)) ## [1] 0.029225 Then our \\(F\\)-statistic evaluates to: \\[ F=\\frac{\\left[0.0292 - 0.0014 \\right]/\\left[15 - 13 \\right]}{{0.0014 / 13 }} = 128.3 \\] If the null hypothesis is true, then the \\(F\\)-statistic will be drawn from an \\(F\\) distribution with numerator df equal to \\(df_{reduced} -df_{full}\\), and denominator df equal to \\(df_{full}\\). Evidence against the null and in favor of the alternative comes from large values of the \\(F\\)-statistic. The \\(p\\)-value associated with the test is the probability of observing an \\(F\\)-statistic at least as large as the one observed if the null hypothesis is true. In R, this \\(p\\)-values can be found with the command pf. pf(128.3, df1 = 2, df2 = 13, lower = FALSE) ## [1] 2.760276e-09 Thus, our \\(p\\)-value is infinitesimal. In light of these data, it is almost completely implausible that there is no linear association between BAC and both the number of beers consumed and the individual’s weight. We’ve taken the above calculations slowly so that we can understand their logic. In R, we can use the anova command to execute the \\(F\\)-test in one fell swoop. anova(reduced.model, full.model) ## Analysis of Variance Table ## ## Model 1: BAC ~ 1 ## Model 2: BAC ~ beers + weight ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 15 0.0292250 ## 2 13 0.0014089 2 0.027816 128.33 2.756e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 2.2.1 Model utility tests The test that we have just conducted for the BAC data turns out to be a type of \\(F\\)-test called the model utility test. In general, with the general regression model \\[ y=\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +...+\\beta_k x_k +\\varepsilon \\] the model utility test is a test of the null hypothesis that all the regression coefficients equal zero, that is, \\(H_0 :\\beta_1 =\\beta_2 =...=\\beta_k =0\\) vs. the alternative that at least one of the partial regression coefficients is not equal to zero. In other words, it is a test of whether the regression model provides a significant improvement in fit compared a simpler model that assumes the \\(y\\)’s are a simple random sample from a Gaussian distirbution. The model utility test is easy for computer programmers to automate, so it is usually included as part of the standard regression output. In the R summary of a regression model, we can find it at the very end of the output. full.model &lt;- lm(BAC ~ beers + weight, data = beer) summary(full.model) ## ## Call: ## lm(formula = BAC ~ beers + weight, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.0162968 -0.0067796 0.0003985 0.0085287 0.0155621 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.986e-02 1.043e-02 3.821 0.00212 ** ## beers 1.998e-02 1.263e-03 15.817 7.16e-10 *** ## weight -3.628e-04 5.668e-05 -6.401 2.34e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.01041 on 13 degrees of freedom ## Multiple R-squared: 0.9518, Adjusted R-squared: 0.9444 ## F-statistic: 128.3 on 2 and 13 DF, p-value: 2.756e-09 Although the model utility test has a grandiose name, it is rarely interesting. Rejecting the null in the model utility test is usually not an impressive conclusion (Quinn and Keough (2002)). You may have also noticed that in SLR the model utility test always provided a \\(p\\)-value that was exactly equal to the \\(p\\)-value generated for the test of \\(H_0\\): \\(\\beta_1 =0\\) vs. \\(H_a\\): \\(\\beta_1 \\ne 0\\). Can you figure out why this is so? 2.3 Categorical predictors So far, we have dealt exclusively with quantitative predictors. Although we haven’t given it much thought, a key feature of quantitative predictors is that their values can be ordered, and that the distance between ordered values is meaningful. For example, in the BAC data, a predictor value of \\(x=3\\) beers consumed is greater than \\(x=2\\) beers consumed. Moreover, the difference between \\(x=2\\) and \\(x=3\\) beers consumed is exactly one-half of the distance between \\(x=2\\) and \\(x=4\\) beers consumed. Another way to think about quantitative predictors is that we could sensibly place all of their values on a number line. Categorical variables are variables whose values cannot be sensibly placed on a number line. Examples include ethnicity or brand of manufacture. We use indicator variables as devices to include categorical predictors in regression models.13 Example. D. K. Sackett investigated mercury accumulation in the tissues of large-mouth bass (a species of fish) in several lakes in the Raleigh, NC area (Sackett et al. (2013)). We will examine data from three lakes: Adger, Bennett’s Millpond, and Waterville. From each lake, several fish were sampled, and the mercury (Hg) content of their tissues was measured. Because fish are known to accumulate mercury in their tissues as they age, the age of each fish (in years) was also determined. The plot below shows the mercury content (in mg / kg) for each fish plotted vs. age, with different plotting symbols used for the three lakes. To stabilize the variance, we will use the log of mercury content as the response variable. There are \\(n=23\\) data points in this data set. fish &lt;- read.table(&quot;data/fish-mercury.txt&quot;, head = T, stringsAsFactors = T) with(fish, plot(log(hg) ~ age, xlab = &quot;age (years)&quot;, ylab = &quot;log(tissue Hg)&quot;, type = &quot;n&quot;)) with(subset(fish, site == &quot;Adger&quot;), points(log(hg) ~ age, pch = &quot;A&quot;)) with(subset(fish, site == &quot;Bennett&quot;), points(log(hg) ~ age, pch = &quot;B&quot;)) with(subset(fish, site == &quot;Waterville&quot;), points(log(hg) ~ age, pch = &quot;W&quot;)) With these data, we would like to ask: when comparing fish of the same age, is there evidence that tissue mercury content in fish differs among the three lakes? To do so, we need to develop a set of \\(indicator\\) variables to capture the differences among the lakes. As is often the case, one has options here. If you wish, you can construct the indicator variables manually, perhaps using a spreadsheet program. Alternatively, most computer programs, including R, will create indicator variables automatically. We’ll sketch out the ideas behind indicator variables first and then see how R creates them. To create a set of indicator variables, we first need to choose one level of the variable as the reference level or baseline. While the scientific context of a problem will sometimes make it more natural to designate one level as a reference, the choice is often arbitrary. In all cases, the choice of a reference level will not affect the ensuing analysis. For every level other than the reference, we create a separate indicator variable that is equal to 1 for that level and is equal to 0 for all levels. Thus, to include a categorical variable with \\(c\\) different levels, we need \\(c−1\\) indicator variables. To see how R constructs indicator variables, we can use the contrast command14 contrasts(fish$site) ## Bennett Waterville ## Adger 0 0 ## Bennett 1 0 ## Waterville 0 1 In the R output above, each column represents an indicator variable, the rows give the levels of the categorical variable, and the numbers give the coding of each indicator variable. We see that R has created two indicator variables (as we’d expect), and labeled them “Bennett” and “Waterville”. The names of the indicator variables provide strong hints about their coding, but to be completely sure we can inspect the numerical coding given below. The indicator labeled “Bennett” takes the value of 1 when site equals Bennett and takes the value of zero otherwise. In other words, it is the indicator for Bennett. The indicator labeled “Waterville” takes the value of 1 when site equals Waterville and takes the value of zero for the other two sites. In other words, it is the indicator for Waterville. Adger, therefore, is the reference site.15 Now let’s fit a regression model with both age and site as predictors. fish_model &lt;- lm(log(hg) ~ age + site, data = fish) summary(fish_model) ## ## Call: ## lm(formula = log(hg) ~ age + site, data = fish) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.47117 -0.08896 0.03796 0.13910 0.31327 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.80618 0.15398 -11.730 3.80e-10 *** ## age 0.14309 0.01967 7.276 6.66e-07 *** ## siteBennett 0.07107 0.12910 0.550 0.5884 ## siteWaterville -0.26105 0.10817 -2.413 0.0261 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2084 on 19 degrees of freedom ## Multiple R-squared: 0.7971, Adjusted R-squared: 0.765 ## F-statistic: 24.88 on 3 and 19 DF, p-value: 8.58e-07 The R output gives us information about the partial regression coefficients associated with age and with each of the indicator variables for site. In an equation, we could write this model as \\[ y=\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +\\beta_3 x_3 +\\varepsilon \\] where \\(x_1\\) gives the age of the fish and \\(x_2\\) and \\(x_3\\) are the indicator variables for Bennett and Waterville, respectively. While the R output gives us tests of the significance of the individual regression coefficients, we want to determine whether there are significant differences among the three sites when comparing fish of the same age. To answer this question, we need to test the hypothesis \\(H_0: \\beta_2 = \\beta_3 = 0\\). We can do this with an \\(F\\)-test. fish_model_full &lt;- lm(log(hg) ~ age + site, data = fish) fish_model_reduced &lt;- lm(log(hg) ~ age, data = fish) anova(fish_model_reduced, fish_model_full) ## Analysis of Variance Table ## ## Model 1: log(hg) ~ age ## Model 2: log(hg) ~ age + site ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 21 1.17251 ## 2 19 0.82529 2 0.34722 3.9969 0.03558 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Thus, we see that there are significant differences among the sites, when comparing fish of the same age (\\(F_{2, 19} = 4.00\\), \\(p = 0.036\\)). For what it’s worth, we also could have obtained this test by running the anova command on the full model. anova(fish_model_full) ## Analysis of Variance Table ## ## Response: log(hg) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## age 1 2.89448 2.89448 66.6374 1.24e-07 *** ## site 2 0.34722 0.17361 3.9969 0.03558 * ## Residuals 19 0.82529 0.04344 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The anova command also gives an \\(F\\)-test for the null hypothesis of no association between age and the response when comparing fish from the same lake, that is, that \\(\\beta_1 = 0\\). This \\(F\\)-test gives the same result as the \\(t\\)-test provided in the earlier summary, because the two tests are identical. Arguably, for single regression coefficients, the output from summary is more useful, because it gives us the estimate of \\(\\beta_1\\), while the anova output only gives us an \\(F\\)-test. Finally, now that we have established that there are significant differences among the sites when comparing fish of the same age, we can go back and make sense of the estimates of \\(\\beta_2\\) and \\(\\beta_3\\) in the full model. The partial regression coefficients associated with an indicator variable quantify the difference between the level that the variable is an indicator for and the reference. In other words, for the fish data, the value \\(\\hat{\\beta}_2 = 0.071\\) tells us that if we compare two fish of the same age, then a fish from Bennett will have a response that is on average 0.071 larger than a fish from Adger (the reference site). The value \\(\\hat{\\beta}_3 = -0.261\\) tells us that, on average, a fish from Waterville will have a response that is 0.261 less than a similarly aged-fish from Adger. Note that these values also give us enough information to compute the average difference in \\(y\\) between two similarly aged fish from Bennett and Waterville, even though that value isn’t directly included in the output. Indicator variables may seem like a bit of an awkward device. Why can’t we just fit a model with a separate intercept for each lake? In fact, we can. In R, the program lm includes the intercept \\(\\beta_0\\) in any model by default, because most regression models include it. However, if we instruct lm to omit the baseline intercept \\(\\beta_0\\), then the program will parameterize the model by the lake-specific intercepts. We instruct lm to omit the intercept by including a -1 on the right-hand side of the model formula as follows: fish_model_alt &lt;- lm(log(hg) ~ age + site - 1, data = fish) summary(fish_model_alt) ## ## Call: ## lm(formula = log(hg) ~ age + site - 1, data = fish) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.47117 -0.08896 0.03796 0.13910 0.31327 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## age 0.14309 0.01967 7.276 6.66e-07 *** ## siteAdger -1.80618 0.15398 -11.730 3.80e-10 *** ## siteBennett -1.73511 0.09440 -18.381 1.47e-13 *** ## siteWaterville -2.06723 0.16351 -12.643 1.07e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2084 on 19 degrees of freedom ## Multiple R-squared: 0.9721, Adjusted R-squared: 0.9662 ## F-statistic: 165.2 on 4 and 19 DF, p-value: 1.781e-14 The model is now parameterized by the lake-specific intercepts and the common slope. While this parameterization is more straightforward, note that the \\(R^2\\) value and the model utility test are now wrong. The software’s routine for calculating \\(R^2\\) and the model utility test assumes that the intercept \\(\\beta_0\\) will be present in the model. Of course, it’s easy to use our original parameterization to get the correct \\(R^2\\) value (and model utility test) and use the alternative parameterization to get the lake-specific slopes. We just have to be careful with regard to the rest of the output when the baseline intercept is omitted. None of this behavior is unique to R. In SAS, PROC GLM has a similar option for reparameterizing the model without the common intercept, but it will cause the computation of \\(R^2\\) and the model utility test to break there as well. Models that combine a single numerical predictor and a single categorical predictor can be visualized by plotting the trend lines for each level of the categorical predictor. In making this plot, it is easier to use the alternative parameterization of the model described in the gray text above. with(fish, plot(log(hg) ~ age, xlab = &quot;age (years)&quot;, ylab = &quot;log(tissue Hg)&quot;, type = &quot;n&quot;)) with(subset(fish, site == &quot;Adger&quot;), points(log(hg) ~ age, pch = &quot;A&quot;, col = &quot;forestgreen&quot;)) with(subset(fish, site == &quot;Bennett&quot;), points(log(hg) ~ age, pch = &quot;B&quot;, col = &quot;red&quot;)) with(subset(fish, site == &quot;Waterville&quot;), points(log(hg) ~ age, pch = &quot;W&quot;, col = &quot;blue&quot;)) abline(a = fish_model_alt$coefficients[2], b = fish_model_alt$coefficients[1], col = &quot;forestgreen&quot;) abline(a = fish_model_alt$coefficients[3], b = fish_model_alt$coefficients[1], col = &quot;red&quot;) abline(a = fish_model_alt$coefficients[4], b = fish_model_alt$coefficients[1], col = &quot;blue&quot;) legend(&quot;topleft&quot;, legend = c(&quot;Lake A&quot;, &quot;Lake B&quot;, &quot;Lake W&quot;), pch = 16, col = c(&quot;forestgreen&quot;, &quot;red&quot;, &quot;blue&quot;)) 2.4 Interactions between predictors Consider (again) the BAC data, with our working model \\[ y =\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +\\varepsilon \\] where \\(y\\) is the response (BAC), \\(x_1\\) is beers consumed, \\(x_2\\) is weight, and \\(\\varepsilon\\) is iid normal error. This is an additive model, in the sense that the joint association between beers consumed and weight (as a pair) and BAC can be found by adding together the individual associations between each of the two predictors and the response. However, we might instead want to allow for the possibility that the association between one predictor and the response itself depends on the value of a second predictor. This state of affairs is called a statistical interaction. More specifically, we call it a statistical interaction between the two predictors with respect to their association with the response. Note that whether or not two predictors interact has nothing to do with whether the predictors themselves are associated.16 An interaction between the two predictors allows the effect of beers consumed to depend on weight, and vice versa. A model with an interaction can be written as: \\[ y =\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +\\beta_3 x_1 x_2 +\\varepsilon \\tag{2.2} \\] There are two equally good ways to code this model in R: fm1 &lt;- lm(BAC ~ beers + weight + beers:weight, data = beer) or fm2 &lt;- lm(BAC ~ beers * weight, data = beer) In the first notation, the colon (:) tells R to include the interaction between the predictors that appear on either side of the colon. In the second notation, the asterisk (*) is shorthand for both the individual predictors and their interaction. summary(fm2) ## ## Call: ## lm(formula = BAC ~ beers * weight, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.0169998 -0.0070909 0.0008463 0.0084267 0.0164373 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.010e-02 3.495e-02 0.861 0.40601 ## beers 2.162e-02 5.760e-03 3.754 0.00275 ** ## weight -2.993e-04 2.241e-04 -1.336 0.20646 ## beers:weight -1.066e-05 3.627e-05 -0.294 0.77393 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0108 on 12 degrees of freedom ## Multiple R-squared: 0.9521, Adjusted R-squared: 0.9402 ## F-statistic: 79.57 on 3 and 12 DF, p-value: 3.453e-08 Interpreting the interaction. The partial regression coefficient associated with an interaction between two predictors (call them “A” and “B”) quantifies the effect that predictor A has on the linear association between predictor B and the response. Or, equivalently, the same partial regression coefficient quantifies the effect that predictor B has on the linear association between predictor A and the response. (It may not be obvious right away that the same regression coefficient permits both interpretations, but you might be able to convince yourself that this is true by doing some algebra with the regression model.) Thus, if we reject the null hypothesis that a partial regression coefficient associated with an interaction equals zero, then we conclude that the effects of the two predictors depend on one another. With the BAC data, the interaction term is not statistically significant. Thus, these data provide no evidence that the association between beers consumed and BAC depends on weight, or vice versa. For the sake of argument, let’s examine the estimate of the interaction between beers consumed and weight, despite the fact that it is not statistically significant. How can we interpret the value \\(\\hat{\\beta}_3 = -1.07 \\times 10^{-5}\\)? This value tells us how the association between beers consumed and BAC changes as weight changes. In other words, the model predicts that a heavier person’s BAC will increase less for each additional beer consumed, compared to a lighter person. How much less? If the heavier person weighs 1 lb more than the lighter person, then one additional beer will increase the heavier person’s BAC by \\(1.07 \\times 10^{-5}\\) less than it increases the lighter person’s BAC. This agrees with our expectations, but these data do not provide enough evidence to declare that this interaction is statistically significant. Alternatively, \\(\\beta_3\\) also tells us how the association between weight and BAC changes as the number of beers consumed changes. That is, as the number of beers consumed increases, then the association between weight and BAC becomes more steeply negative. Again, this coincides with our expectation, despite the lack of statistical significance. Interpreting partial regression coefficients in the presence of an interaction. There is a major and unexpected complication that ensues from including an interaction term in a regression model. This complication concerns how we interpret the partial regression coefficients associated with the individual predictors engaged in the interaction, that is, the regression coefficients \\(\\beta_1\\) and \\(\\beta_2\\) in eq. (2.2). It turns out that, in (2.2), the partial regression coefficient associated with an individual predictor quantifies the relationship between that predictor and the response when the other predictor involved in the interaction equals 0. Sometimes this interpretation is scientifically meaningful, but usually it isn’t. For example, in the BAC model that includes the interaction above, the parameter \\(\\beta_1\\) now quantifies the association between beers consumed and BAC for people who weigh 0 lbs. Obviously, this is a meaningless quantity. Alternatively, the parameter \\(\\beta_2\\) quantifies the association between weight and BAC for people who have consumed 0 beers. This is a bit less ridiculous — in fact, it makes a lot of sense — but still requires extrapolating the model fit outside the range of the observed data, because there are no data points here for people who have had 0 beers.17 To summarize, the subtlety here is that if we compare the additive model \\[ y =\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +\\varepsilon \\] with the model that includes an interaction \\[ y =\\beta_0 +\\beta_1 x_1 +\\beta_2 x_2 +\\beta_3 x_1 x_2 +\\varepsilon, \\] the parameters \\(\\beta_1\\) and \\(\\beta_2\\) have completely different meanings in these two models. In other words, adding an interaction between two predictors changes the meaning of the regression coefficient associated with the individual predictors involved in the interaction. This is a subtlety that routinely confuses even top investigators. It’s a hard point to grasp, but essential for interpreting models that include interactions correctly. We can ease the interpretation of partial regression coefficients in a model that includes an interaction by centering the predictors. Recall that centering the predictors means creating new versions of each predictor by subtracting off their respective averages. For the BAC data, we could create centered versions of the two predictors by: \\[ \\begin{array}{l} {x_1^{ctr} =x_1 -\\bar{x}_1 } \\\\ {x_2^{ctr} =x_2 -\\bar{x}_{2} } \\end{array} \\] We then fit the model with the interaction, using the centered predictors instead: beer$beers.c &lt;- beer$beers - mean(beer$beers) beer$weight.c &lt;- beer$weight - mean(beer$weight) head(beer) ## BAC weight beers beers.c weight.c ## 1 0.100 132 5 0.1875 -39.5625 ## 2 0.030 128 2 -2.8125 -43.5625 ## 3 0.190 110 9 4.1875 -61.5625 ## 4 0.120 192 8 3.1875 20.4375 ## 5 0.040 172 3 -1.8125 0.4375 ## 6 0.095 250 7 2.1875 78.4375 fm3 &lt;- lm(BAC ~ beers.c * weight.c, data = beer) summary(fm3) ## ## Call: ## lm(formula = BAC ~ beers.c * weight.c, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.0169998 -0.0070909 0.0008463 0.0084267 0.0164373 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.402e-02 2.849e-03 25.984 6.45e-12 *** ## beers.c 1.980e-02 1.447e-03 13.685 1.10e-08 *** ## weight.c -3.506e-04 7.206e-05 -4.865 0.000388 *** ## beers.c:weight.c -1.066e-05 3.627e-05 -0.294 0.773927 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0108 on 12 degrees of freedom ## Multiple R-squared: 0.9521, Adjusted R-squared: 0.9402 ## F-statistic: 79.57 on 3 and 12 DF, p-value: 3.453e-08 Note that centering the predictors does not change the estimated interaction or its statistical significance. The main advantage of centering the predictors is that the partial regression coefficients associated with the centered versions of the predictors have a nice interpretation. Now, the partial regression coefficients associated with the main effects quantify the relationship between the predictor and the response when the other predictor involved in the interaction equals its average value. While we’re at it, we can also return to the fish data to ask if fish accumulate mercury in their tissues at different rates in the three lakes. fish_model_interaction &lt;- lm(log(hg) ~ age * site, data = fish) fish_model_additive &lt;- lm(log(hg) ~ age + site, data = fish) anova(fish_model_additive, fish_model_interaction) ## Analysis of Variance Table ## ## Model 1: log(hg) ~ age + site ## Model 2: log(hg) ~ age * site ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 19 0.82529 ## 2 17 0.75322 2 0.07207 0.8133 0.4599 The interaction between age and lake is not significant (\\(F_{2, 17} = 0.81\\), \\(p=0.46\\)). There is no evidence that the rate at which fish accumulate mercury in their tissues differs among the three lakes. Just for fun, we can plot the model with the interaction to see how the plot differs from the additive model that we considered earlier. As we did before, we’ll use the trick of fitting the model without the baseline intercept to make it easier to extract the slopes and intercepts of the lake-specific trend lines. fish_model_alt &lt;- lm(log(hg) ~ site + age:site - 1, data = fish) with(fish, plot(log(hg) ~ age, xlab = &quot;age (years)&quot;, ylab = &quot;log(tissue Hg)&quot;, type = &quot;n&quot;)) with(subset(fish, site == &quot;Adger&quot;), points(log(hg) ~ age, pch = &quot;A&quot;, col = &quot;forestgreen&quot;)) with(subset(fish, site == &quot;Bennett&quot;), points(log(hg) ~ age, pch = &quot;B&quot;, col = &quot;red&quot;)) with(subset(fish, site == &quot;Waterville&quot;), points(log(hg) ~ age, pch = &quot;W&quot;, col = &quot;blue&quot;)) abline(a = fish_model_alt$coefficients[1], b = fish_model_alt$coefficients[4], col = &quot;forestgreen&quot;) abline(a = fish_model_alt$coefficients[2], b = fish_model_alt$coefficients[5], col = &quot;red&quot;) abline(a = fish_model_alt$coefficients[3], b = fish_model_alt$coefficients[6], col = &quot;blue&quot;) legend(&quot;topleft&quot;, legend = c(&quot;Lake A&quot;, &quot;Lake B&quot;, &quot;Lake W&quot;), pch = 16, col = c(&quot;forestgreen&quot;, &quot;red&quot;, &quot;blue&quot;)) A picture may be worth a thousand words, but it isn’t worth a test! The plot of the model suggests that fish in lake W accumulate mercury more slowly than fish in the other two lakes, but the differences among the slopes are no more greater we would have expected from random variation. 2.5 (Multi-)Collinearity Collinearity (or what is sometimes also called multi-collinearity) refers to correlations among predictors, or among weighted sums of predictors. In a designed experiment, collinearity should not be an issue: The experimenter should be able to assign predictors in such a way that predictors are not strongly correlated with one another, or even better, are perfectly uncorrelated.18 With observational data, however, collinearity is often the rule more than the exception. This is especially true when the number of predictors becomes large relative to the number of data points. For example, the problem is especially acute in genomic studies, in which one may seek to find genetic correlates of phenotypic differences with a sample of a few dozen genomes, each of which contains genotypes at several thousand or more loci. In this section, we will explain what collinearity is, how it affects regression modeling, how it can be measured, and what (if anything) can be done about it. To illustrate, we’ll use a data set that details the tar content, nicotine content, weight, and carbon monoxide content of a couple dozen brands of cigarettes. I found these data in McIntyre (1994), who offers the following context: The Federal Trade Commission annually rates varieties of domestic cigarettes according to their tar, nicotine, and carbon monoxide content. The United States Surgeon General considers each of these substances hazardous to a smoker’s health. Past studies have shown that increases in the tar and nicotine content of a cigarette are accompanied by an increase in the carbon monoxide emitted from the cigarette smoke. The dataset presented here contains measurements of weight and tar, nicotine, and carbon monoxide (CO) content for 25 brands of cigarettes. The data were taken from Mendenhall and Sincich (2012). The original source of the data is the Federal Trade Commission.” cig &lt;- read.table(&quot;data/cigarettes.txt&quot;, head = T) head(cig) ## Brand tar nicotine weight co ## 1 Alpine 14.1 0.86 0.9853 13.6 ## 2 Benson&amp;Hedges 16.0 1.06 1.0938 16.6 ## 3 BullDurham 29.8 2.03 1.1650 23.5 ## 4 CamelLights 8.0 0.67 0.9280 10.2 ## 5 Carlton 4.1 0.40 0.9462 5.4 ## 6 Chesterfield 15.0 1.04 0.8885 15.0 (Note that the first variable in the data set is a character string that gives the brand name of each cigarette. In this case, we do not want to treat this as a categorical predictor, so we exclude the stringsAsFactors = T argument from the read.table command.) Here is a pairs plot of the data: pairs(cig[, 2:5]) We wish to use tar content, nicotine content, and weight to build a predictive model of carbon monoxide content.19 Let’s first observe that, when considered on their own, both tar content and nicotine content have strongly significant associations with carbon monoxide content, as the pairs plot suggests. summary(lm(co ~ tar, data = cig)) ## ## Call: ## lm(formula = co ~ tar, data = cig) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.1124 -0.7167 -0.3754 1.0091 2.5450 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.74328 0.67521 4.063 0.000481 *** ## tar 0.80098 0.05032 15.918 6.55e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.397 on 23 degrees of freedom ## Multiple R-squared: 0.9168, Adjusted R-squared: 0.9132 ## F-statistic: 253.4 on 1 and 23 DF, p-value: 6.552e-14 summary(lm(co ~ nicotine, data = cig)) ## ## Call: ## lm(formula = co ~ nicotine, data = cig) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.3273 -1.2228 0.2304 1.2700 3.9357 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.6647 0.9936 1.675 0.107 ## nicotine 12.3954 1.0542 11.759 3.31e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.828 on 23 degrees of freedom ## Multiple R-squared: 0.8574, Adjusted R-squared: 0.8512 ## F-statistic: 138.3 on 1 and 23 DF, p-value: 3.312e-11 Yet in a model that includes all three predictors, only tar content seems to be statistically significant: cig.model &lt;- lm(co ~ tar + nicotine + weight, data = cig) summary(cig.model) ## ## Call: ## lm(formula = co ~ tar + nicotine + weight, data = cig) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.89261 -0.78269 0.00428 0.92891 2.45082 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.2022 3.4618 0.925 0.365464 ## tar 0.9626 0.2422 3.974 0.000692 *** ## nicotine -2.6317 3.9006 -0.675 0.507234 ## weight -0.1305 3.8853 -0.034 0.973527 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.446 on 21 degrees of freedom ## Multiple R-squared: 0.9186, Adjusted R-squared: 0.907 ## F-statistic: 78.98 on 3 and 21 DF, p-value: 1.329e-11 The multiple regression model suggests that if we compare cigarettes with the same nicotine content (and weight), then there will be a (strongly) significant statistical association between tar content and carbon monoxide content. On the other hand, if we compare cigarettes with the same tar content (and weight), then there will not be a significant association between the nicotine content and the carbon monoxide content. This seems like a strong distinction. Do we trust it? The issue here is that tar and nicotine content are strongly correlated with one another, raising legitimate questions about whether we can separate the associations between one of the predictors and the response from the other. Indeed, in light of the strong correlation between tar content and nicotine content, does our comparative interpretation of the regression coefficients even make sense? If two cigarettes have the same nicotine content and weight, then by how much can their tar content differ? This is the issue of collinearity. Perfect collinearity occurs when two predictors are perfectly correlated with one another. Perfect collinearity is rare (unless the number of predictors exceeds the number of data points, in which case it is inevitable). However, if predictors are strongly (but nor perfectly) correlated, trouble still lurks. Indeed, collinearity is not just caused by strong correlations between pairs of predictors: It can also be caused by strong correlations between weighted sums of predictors. For this reason, when there are many predictors relative to the number of data points, collinearity is nearly inevitable. The usual guidance is that collinearity makes the estimated regression coefficients unreliable or unstable, in the sense that small changes in the data set can trigger large changes in the model fit (Bowerman and O’Connell (1990)). This sensitivity to small changes makes it difficult, if not impossible, to have confidence in our inferences about the estimated partial regression coefficients. Collinearity is not a problem for prediction, however. As Quinn and Keough (2002) (p. 127) say: As long as we are not extrapolating beyond the range of our predictor variables and we are making predictions from data with a similar pattern of collinearity as the data to which we fitted our model, collinearity doesn’t necessarily prevent us from estimating a regression model that fits the data well and has good predictive power (Rawlings, Pantula, and Dickey (1998)). It does, however, mean that we are not confident in our estimates of the model parameters. A different sample from the same population of observations, even using the same values of the predictor variables, might produce very different parameter estimates. In other words, we can still use our regression model for prediction, as long as we supply values of tar content and nicotine content that are consistent with the strong correlation between those two variables in the original data set. Collinearity is usually assessed by a variance inflation factor (VIF). The VIF is so named because it measures the amount by which the correlations among the predictors increase the standard error (and thus the variance) of the estimated regression coefficients. A separate VIF can be calculated for each predictor in the model. There is a relatively simple recipe for calculating VIFs that is somewhat edifying, although it’s easier to let the software compute the VIFs for you. Here’s the recipe if you are interested: To calculate the VIF for predictor \\(x_j\\), do the following: Regress \\(x_j\\) against all other predictors. That is, fit a new regression model in which \\(x_j\\) is the response. Note that the actual response \\(y\\) is not included in this model. Calculate \\(R^2\\). The VIF associated with predictor \\(x_j\\) is \\(1/\\left(1-R^2 \\right)\\). The interesting feature of the recipe is that it depends only on the values of the predictors — the response \\(y\\) plays no part. The recipe also has a certain logic, in the sense that if a predictor is strongly collinear with the other predictors in the model, then we should be able to predict that predictor well using the other predictors in the model. The reason why the VIF is calculated as \\(1/\\left(1-R^2 \\right)\\) in the last step is a bit of a mystery, and has to do with the fact that VIFs were originally developed to measure the increase in the variance of the regression coefficients caused by the collinearity. Here, we’ll use the vif function found in the car package to compute the VIFs for the cigarette model: car::vif(cig.model) ## tar nicotine weight ## 21.630706 21.899917 1.333859 Larger values of VIF indicate stronger collinearity. For the cigarette data, the VIFs tell us that both tar and nicotine are strongly collinear with the other predictors in the model, but weight is not strongly collinear with tar and nicotine. So that’s the good news: that collinearity is readily measured. The bad news is two-fold. First, VIFs are a continuous measure. The natural question is how large the VIF needs to be before one needs to worry about it. There’s no bright line to be found here. Most texts suggests that a VIF \\(\\geq\\) 10 indicates strong enough collinearity that additional measures should be taken. As always, don’t take the bright-line aspect of this rule too seriously; a VIF of 9.9 is not meaningfully different from a VIF of 10.1. The second half of the bad news is that there’s no easy fix for collinearity. In some sense, this is just a statement of common sense: If two predictors and a response vary together, then it is difficult (if not impossible) to tease apart the effect of one predictor on the response from the effect of the other predictor with a statistical model. With the cigarette data, for example, if we really wanted to characterize the effects of tar and nicotine content separately, then we’d need to find some cigarettes with high tar content and low nicotine content, or vice versa. The usual recommendations for coping with collinearity attempt to stabilize the estimated partial regression coefficients at the expense of accepting a (hopefully) small bias.20 Here are two: Omit predictors that are highly correlated with other predictors in the model. The rationale here is that highly correlated predictors may just be redundant measures of the same thing. As an example, in the cigarette data, tar content and nicotine content may both be driven by the same underlying features of the cigarette’s ingredients. If this is true, there is little to be gained by including both variables as predictors in a regression model. Use principal components analysis (PCA) to reduce the number of predictors, and use principal components as predictors. PCA is a multivariate statistical method that takes several variables and produces a smaller number of new variables (the “principal components”) that contain the majority of the information in the original variables. The advantage of using a principal component as a predictor is that different principal components are guaranteed to be independent of one another, by virtue of how they are calculated. The major disadvantage of using principal components is that the newly created predictors (the principal components) are amalgams of the original variables, and thus it is more difficult to assign a scientific interpretation to the partial regression coefficients associated with each. So, using PCA to find new predictors yields a more robust statistical model, albeit at the expense of reduced interpretability. 2.6 Variable selection: Choosing the best model So far, we’ve learned how to construct and fit regression models that can potentially include many different types of terms, including multiple predictors, transformations of the predictors, indicator variables for categorical predictors, interactions, and polynomial terms. Even a small set of possible predictors can produce a large array of possible models. How do we go about choosing the “best” model? First, we have to define what we mean by a “best” model. What are the qualities of a good model? Parsimony. We seek a model that adequately captures the “signal” in the data, and no more. There are both philosophical and statistical reasons for seeking a parsimonious model. The statistical motivation is that a model with too many unnecessary predictors is prone to detect spurious patterns that would not appear in repeated samples from the same population. We call this phenomenon “overfitting” or “fitting the noise”. In technical terms, fitting a model with too many unnecessary predictors increases the standard errors of the parameter estimates. Interpretability. We often want to use regression models to understand associations between predictors and the response and to shed light on the data-generating process. This argues for keeping models simple. There are occasions where the sole goal of regression modeling is prediction and in this case interpretability is less important. This is often the case in so-called “big data” applications, where prediction is the primary goal and understanding is only secondary. Statistical inference. As scientists, we are not interested merely in describing patterns in our data set. Instead, we want to use the data to draw inferences about the population from which the sample was drawn. Therefore, we want a model that meets the assumptions of regression so that we can use regression theory to draw statistical inferences. In statistical jargon, the process of choosing which predictors to include in a regression model and which to leave out is called variable selection. More generally, beyond a regression context, the problem of identifying the best statistical model is called model selection. We will look at several automated routines for choosing the best model. Helpful as these routines are, they are no substitute for intelligent analysis. Feel free to use an automated variable selection route to get started, but don’t throw your brain out the window in the process. Also, remember that there is a hierarchy among model terms in regression. Most automated variable selection routines do not incorporate this hierarchy, so we must impose it ourselves. In most cases, the following rules should be followed: Models that include an interaction between predictors should also include the predictors individually. Models that include polynomial powers of a predictor should include all lower-order terms of that predictor. Automated variable selection routines can be grouped into two types: ranking methods and sequential methods. 2.6.1 Ranking methods Ranking methods work best when it is computationally feasible to fit every possible candidate model. In these situations, we calculate a metric that quantifies the model’s adequacy, and select the model that scores the best with respect to that metric. There are several possible metrics to choose from, and they don’t always point to the same best model. We will look at three different metric that enjoy wide use. Throughout this section, we will refer to the number of predictors in a model, and denote this quantity by \\(k\\). To remove ambiguity, what we mean in this case is the number of partial regression coefficients to be estimated. So, for example, we would say that the model \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1^2 + \\beta_4 x_1 x_2 + \\varepsilon\\) has \\(k = 4\\) predictors. Before beginning, we should note that \\(R^2\\) is not a good choice for a ranking metric. This is because adding a predictor will never decrease \\(R^2\\). Therefore, \\(R^2\\) can only be used to compare models that have the same number of predictors. Adjusted \\(R^2\\). Adjusted \\(R^2\\) is a penalized version of \\(R^2\\) that imposes a penalty for each additional parameter added to the model. The (rather opaque) formula for adjusted \\(R^2\\) is \\[ {\\rm Adj-}R^2 =1-\\frac{n-1}{n-\\left(k+1\\right)} \\left(1-R^2 \\right) \\] The model with the largest Adj-\\(R^2\\) is considered best. PRESS statistic (PRESS = PRedicted Sum of Squares) Step 1. Remove the first data point. Step 2: Fit the model to the remaining data. Step 3. Use the model from step 2 to predict the removed data point, \\(\\hat{y}_1^*\\). Step 4. Add the first data point back to the data set. Step 5. Repeat steps 1-4 for each data point in turn. The PRESS statistic is \\(\\sum_{i=1}^n\\left(y_i -\\hat{y}_i^* \\right)^2\\). The model with the smallest PRESS statistic is considered best. The PRESS statistic is interesting because it is an example of a more general idea called cross-validation. The idea of cross-validation is to remove one or more points from a data set, fit the model to the remaining data, and use the fitted model to predict the data point(s) that were removed. A good model should accurately predict the removed data point(s). There are many variations on the idea of cross-validation, including schemes that remove a subset of the data instead of just a single data point. The removed data are often called the “testing data”, and the data to which the model are fit (i.e., the data that are not removed) are called the “training data”. AIC (Akaike’s Information Criterion) AIC is also a penalized goodness-of-fit measure, like adjusted \\(R^2\\). AIC enjoys a bit more theoretical support than adjusted \\(R^2\\) and is more versatile, although its derivation is a bit more opaque. (As the name suggests, AIC has its roots in information theory.) The general form of AIC involves math that is beyond the scope of ST 512, but we can write down the specific formula for regression models, which is \\[ AIC=n\\ln \\left[\\frac{SSE}{n} \\right]+2\\left(k+1\\right) \\] The smallest value of AIC is best. (Smaller here means algebraically smaller — that is, further to the left on the number line — not closer to zero.) Despite its theoretical support, AIC tends to favor models with too many predictors. 2.6.2 Sequential methods Sequential methods work best for problems where the set of candidate models is so vast that fitting all the candidate models is not feasible. Because computers are faster today than they were years ago, it is now feasible to fit a large number of candidate models quickly, and thus sequential methods are less necessary today than they were years ago. Nevertheless, the ideas are straightforward. There are three different types of sequential methods, based on the direction in which the model is built. In forward selection, we begin with the simplest possible model (namely, \\(y = \\beta_0 + \\varepsilon\\)), and grow the model by adding predictors to it. In backwards elimination, we start with the most expansive possible model and shrink it by removing unnecessary predictors. In stepwise selection, we start with the simplest possible model (namely, \\(y = \\beta_0 + \\varepsilon\\)) and then merge forwards and backwards steps, either growing and shrinking the model until converging on one that cannot be improved. Stepwise selection is used more often than the other two variations. Each of the three procedures could possibly lead to a different best model. Here is the algorithm for forward selection: Initiation step: Start with the model \\(y=\\beta_0 +\\varepsilon\\). This is the initial “working” model. Iteration step: Fit a set of candidate models, each of which differs from the working model by the inclusion of a single additional model term. Be aware that the set of candidate models must abide our rules of thumb about hierarchy. (That is, we wouldn’t consider a model like \\(y = \\beta_0 + \\beta_1 x_1 x_2 + \\varepsilon\\).) Ask: Do any of the candidate models improve upon the working model? If the answer to this question is “yes”, then choose the model that improves upon the working model the most. Making this model the working model, and begin a new iteration (return to step 2). (Termination step): If the answer to this question is “no”, then stop. The current working model is the final model. The algorithm for backwards elimination is similar: Initiation step: Start with the most expansive possible model. Usually, this will be the model with all possible predictors, and potentially with all possible first- (and conceivably second-) order interactions. Note that we can consider a quadratic term to be an interaction of a predictor with itself in this context. This is the initial “working” model. Iteration step: Fit a set of candidate models, each of which differs from the working model by the elimination of a single model term. Again, be aware that the set of candidate models must abide our rules of thumb about hierarchy, so (for example) we wouldn’t consider a model that removes \\(x_1\\) if the interaction \\(x_1 x_2\\) is still in the model. (Same as forward selection.) Ask: Do any of the candidate models improve upon the working model? If the answer to this question is “yes”, then choose the model that improves upon the working model the most. Making this model the working model, and begin a new iteration (return to step 2). (Termination step): If the answer to this question is “no”, then stop. The current working model is the final model. The algorithm for stepwise selection initiates the algorithm with \\(y=\\beta_0 +\\varepsilon\\), and then forms a set of candidate models that differ from the working model by either the addition or elimination of a single model term. The algorithm proceeds until the working model cannot be improved either by a single addition or elimination. So far, we have been silent about how we determine whether a candidate model improves on the working model, and if so, how to find the candidate model that offers the most improvement. We can use any of our ranking methods at this step. Historically, \\(p\\)-values have often been used to determine whether a candidate model improves on the working model, though this practice has largely been discontinued. The argument against it is that using \\(p\\)-values for variable selection destroys their interpretation in the context of hypothesis testing. This being said, any statistical tests can only be regarded as descriptive if the tests occur in the context of a model that has been identified by model selection. Statistical tests only have their advertised properties if decisions about which predictors to include are made before looking at the data. The `step’ routine in R uses AIC as its default criterion for adding or dropping terms in stepwise selection. Here is an example of stepwise selection with the cheese data, considering only models without interactions or polynomial terms. fm0 &lt;- lm(taste ~ 1, data = cheese) # the initial model y = b0 + eps step(fm0, taste ~ Acetic + H2S + Lactic, data = cheese) ## Start: AIC=168.29 ## taste ~ 1 ## ## Df Sum of Sq RSS AIC ## + H2S 1 4376.7 3286.1 144.89 ## + Lactic 1 3800.4 3862.5 149.74 ## + Acetic 1 2314.1 5348.7 159.50 ## &lt;none&gt; 7662.9 168.29 ## ## Step: AIC=144.89 ## taste ~ H2S ## ## Df Sum of Sq RSS AIC ## + Lactic 1 617.2 2669.0 140.65 ## &lt;none&gt; 3286.1 144.89 ## + Acetic 1 84.4 3201.7 146.11 ## - H2S 1 4376.7 7662.9 168.29 ## ## Step: AIC=140.65 ## taste ~ H2S + Lactic ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 2669.0 140.65 ## + Acetic 1 0.55 2668.4 142.64 ## - Lactic 1 617.18 3286.1 144.89 ## - H2S 1 1193.52 3862.5 149.74 ## ## Call: ## lm(formula = taste ~ H2S + Lactic, data = cheese) ## ## Coefficients: ## (Intercept) H2S Lactic ## -27.592 3.946 19.887 There is no prescription for building models automatically. (If there were, someone would have written a computer package implementing the procedure and would be filthy rich.) Here is one cycle of steps for building a regression model, courtesy of Dr. Roger Woodard, formerly of NCSU: Examine univariate (ST 511) summaries of the data (summary statistics, boxplots, etc.). Identify unusual values or possible problems. (Don’t take it on faith that your data are all correct!) Examine scatterplots with all variables. Find variables that are closely correlated with the response and with each other. Candidate model selection: Identify a model that includes relevant variables. Use automated selection procedures if you wish. Assumption checking: Check (standardized) residuals. Determine if polynomial terms or transformations may be needed. Examine collinearity diagnostics. Inspect VIFs and pairwise correlations between variables. Decide if some variables may be removed or added. Revision. Add or remove terms based on steps 4-5. Return to step 3. Prediction and testing: Consider validating the model with a sample that was not included in building the model. 2.7 Leverage, influential points, and standardized residuals 2.7.1 Leverage Recall that in SLR, a data point can have undue influence on the regression model if the value of the predictor, \\(x\\), is for away from \\(\\bar{x}\\). The same notion applies in MLR: data points can be unduly influential if their combination of predictors lies far away from the “center of mass” of the other predictors. However, with multiple predictors, it is harder to detect influential points visually. The leverage of a data point is a measure of its distance from the center of mass of the predictors, and hence its influence. The formula for calculating leverages is complicated, so we’ll use a computer to calculate them. Leverages are usually denoted with the letter \\(h\\), so the leverage for the \\(i\\)th data point is \\(h_i\\). If we looked at the equation for a leverage, however, we would discover that leverages are strictly functions of the predictors, and do not depend on th response. To calculate leverages in R, first pass the regression model object to the function influence.lm. This function returns several diagnostic measures; the leverages are contained in the component called hat. To extract the leverages, use code like the following: fm1 &lt;- lm(BAC ~ weight + beers, data = beer) fm1.diagnostics &lt;- lm.influence(fm1) lev &lt;- fm1.diagnostics$hat Here is a look at some of the leverage values for the BAC data: head(cbind(beer, lev)) ## BAC weight beers beers.c weight.c lev ## 1 0.100 132 5 0.1875 -39.5625 0.1118535 ## 2 0.030 128 2 -2.8125 -43.5625 0.1948842 ## 3 0.190 110 9 4.1875 -61.5625 0.5176556 ## 4 0.120 192 8 3.1875 20.4375 0.2029871 ## 5 0.040 172 3 -1.8125 0.4375 0.1111125 ## 6 0.095 250 7 2.1875 78.4375 0.2588899 Not surprisingly, the point with the greatest leverage is the 110-lb person who drank 9 beers. What qualifies as a large leverage? It can be shown that, in a regression model with \\(k\\) predictors, the total of the leverages for all the data points must equal \\(k+1\\). Therefore, a typical leverage value will be \\((k+1)/n\\). 2.7.2 Standardized residuals Data points associated with large leverages tend to have smaller (raw) residuals. A better choice than analyzing the raw residuals is to analyze the standardized residuals. The formula for a standardized residual is: \\[ e_i^{(s)} =\\frac{e_i}{s_\\varepsilon \\sqrt{1-h_i} } \\] The benefit of standardized residuals is that if our model assumptions are appropriate, then they should behave like an iid sample from a normal distribution with mean 0 and variance 1. That is to say, most standardized residuals should be between -2 and +2, and only a few should be \\(&lt; -3\\) or \\(&gt; +3\\). Some texts call these “studentized residuals” instead of standardized residuals. R does not have a built-in function for calculating standardized residuals. However, there is a library of functions called the MASS library that contains the function stdres. (MASS is an acronym for , which is one of the original advanced texts for using R. It is written by W.N. Venables and B.D. Ripley.) MASS::stdres(fm1) ## 1 2 3 4 5 6 7 ## 0.8307561 -0.3611695 1.4198337 -1.0767727 0.2664003 0.6708175 1.6189097 ## 8 9 10 11 12 13 14 ## -1.6125272 -1.6623992 1.2179975 -0.2650284 0.1241508 -0.8509379 -0.0485307 ## 15 16 ## 1.0854287 -0.6259662 2.7.3 Cook’s distance Leverages and standardized residuals can be combined into various quantities that measure the influence each observation has on the fitted regression line. One of the most popular of these are Cook’s distance, \\(D_i\\). In R, if you pass a regression model to the command plot, it will produce four (somewhat sophisticated) diagnostic plots. The last of these shows Cook’s distance. plot(fm1) Observations with large values of Cook’s distance merit greater scrutiny. Appendix: Regression as a linear algebra problem This section assumes familiarity with linear algebra. Ultimately, the linear statistical model (that encompasses regression and ANOVA) is a linear-algebra problem. In short, the least-squares estimates are found by projecting the data vector (an element of \\(\\mathcal{R}^n\\)) onto the linear subspace of \\(\\mathcal{R}^n\\) spanned by the predictors. In matrix notation, the regression equations for a model with \\(k\\) predictors can be written compactly as \\[ \\mathbf{Y}= \\mathbf{X}\\mathbf{\\beta}+ \\mathbf{\\epsilon} \\] where \\[ \\mathbf{Y}= \\left[\\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array}\\right], \\] \\[ \\mathbf{X}= \\left[\\begin{array}{cccc} 1 &amp; x_{11} &amp; \\cdots &amp; x_{1k} \\\\ 1 &amp; x_{21} &amp; \\cdots&amp; x_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; \\cdots &amp; x_{nk} \\end{array}\\right], \\] \\[ \\mathbf{\\beta}=\\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_k \\end{array}\\right], \\] \\[ \\mathbf{\\epsilon}=\\left[\\begin{array}{c} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{array}\\right]. \\] The most important component of the above equation is the \\(\\mathbf{X}\\) matrix, also called the design matrix. Here are the first few rows of the design matrix for the BAC regression that includes beers consumed and weight as the two predictors: \\[ \\mathbf{X}=\\left[\\begin{array}{ccc} {1} &amp; {5} &amp; {132} \\\\ {1} &amp; {2} &amp; {128} \\\\ {1} &amp; {9} &amp; {110} \\\\ {1} &amp; {8} &amp; {192} \\\\ {\\vdots } &amp; {\\vdots } &amp; {\\vdots } \\end{array}\\right] \\] The short of the long is that the vector of least-squares estimates can be found by the matrix equation \\[ \\hat{\\mathbf{\\beta}}= \\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39;\\mathbf{Y} \\] Singular, or pathological, design matrices The power and beauty of the equation \\(\\hat{\\mathbf{\\beta}}= \\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39;\\mathbf{Y}\\) is that it works for any regression or ANOVA model, not just SLR. However, the matrix inverse \\(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\) does not exist for every possible choice of \\(\\mathbf{X}\\) matrices. Roughly, there are some pathological \\(\\mathbf{X}\\) matrices for which trying to find the matrix inverse \\(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\) is equivalent to dividing by zero. For example, suppose you are studying the effect of temperature on weight gain in fish, and measure temperature in both degrees Fahrenheit and Centigrade. (Recall that one can convert between Fahrenheit and Centigrade by the equation F = (9/5) C + 32.) The design matrix might be \\[ \\mathbf{X}=\\left[\\begin{array}{ccc} {1} &amp; {5} &amp; {41} \\\\ {1} &amp; {10} &amp; {50} \\\\ {1} &amp; {15} &amp; {59} \\\\ {1} &amp; {20} &amp; {68} \\end{array}\\right] \\] where the predictor in the 2\\(^{nd}\\) column is degrees Centigrade and the predictor in the 3\\(^{rd}\\) column is degrees Fahrenheit. Let’s try to fit a regression model in R for some made up values of \\(y\\): bad.data &lt;- data.frame(y = c(2.4, 6.1, 4.4, 7.0), degC = c(5, 10, 15, 20), degF = c(41, 50, 59, 68)) summary(lm(y ~ degC + degF, data = bad.data)) ## ## Call: ## lm(formula = y ~ degC + degF, data = bad.data) ## ## Residuals: ## 1 2 3 4 ## -0.76 1.73 -1.18 0.21 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.9500 1.9378 1.006 0.420 ## degC 0.2420 0.1415 1.710 0.229 ## degF NA NA NA NA ## ## Residual standard error: 1.582 on 2 degrees of freedom ## Multiple R-squared: 0.5938, Adjusted R-squared: 0.3908 ## F-statistic: 2.924 on 1 and 2 DF, p-value: 0.2294 The NA values for the partial regression coefficient associated with degrees Fahrenheit tells us that something has gone awry. While this may seem to be cause for concern, all that is happening here is that the model contains two predictors that are perfectly correlated with one another. (Actually, \\(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\) will fail to exist if the regression contains two different weighted sums of predictors that are perfectly correlated with one another.) When regression models contain perfectly correlated predictors (or perfectly correlated weighted sums of predictors), then there is no way to separate the linear associations between each of the (combinations of) predictors and the response. Thus, the inability to find least-squares regression estimates in some cases is just the logical consequence of the fact that it is impossible to separate the effects of two perfectly correlated predictors on a single response. Unfortunately, the computer output doesn’t provide a plain-language explanation of the problem. Each software package behaves a bit differently when faced with a case where the matrix inverse \\(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\) does not exist. In R, the program lm lops off columns of the design matrix (starting from the right-most column) until it obtains a design matrix where the inverse \\(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\) does exist, and the computations can proceed. This is why the output above does not provide values for the regression coefficient associated with degrees Fahrenheit. There is nothing about degrees Fahrenheit as opposed to degrees Celsius that makes degrees Fahrenheit a problematic predictor. It is simply that the two predictors are perfectly correlated, so lm responds by deleting the last predictor given — in this case degrees Fahrenheit — and proceeding. Additional results We can derive additional results by using using the theory of multivariate normal random variables. The distributional assumptions of the regression model are encapsulated in the assumption that \\(\\mathbf{\\epsilon}\\) has a multivariate normal distribution with mean \\(\\mathbf{0}\\) (a vector of 0’s) and variance matrix \\(\\sigma^2_\\varepsilon \\mathbf{I}\\), where \\(\\mathbf{I}\\) is the (\\(n\\)-by-\\(n\\)) identity matrix. (Recall that a variance matrix includes variances on the diagonal and covariances in the off-diagonal elements. Thus, the statement \\(\\bf{\\Sigma} = \\sigma^2_\\varepsilon \\mathbf{I}\\) says that every component of \\(\\mathbf{\\epsilon}\\) has variance \\(\\sigma^2_\\varepsilon\\), and that the covariance between any two components is 0.) Or, using \\(\\mathcal{N}\\) to denote a (univariate or multivariate) normal distribution, we can write \\[ \\mathbf{\\epsilon}\\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2_\\varepsilon \\mathbf{I}). \\] It then follows that \\[ \\mathbf{Y}\\sim \\mathcal{N}(\\mathbf{X}\\mathbf{\\beta}, \\sigma^2_\\varepsilon \\mathbf{I}). \\] Let \\(\\mathrm{E}\\left[\\cdot\\right]\\) and \\(\\mbox{Var}\\left(\\cdot\\right)\\) denote the expectation (mean) and variance of a random variable, respectively. First, we can show that \\(\\hat{\\mathbf{\\beta}}\\) is an ``unbiased’’ estimate of \\(\\mathbf{\\beta}\\), using the linearity of expectations: \\[\\begin{eqnarray*} \\mathrm{E}\\left[\\hat{\\mathbf{\\beta}}\\right] &amp; = &amp; \\mathrm{E}\\left[\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39;\\mathbf{Y}\\right] \\\\ &amp; = &amp; \\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39;\\mathrm{E}\\left[\\mathbf{Y}\\right] \\hspace{0.5in} \\mbox{(linearity)}\\\\ &amp; = &amp; \\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39; \\mathbf{X}\\mathbf{\\beta}\\\\ &amp; = &amp; \\mathbf{\\beta}. \\end{eqnarray*}\\] Next, we can find the variance of \\(\\hat{\\mathbf{\\beta}}\\) using a result for the variance of linear combinations of random variables: \\[\\begin{eqnarray*} \\mbox{Var}\\left(\\hat{\\mathbf{\\beta}}\\right) &amp; = &amp; \\mbox{Var}\\left(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39;\\mathbf{Y}\\right) \\\\ &amp; = &amp; \\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39; \\mbox{Var}\\left(\\mathbf{Y}\\right) \\mathbf{X}\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\\\ &amp; = &amp; \\sigma^2_{\\varepsilon} \\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39; \\mathbf{X}\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\\\ &amp; = &amp; \\sigma^2_{\\varepsilon} \\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}. \\end{eqnarray*}\\] The second equality above is a quadratic form, and uses the fact that \\(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\) is symmetric (and thus equal to its transpose). The final result, \\(\\mbox{Var}\\left(\\hat{\\mathbf{\\beta}}\\right) = \\sigma^2_{\\varepsilon} \\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\), shows that the variances of the least squares estimates (and thus their standard errors) are proportional to the diagonal elements of \\(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\). This result will become important in multiple regression when we discuss multicollinearity. Finally, let \\(\\mathbf{\\hat{Y}}\\) be a vector of fitted values, i.e., \\(\\mathbf{\\hat{Y}}= \\left[ \\hat{y}_1, \\hat{y_2}, \\ldots, \\hat{y_n} \\right]&#39;\\). We can find an experssion for \\(\\mathbf{\\hat{Y}}\\) simply as: \\[\\begin{eqnarray*} \\mathbf{\\hat{Y}}&amp; = &amp; \\mathbf{X}\\hat{\\mathbf{\\beta}}\\\\ &amp; = &amp; \\mathbf{X}\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39; \\mathbf{Y} \\end{eqnarray*}\\] The matrix \\(\\mathbf{X}\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39;\\) (sometimes called the hat matrix, because it maps \\(\\mathbf{Y}\\) to \\(\\mathbf{\\hat{Y}}\\)) is a projection matrix, and thus the fitted values are the orthogonal projection of \\(\\mathbf{Y}\\) onto the columnspace of \\(\\mathbf{X}\\). The right-triangle that results from the vectors \\(\\mathbf{Y}\\) (the hypotenuse), \\(\\mathbf{\\hat{Y}}\\), and \\(\\mathbf{e}= \\mathbf{Y}- \\mathbf{\\hat{Y}}\\) gives rise to the sum-of-squares decomposition behind \\(R^2\\). There are many good texts that present the linear-algebra formulation of the linear statistical model, including Sen and Srivastava (1997) and Monahan (2008). Bibliography "],["non-linear-regression-models.html", "Chapter 3 Non-linear regression models 3.1 Polynomial regression 3.2 Non-linear least squares 3.3 \\(^\\star\\)Smoothing methods", " Chapter 3 Non-linear regression models In this chapter, we examine several methods for characterizing non-linear associations between a predictor variable and the response. To keep things simple, we return to focusing on settings with a single predictor. However, the ideas in this chapter can readily be incorporated into models with several predictor variables. 3.1 Polynomial regression Polynomial regression uses the machinery of multiple regression to model non-linear relationships. A \\(k^{th}\\) order polynomial regression model is \\[ y=\\beta_0 +\\beta_1 x+\\beta_2 x^2 +\\beta_3 x^{3} +\\ldots +\\beta_k x^{k} +\\varepsilon \\] where the error term is subject to the standard regression assumptions. In practice, the most commonly used models are quadratic (\\(k=2\\)) and cubic (\\(k=3\\)) polynomials. Before proceeding, a historical note is worthwhile. It used to be that polynomial regression was the only way to accommodate non-linear relationships in regression models. In the present day, non-linear least squares allows us to fit a much richer set of non-linear models to data. However, in complex models (especially complex ANOVA models for designed experiments), there are still cases where it is easier to add a quadratic term to accommodate a non-linear association than it is to adopt the machinery of non-linear least squares. Thus, it is still worthwhile to know a little bit about polynomial regression, but don’t shoehorn every non-linear association into a polynomial regression if an alternative non-linear model is more suitable. Example. In the cars data, the relationship between highway mpg and vehicle weight is clearly non-linear: cars &lt;- read.table(&quot;data/cars.txt&quot;, head = T, stringsAsFactors = T) with(cars, plot(mpghw ~ weight, xlab = &quot;Vehicle weight (lbs)&quot;, ylab = &quot;Highway mpg&quot;)) To fit a quadratic model, we could manually create a predictor equal to weight-squared. Or, in R, we could create the weight-squared predictor within the call to “lm” by using the following syntax: quad &lt;- lm(mpghw ~ weight + I(weight^2), data = cars) summary(quad) ## ## Call: ## lm(formula = mpghw ~ weight + I(weight^2), data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.4386 -1.8216 0.1789 2.3617 7.5031 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.189e+01 6.332e+00 14.511 &lt; 2e-16 *** ## weight -2.293e-02 3.119e-03 -7.353 1.64e-11 *** ## I(weight^2) 1.848e-06 3.739e-07 4.942 2.24e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.454 on 136 degrees of freedom ## Multiple R-squared: 0.7634, Adjusted R-squared: 0.7599 ## F-statistic: 219.4 on 2 and 136 DF, p-value: &lt; 2.2e-16 In the quadratic regression \\(y=\\beta_0 +\\beta_1 x+\\beta_2 x^2 +\\varepsilon\\), the test of \\(H_0\\): \\(\\beta_2=0\\) vs. \\(H_a\\): \\(\\beta_2 \\ne 0\\) is tantamount to a test of whether the quadratic model provides a significantly better fit than the linear model. In this case, we can conclusively reject \\(H_0\\): \\(\\beta_2=0\\) in favor of \\(H_a\\): \\(\\beta_2 \\ne 0\\) , and thus conclude that the quadratic model provides a significantly better fit than the linear model. However, in the context of the quadratic model, the test of \\(H_0\\): \\(\\beta_1=0\\) vs. \\(H_a\\): \\(\\beta_1 \\ne 0\\) doesn’t give us much useful information. In the context of the quadratic model, the null hypothesis \\(H_0\\): \\(\\beta_1=0\\) is equivalent to the model \\(y=\\beta_0 +\\beta_2 x^2 +\\varepsilon\\). This is a strange model, and there is no reason why we should consider it. Thus, we disregard the inference for \\(\\beta_1\\), and (by similar logic) we disregard the inference for \\(\\beta_0\\) as well. If a quadratic model is good, will the cubic model \\(y=\\beta_0 +\\beta_1 x+\\beta_2 x^2 +\\beta_3 x^{3} +\\varepsilon\\) be even better? Let’s see: cubic &lt;- lm(mpghw ~ weight + I(weight^2) + I(weight^3), data = cars) summary(cubic) ## ## Call: ## lm(formula = mpghw ~ weight + I(weight^2) + I(weight^3), data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.247 -1.759 0.281 2.411 7.225 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.164e+02 2.697e+01 4.318 3.03e-05 *** ## weight -4.175e-02 2.033e-02 -2.054 0.042 * ## I(weight^2) 6.504e-06 4.984e-06 1.305 0.194 ## I(weight^3) -3.715e-10 3.966e-10 -0.937 0.351 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.456 on 135 degrees of freedom ## Multiple R-squared: 0.7649, Adjusted R-squared: 0.7597 ## F-statistic: 146.4 on 3 and 135 DF, p-value: &lt; 2.2e-16 In the cubic model, the test of \\(H_0\\): \\(\\beta_3=0\\) vs. \\(H_a\\): \\(\\beta_3 \\ne 0\\) is tantamount to a test of whether the cubic model provides a significantly better fit than the quadratic model. The \\(p\\)-value associated with the cubic term suggests that the cubic model does not provide a statistically significant improvement in fit compared to the quadratic model. At this point, you might wonder if we are limited only to comparing models of adjacent orders, that is, quadratic vs. linear, cubic vs. quadratic, etc. The answer is no — we can, for example, test whether a cubic model provides a significantly better fit than a linear model. To do so, we would have to test \\(H_0\\): \\(\\beta_2 = \\beta_3 = 0\\) in the cubic model. We can test this null hypothesis with an \\(F\\)-test. Even though as cubic model does not offer a significantly better fit than a quadratic model, we have not necessarily ruled out the possibility that a higher-order polynomial model might provide a significantly better fit. However, higher-order polynomials (beyond a cubic) are typically difficult to justify on scientific grounds, and offend our sense of parsimony. Plus, a plot of the quadratic model and the associated residuals suggest that a quadratic model captures the trend in the data well: with(cars, plot(mpghw ~ weight, xlab = &quot;Vehicle weight (lbs)&quot;, ylab = &quot;Highway mpg&quot;)) quad &lt;- with(cars, lm(mpghw ~ weight + I(weight^2))) quad.coef &lt;- as.vector(coefficients(quad)) quad.fit &lt;- function(x) quad.coef[1] + quad.coef[2] * x + quad.coef[3] * x^2 curve(quad.fit, from = min(cars$weight), to = max(cars$weight), add = TRUE, col = &quot;red&quot;) plot(x = fitted(quad), y = resid(quad), xlab = &quot;Fitted values&quot;, ylab = &quot;Residuals&quot;) abline(h = 0, lty = &quot;dashed&quot;) Therefore, the quadratic model clearly provides the best low-order polynomial fit to these data. Finally, it doesn’t make sense to consider models that include higher-order terms without lower-order terms. For example, we wouldn’t usually consider a cubic model without an intercept, or a quadratic model without a linear term. Geometrically, these models are constrained in particular ways. If such a constraint makes sense scientifically, entertaining the model may be warranted, but this situation arises only rarely. Thus, our strategy for fitting polynomial models is to choose the lowest-order model that provides a reasonable fit to the data, and whose highest-order term is statistically significant. 3.2 Non-linear least squares Today, software is readily available to fit non-linear models to data using the same least-squares criterion that we use to estimate parameters in the linear model. The computation involved in fitting a non-linear model is fundamentally different from the computation involved in a linear model. A primary difference is that there is no all-purpose formula like \\(\\hat{\\mathbf{\\beta}}=\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1} \\mathbf{X}&#39;\\mathbf{Y}\\) available for the non-linear model. Therefore, parameter estimates (and their standard errors) have to be found using a numerical algorithm. (We’ll see more about what this means in a moment.) However, these algorithms are sufficiently well developed that they now appear in most common statistical software packages, such as R, SAS, or others. In R, the command that we use to fit a non-linear model is nls, for [n]on-linear [l]east [s]quares. In SAS, non-linear models can be fit using PROC NLIN. Ex. Puromycin. This example is taken directly from the text , by D.M. Bates and D.G. Watts Bates and Watts (1988). The data themselves are from Treloar (1974, MS Thesis, Univ of Toronto), who studied the relationship between the velocity of an enzymatic reaction (the response, measured in counts / minute\\(^2\\)) vs. the concentration of a particular substrate (the predictor, measured in parts per million). The experiment was conducted in the presence of the puromycin Puromycin. The data are shown below. puromycin &lt;- read.table(&quot;data/puromycin.txt&quot;, head = T, stringsAsFactors = T) with(puromycin, plot(velocity ~ conc, xlab = &quot;concentration&quot;, ylab = &quot;velocity&quot;)) It is hypothesized that these data can be described by the Michaelis-Menten model for puromycin kinetics. The Michaelis-Menten model is: \\[ y=\\frac{\\theta_1 x}{\\theta_2 +x} +\\varepsilon \\] We continue to assume that the errors are iid normal with mean 0 and unknown but constant variance, i.e., \\(\\varepsilon_i \\sim \\mathcal{N}\\left(0,\\sigma_{\\varepsilon}^2 \\right)\\). With non-linear models, it is helpful if one can associate each of the parameters with a particular feature of the best-fitting curve. With these data, it seems that the best fitting curve is one that will increase at a decelerating rate until it approaches an asymptote. A little algebra shows that we can interpret \\(\\theta_1\\) directly as the asymptote (that is, the limiting value of the curve as \\(x\\) gets large), and \\(\\theta_2\\) as the value of the predictor at which the fitted curve reaches one-half of its asymptotic value. To estimate parameters, we can define a least-squares criterion just as before. That is to say, the least-squares estimates of \\(\\theta_1\\) and \\(\\theta_2\\) will be the values that minimize \\[ SSE=\\sum_{i=1}^ne_i^2 = \\sum_{i=1}^n\\left(y_i -\\hat{y}_i \\right)^2 =\\sum_{i=1}^n\\left(y_i -\\left[\\frac{\\hat{\\theta }_1 x_i }{\\hat{\\theta }_{2} +x_i } \\right]\\right)^2 \\] However, unlike with the linear model, there is no formula that can be solved directly to find the least-squares estimates. Instead, the least-squares estimates (and their standard errors) must be found using a numerical minimization algorithm. That is, the computer will use a routine to iteratively try different parameter values (in an intelligent manner) and proceed until it thinks it has found a set of parameter values that minimize the SSE (within a certain tolerance). While we can trust that the numerical minimization routine implemented by R or SAS is a reasonably good one, all numerical minimization routines rely critically on finding a good set of starting values for the parameters. That is, unlike in a linear model, we must initiate the algorithm with a reasonable guess of the parameter values that is in the ballpark of the least-squares estimates. Here is where it is especially beneficial to have direct interpretations of the model parameters. Based on our previous analysis, we might choose a starting values of (say) \\(\\theta_1 = 200\\) and \\(\\theta_2 = 0.1\\). (Note that R will try to find starting values if they aren’t provided. However, the documentation to nls says that these starting values are a “very cheap guess”.) Equipped with our choice of starting values, we are ready to find the least-squares estimates using nls: fm1 &lt;- nls(velocity ~ theta1 * conc / (theta2 + conc), data = puromycin, start = list(theta1 = 200, theta2 = 0.1)) summary(fm1) ## ## Formula: velocity ~ theta1 * conc/(theta2 + conc) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## theta1 2.127e+02 6.947e+00 30.615 3.24e-11 *** ## theta2 6.412e-02 8.281e-03 7.743 1.57e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.93 on 10 degrees of freedom ## ## Number of iterations to convergence: 6 ## Achieved convergence tolerance: 6.085e-06 In the call to nls, the first argument is a formula where we specify the non-linear model that we wish to fit. In this data set, “velocity” is the response and “conc” is the predictor. The last argument to nls is a list of starting values. The list contains one starting value for each parameter in the model. (In R, “lists” are like vectors, except that lists can contain things other than numbers.) The output shows that the least squares estimates are \\(\\hat{\\theta}_1 =212.7\\) and \\(\\hat{\\theta}_2 =0.064\\). We also get estimated standard errors for each of the parameters, as well as \\(t\\)-tests of \\(H_0\\): \\(\\theta =0\\) vs. \\(H_a\\): \\(\\theta \\ne 0\\). Note that the \\(t\\)-tests are not particularly useful in this case — there’s no reason why we would entertain the possibility that either \\(\\theta_1\\) or \\(\\theta_2\\) are equal to 0. The last portion of the output from nls tells us about the performance of the numerical algorithm that was used to find the least-squares estimates. We won’t delve into this information here, but if you need to use non-linear least squares for something important, be sure to acquaint yourself with what this output means. Like linear least-squares, there are cases where non-linear least squares will not work (or will not work well), and it is this portion of the output that will give you a clue when you’ve encountered one of these cases. We can examine the model fit by overlaying a fitted curve: with(puromycin, plot(velocity ~ conc, xlab = &quot;concentration&quot;, ylab = &quot;velocity&quot;)) mm.fit &lt;- function(x) (212.7 * x) / (0.06412 + x) curve(mm.fit, from = min(puromycin$conc), to = max(puromycin$conc), col = &quot;red&quot;, add = TRUE) It is instructive to compare the fit of this non-linear model with the fit from a few polynomial regressions. Neither the quadratic nor the cubic models fits very well in this case. Polynomial models often have a difficult time handling a data set with an asymptote. In this case, the Michaelis-Menten model clearly seems preferable. quad &lt;- lm(velocity ~ conc + I(conc^2), data = puromycin) cubic &lt;- lm(velocity ~ conc + I(conc^2) + I(conc^3), data = puromycin) quad.coef &lt;- as.vector(coefficients(quad)) quad.fit &lt;- function(x) quad.coef[1] + quad.coef[2] * x + quad.coef[3] * x^2 cubic.coef &lt;- as.vector(coefficients(cubic)) cubic.fit &lt;- function(x) cubic.coef[1] + cubic.coef[2] * x + cubic.coef[3] * x^2 + cubic.coef[4] * x^3 with(puromycin, plot(velocity ~ conc, xlab = &quot;concentration&quot;, ylab = &quot;velocity&quot;, ylim = c(min(velocity), 230))) curve(quad.fit, from = min(puromycin$conc), to = max(puromycin$conc), add = TRUE, col = &quot;blue&quot;) curve(cubic.fit, from = min(puromycin$conc), to = max(puromycin$conc), add = TRUE, col = &quot;forestgreen&quot;) legend(x = 0.6, y = 100, legend = c(&quot;quadratic&quot;, &quot;cubic&quot;), col = c(&quot;blue&quot;, &quot;darkgreen&quot;), lty = &quot;solid&quot;, bty = &quot;n&quot;) 3.3 \\(^\\star\\)Smoothing methods Sometimes, all we want to do is to generate a curve that characterizes the relationship between two variables, and we don’t necessarily care about describing that curve with a parameterized equation. This section describes several methods for doing so. The contents of this section are in an early stage of development. 3.3.1 Loess smoothers “Loess” is an acronym for [lo]cal regr[ess]ion. Nomenclature can be a bit frustrating with loess models. As we will see later, some versions of loess models use weighted least squares instead of ordinary least squares, and are called “lowess” models to emphasize the use of weighted least squares. However, the basic R routine for fitting lo(w)ess models is called loess, but uses the weighted least-squares fitting with its default factory settings. We will illustrate loess smoothers with the bioluminescence data found in the ISIT data set. These data can be found by visiting the webpage for the book “Mixed Effects Models and Extensions in Ecology with R” by Zuur et al. (Zuur et al. (2009)). ## download the data from the book&#39;s website isit &lt;- read.table(&quot;data/ISIT.txt&quot;, head = T) ## extract the data from station 16 st16 &lt;- subset(isit, Station == 16) ## retain just the variables that we want, and rename st16 &lt;- st16[, c(&quot;SampleDepth&quot;, &quot;Sources&quot;)] names(st16) &lt;- c(&quot;depth&quot;, &quot;sources&quot;) with(st16, plot(sources ~ depth)) Fit a loess smoother using the factory settings: st16.lo &lt;- loess(sources ~ depth, data = st16) summary(st16.lo) ## Call: ## loess(formula = sources ~ depth, data = st16) ## ## Number of Observations: 51 ## Equivalent Number of Parameters: 4.33 ## Residual Standard Error: 4.18 ## Trace of smoother matrix: 4.73 (exact) ## ## Control settings: ## span : 0.75 ## degree : 2 ## family : gaussian ## surface : interpolate cell = 0.2 ## normalize: TRUE ## parametric: FALSE ## drop.square: FALSE Plot the fit, this takes a little work depth.vals &lt;- with(st16, seq(from = min(depth), to = max(depth), length = 100)) st16.fit &lt;- predict(object = st16.lo, newdata = depth.vals, se = TRUE) with(st16, plot(sources ~ depth)) lines(x = depth.vals, y = st16.fit$fit, col = &quot;blue&quot;) # add 95% error bars lines(x = depth.vals, y = st16.fit$fit + st16.fit$se.fit * qt(p = .975, df = st16.fit$df), col = &quot;blue&quot;, lty = &quot;dashed&quot;) lines(x = depth.vals, y = st16.fit$fit - st16.fit$se.fit * qt(p = .975, df = st16.fit$df), col = &quot;blue&quot;, lty = &quot;dashed&quot;) Examine the residuals: ## see what the fit returns; maybe the residuals are already there names(st16.lo) # they are! ## [1] &quot;n&quot; &quot;fitted&quot; &quot;residuals&quot; &quot;enp&quot; &quot;s&quot; &quot;one.delta&quot; ## [7] &quot;two.delta&quot; &quot;trace.hat&quot; &quot;divisor&quot; &quot;robust&quot; &quot;pars&quot; &quot;kd&quot; ## [13] &quot;call&quot; &quot;terms&quot; &quot;xnames&quot; &quot;x&quot; &quot;y&quot; &quot;weights&quot; plot(st16.lo$residuals ~ st16$depth) abline(h = 0, lty = &quot;dotted&quot;) Let’s look at how changing the span changes the fit. We’ll write a custom function to fit a LOESS curve, and then call the function with various values for the span. PlotLoessFit &lt;- function(x, y, return.fit = FALSE, ...){ # Caluclates a loess fit with the &#39;loess&#39; function, and makes a plot # # Args: # x: predictor # y: response # return.fit: logical # ...: Optional arguments to loess # # Returns: # the loess fit my.lo &lt;- loess(y ~ x, ...) x.vals &lt;- seq(from = min(x), to = max(x), length = 100) my.fit &lt;- predict(object = my.lo, newdata = x.vals, se = TRUE) plot(x, y) lines(x = x.vals, y = my.fit$fit, col = &quot;blue&quot;) lines(x = x.vals, y = my.fit$fit + my.fit$se.fit * qt(p = .975, df = my.fit$df), col = &quot;blue&quot;, lty = &quot;dashed&quot;) lines(x = x.vals, y = my.fit$fit - my.fit$se.fit * qt(p = .975, df = my.fit$df), col = &quot;blue&quot;, lty = &quot;dashed&quot;) if (return.fit) { return(my.lo) } } Now we’ll call the function several times, each time chanigng the value of the span argument to the loess function: PlotLoessFit(x = st16$depth, y = st16$sources, span = 0.5) PlotLoessFit(x = st16$depth, y = st16$sources, span = 0.25) PlotLoessFit(x = st16$depth, y = st16$sources, span = 0.1) Let’s try a loess fit with a locally linear regression: PlotLoessFit(x = st16$depth, y = st16$sources, span = 0.25, degree = 1) 3.3.2 Splines We’ll use the gam function in the mgcv package to fit splines and additive models. The name of the package is an acronym for “Mixed GAM Computation Vehicle”. GAM is an acronym for Generalized Additive Model. Warning. I do not understand much of the functionality of mgcv::gam. What follows is my best guess of how the procedure works. The code below fits a regression spline to the bioluminescence data. Actually, the code fits an additive model with the spline as the only predictor. We will say more about additive models later. For now, it is sufficient to think about an additive model as a type of regression in which the linear effect of the predictor has been replaced by a spline. In other words, in terms of a word equation, the model can be represented as \\[ \\mbox{response = intercept + spline + error} \\] The s() component of the model formula designates a spline, and specifies details about the particular type of spline to be fit. The fx = TRUE component of the formula indicates that the amount of smoothing is fixed. The default value for the fx argument is fx = FALSE, in which case the amount of smoothing is determined by (generalized) cross-validation. When fx = TRUE, the parameter k determines the dimensionality (degree of flexibility) of the spline. Larger values of k correspond to greater flexibility, and a less smooth fit. I think that the number of knots is \\(k-4\\), such that setting \\(k=4\\) fits a familiar cubic polynomial with no knots. Setting \\(k=5\\) then fits a regression spline with one knot, etc. I have not been able to figure out where the knots are placed. In any case, we’ll fit a regression spline with two knots: library(mgcv) ## Loading required package: nlme ## This is mgcv 1.8-41. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. st16.rspline &lt;- mgcv::gam(sources ~ s(depth, k = 6, fx = TRUE), data = st16) plot(st16.rspline, se = TRUE) Note that the plot includes only the portion of the model attributable to the covariate effect. This is because we have actually fit an additive model (e.g., a GAM). The plot shows only the spline component, which thus does not include the intercept. To visualize the fit, we’ll need to do a bit more work. with(st16, plot(sources ~ depth)) st16.fit &lt;- predict(st16.rspline, newdata = data.frame(depth = depth.vals), se = TRUE) lines(x = depth.vals, y = st16.fit$fit) ## add +/- 2 SE following Zuur; this is only approximate. ## should probably use a critical value from a t-dist with n - edf df, that is, 51 - 5 = 46 df lines(x = depth.vals, y = st16.fit$fit + 2 * st16.fit$se.fit, lty = &quot;dashed&quot;) lines(x = depth.vals, y = st16.fit$fit - 2 * st16.fit$se.fit, lty = &quot;dashed&quot;) We see that this particular fit is not flexible enough to capture the trend in luminescence at low depth. Let’s take a look at the information produced by a call to summary: summary(st16.rspline) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## sources ~ s(depth, k = 6, fx = TRUE) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.4771 0.5858 21.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(depth) 5 5 122.6 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.924 Deviance explained = 93.2% ## GCV = 19.837 Scale est. = 17.503 n = 51 This summary requires a bit more explanation as well. In this GAM, the spline component of the model effectively creates a set of new predictor variables. A regression spline with \\(x\\) knots requires \\(x+3\\) new regression predictors to fit the spline. In this fit, there are two knots, so the spline requires 5 new predictor variables. Because the predictors are determined in advance with regression splines, we can use the usual theory of \\(F\\)-tests from regression to assess the statistical significance of the spline terms. In the section of the output labeled “Approximate significance of smooth terms”, we see that these 5 predictors together provide a significantly better fit than a model that does not include the spline. I believe this test is actually exact. I think that it is labeled “approximate” because the default behavior of mgcv::gam is to fit a smoothing spline, for which the test is indeed only approximate. We’ll discuss this more when we study a smoothing spline fit. Now we’ll fit and plot a smoothing spline. A smoothing spline differs from a regression spline by using generalized cross-validation to determine the appropriate smoothness. st16.spline &lt;- mgcv::gam(sources ~ s(depth), data = st16) plot(st16.spline, se = TRUE) # note that the plot does not include the intercept Again, we make a plot that includes both the points and the fit with(st16, plot(sources ~ depth)) st16.fit &lt;- predict(st16.spline, newdata = data.frame(depth = depth.vals), se = TRUE) lines(x = depth.vals, y = st16.fit$fit) ## add +/- 2 SE following Zuur; this is only approximate. ## should probably use a critical value from a t-dist with n - edf df, that is, 51 - 9.81 = 41.19 df lines(x = depth.vals, y = st16.fit$fit + 2 * st16.fit$se.fit, lty = &quot;dashed&quot;) lines(x = depth.vals, y = st16.fit$fit - 2 * st16.fit$se.fit, lty = &quot;dashed&quot;) Let’s ask for a summary: summary(st16.spline) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## sources ~ s(depth) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.4771 0.3921 31.82 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(depth) 8.813 8.99 158.2 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.966 Deviance explained = 97.2% ## GCV = 9.7081 Scale est. = 7.8402 n = 51 Note especially the edf component in the “Approximate significance of smooth terms” section. The label edf stands for effective degrees of freedom. We can think of the edf as the effective number of new predictors that have been added to the model to accommodate the spline. For a smoothing spline, the number and values of the newly created predictors are determined by fitting the model to the data. Because the predictors are calculated in this way, the usual theory of \\(F\\)-testing does not apply. This is why the \\(F\\)-test shown for the smoothing spline is labeled as “approximate”. Find the AIC for the smoothing spline fit: AIC(st16.spline) ## [1] 260.4811 Here’s a small detail. Notice that the syntax of the call to predict is slightly different when making a prediction for a loess object vs. making a prediction for a gam object (which the spline fit is). For a call to predict with a loess object, the new predictor values can be provided in the form of a vector. So, we were able to use depth.vals &lt;- with(st16, seq(from = min(depth), to = max(depth), length = 100)) st16.fit &lt;- predict(object = st16.lo, newdata = depth.vals, se = TRUE) However, for a call to predict with a gam object, the new predictor values must be provided in the form of a new data frame, with variable names that match the variables in the gam model. So, to get predicted values for the spline fit, we needed to use the more cumbersome depth.vals &lt;- with(st16, seq(from = min(depth), to = max(depth), length = 100)) st16.fit &lt;- predict(st16.spline, newdata = data.frame(depth = depth.vals), se = TRUE) 3.3.3 Generalized additive models (GAMs) Generalized additive models replace the usual linear terms that appear in multiple regression models with splines. That is, suppose we seek to model the relationship between a response \\(y\\) and two predictors, \\(x_1\\) and \\(x_2\\). A standard regression model without polynomial effects or interactions would be written as \\[ y = \\beta_0 + \\beta_1 x_1 +\\beta_2 x_2 + \\varepsilon \\] where \\(\\varepsilon\\) is assumed to be an iid Gaussian random variate with variance \\(\\sigma^2_\\varepsilon\\). This is an additive model, in the sense that the combined effects of the two predictors equal the sum of their individual effects. A generalized additive model (GAM) replaces the individual regression terms with splines. Continuing with the generic example, a GAM would instead model the effects of the two predictors as \\[ y = \\beta_0 + s(x_1) +s(x_2) + \\varepsilon \\] where \\(s(\\cdot)\\) represents a spline. We continue to assume that, conditional on the covariate effects, the responses are normally distributed with constant variance \\(\\sigma^2_\\varepsilon\\). We will illustrate additive modeling using the bird data found in Appendix A of Zuur et al. (2009). Zuur et al. report that these data originally appeared in Loyn (1987) and were featured in Quinn &amp; Keough (2002)’s text. Zuur et al. describe these data in the following way: Forest bird densities were measured in 56 forest patches in south-eastern Victoria, Australia. The aim of the study was to relate bird densities to six habitat variables; size of the forest patch, distance to the nearest patch, distance to the nearest larger patch, mean altitude of the patch, year of isolation by clearing, and an index of stock grazing history (1 = light, 5 = intensive). We first read the data and perform some light exploratory analysis and housekeeping. rm(list = ls()) require(mgcv) bird &lt;- read.table(&quot;data/Loyn.txt&quot;, head = T) summary(bird) ## Site ABUND AREA DIST ## Min. : 1.00 Min. : 1.50 Min. : 0.10 Min. : 26.0 ## 1st Qu.:14.75 1st Qu.:12.40 1st Qu.: 2.00 1st Qu.: 93.0 ## Median :28.50 Median :21.05 Median : 7.50 Median : 234.0 ## Mean :28.50 Mean :19.51 Mean : 69.27 Mean : 240.4 ## 3rd Qu.:42.25 3rd Qu.:28.30 3rd Qu.: 29.75 3rd Qu.: 333.2 ## Max. :56.00 Max. :39.60 Max. :1771.00 Max. :1427.0 ## LDIST YR.ISOL GRAZE ALT ## Min. : 26.0 Min. :1890 Min. :1.000 Min. : 60.0 ## 1st Qu.: 158.2 1st Qu.:1928 1st Qu.:2.000 1st Qu.:120.0 ## Median : 338.5 Median :1962 Median :3.000 Median :140.0 ## Mean : 733.3 Mean :1950 Mean :2.982 Mean :146.2 ## 3rd Qu.: 913.8 3rd Qu.:1966 3rd Qu.:4.000 3rd Qu.:182.5 ## Max. :4426.0 Max. :1976 Max. :5.000 Max. :260.0 # get rid of the &#39;Site&#39; variable; it is redundant with the row label bird &lt;- bird[, -1] # log-transform area, distance, ldistance, to remove right-skew bird$L.AREA &lt;- log(bird$AREA) bird$L.DIST &lt;- log(bird$DIST) bird$L.LDIST &lt;- log(bird$LDIST) # change YR.ISOL to years since isolation (study was published in 1987) bird$YR.ISOL &lt;- 1987 - bird$YR.ISOL # keep the only the variables we want bird &lt;- bird[, c(&quot;ABUND&quot;, &quot;L.AREA&quot;, &quot;L.DIST&quot;, &quot;L.LDIST&quot;, &quot;YR.ISOL&quot;, &quot;ALT&quot;, &quot;GRAZE&quot;)] summary(bird) ## ABUND L.AREA L.DIST L.LDIST ## Min. : 1.50 Min. :-2.3026 Min. :3.258 Min. :3.258 ## 1st Qu.:12.40 1st Qu.: 0.6931 1st Qu.:4.533 1st Qu.:5.064 ## Median :21.05 Median : 2.0127 Median :5.455 Median :5.824 ## Mean :19.51 Mean : 2.1459 Mean :5.102 Mean :5.859 ## 3rd Qu.:28.30 3rd Qu.: 3.3919 3rd Qu.:5.809 3rd Qu.:6.816 ## Max. :39.60 Max. : 7.4793 Max. :7.263 Max. :8.395 ## YR.ISOL ALT GRAZE ## Min. :11.00 Min. : 60.0 Min. :1.000 ## 1st Qu.:21.00 1st Qu.:120.0 1st Qu.:2.000 ## Median :24.50 Median :140.0 Median :3.000 ## Mean :37.25 Mean :146.2 Mean :2.982 ## 3rd Qu.:59.50 3rd Qu.:182.5 3rd Qu.:4.000 ## Max. :97.00 Max. :260.0 Max. :5.000 Our first attempt at a GAM will entertain smoothing splines for all of the continuous predictors in the model. We will use a linear term for GRAZE because there are too few unique values to support a smooth term: bird.gam1 &lt;- mgcv::gam(ABUND ~ s(L.AREA) + s(L.DIST) + s(L.LDIST) + s(YR.ISOL) + GRAZE + s(ALT), data = bird) summary(bird.gam1) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## ABUND ~ s(L.AREA) + s(L.DIST) + s(L.LDIST) + s(YR.ISOL) + GRAZE + ## s(ALT) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.4443 2.7798 9.153 9.42e-12 *** ## GRAZE -1.9885 0.8968 -2.217 0.0318 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(L.AREA) 2.446 3.089 12.635 3.98e-06 *** ## s(L.DIST) 3.693 4.559 0.855 0.461 ## s(L.LDIST) 1.000 1.000 0.386 0.538 ## s(YR.ISOL) 1.814 2.238 1.231 0.262 ## s(ALT) 1.000 1.000 0.629 0.432 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.72 Deviance explained = 77.6% ## GCV = 40.987 Scale est. = 32.238 n = 56 The output reports the partial regression coefficient for the lone quantitative predictor GRAZE, and approximate significance tests for the smooth terms for each of the other predictors. We can visualize these smooth terms with a call to plot: plot(bird.gam1) In the interest of time, we take a casual approach to variable selection here. We’ll drop smooth terms that are clearly not significant to obtain: bird.gam2 &lt;- mgcv::gam(ABUND ~ s(L.AREA) + GRAZE, data = bird) summary(bird.gam2) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## ABUND ~ s(L.AREA) + GRAZE ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 28.400 2.201 12.903 &lt; 2e-16 *** ## GRAZE -2.980 0.686 -4.344 6.56e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(L.AREA) 2.284 2.903 13.18 3.4e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.68 Deviance explained = 69.9% ## GCV = 39.992 Scale est. = 36.932 n = 56 plot(bird.gam2) Note that the GRAZE variable is currently treated as a numerical predictor. We’ll try fitting a model with GRAZE as a factor. First we’ll create a new variable that treats GRAZE as a factor. We’ll use the summary command to confirm that the new variable fGRAZE is indeed a factor. bird$fGRAZE &lt;- as.factor(bird$GRAZE) summary(bird) ## ABUND L.AREA L.DIST L.LDIST ## Min. : 1.50 Min. :-2.3026 Min. :3.258 Min. :3.258 ## 1st Qu.:12.40 1st Qu.: 0.6931 1st Qu.:4.533 1st Qu.:5.064 ## Median :21.05 Median : 2.0127 Median :5.455 Median :5.824 ## Mean :19.51 Mean : 2.1459 Mean :5.102 Mean :5.859 ## 3rd Qu.:28.30 3rd Qu.: 3.3919 3rd Qu.:5.809 3rd Qu.:6.816 ## Max. :39.60 Max. : 7.4793 Max. :7.263 Max. :8.395 ## YR.ISOL ALT GRAZE fGRAZE ## Min. :11.00 Min. : 60.0 Min. :1.000 1:13 ## 1st Qu.:21.00 1st Qu.:120.0 1st Qu.:2.000 2: 8 ## Median :24.50 Median :140.0 Median :3.000 3:15 ## Mean :37.25 Mean :146.2 Mean :2.982 4: 7 ## 3rd Qu.:59.50 3rd Qu.:182.5 3rd Qu.:4.000 5:13 ## Max. :97.00 Max. :260.0 Max. :5.000 Now we’ll proceed to fit the model bird.gam3 &lt;- gam(ABUND ~ s(L.AREA) + fGRAZE, data = bird) plot(bird.gam3) summary(bird.gam3) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## ABUND ~ s(L.AREA) + fGRAZE ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22.727275 1.944080 11.691 1.11e-15 *** ## fGRAZE2 0.006623 2.845343 0.002 0.998152 ## fGRAZE3 -0.660124 2.585878 -0.255 0.799592 ## fGRAZE4 -2.170994 3.050736 -0.712 0.480122 ## fGRAZE5 -11.913966 2.872911 -4.147 0.000136 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(L.AREA) 2.761 3.478 11.67 4.71e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.723 Deviance explained = 75.7% ## GCV = 37.013 Scale est. = 31.883 n = 56 To formally compare the models with GRAZE as a numerical vs. categorical predictor, we’ll have to use AIC. We can’t use an \\(F\\)-test here because we have used smoothing splines to capture the effect of L.AREA. Thus, the models are not nested. (If we had used regression splines for L.AREA, then the models would have been nested.) We can extract the AICs for these models by a simple call to the AIC function. AIC(bird.gam2) ## [1] 367.1413 AIC(bird.gam3) ## [1] 361.9655 We can see the contrasts used to incorporate the factor fGRAZE in the model by a call to contrasts: with(bird, contrasts(fGRAZE)) ## 2 3 4 5 ## 1 0 0 0 0 ## 2 1 0 0 0 ## 3 0 1 0 0 ## 4 0 0 1 0 ## 5 0 0 0 1 The output here is somewhat opaque because the levels of fGRAZE are 1, 2, \\(\\ldots\\), 5. The output of the call to contrasts shows each of the newly created indicator variables as a column. For example, the first column shows that the predictor named fGRAZE2 takes the value of 1 when the variable fGRAZE equals 2, and is 0 otherwise. Fit an additive model with only a smooth effect of L.AREA, in order to show residuals vs. GRAZE: bird.gam4 &lt;- gam(ABUND ~ s(L.AREA), data = bird) plot(x = bird$GRAZE, y = bird.gam4$residuals) abline(h = 0, lty = &quot;dashed&quot;) Both the plot and the model output suggest that the effect of grazing is primarily due to lower bird abundance in the most heavily grazed category. To conclude, we’ll conduct a formal test of whether the model with GRAZE as a factor provides a significantly better fit than the model with a linear effect of GRAZE. In this case, we have to use regression splines for the smooth effect of L.AREA. We’ll use regression “splines” without any internal knots, (which are actually not splines at all, just a cubic trend) because the effect of log area seems to be reasonably well captured by a cubic trend anyway: bird.gam5 &lt;- gam(ABUND ~ s(L.AREA, k = 4, fx = TRUE) + GRAZE, data = bird) bird.gam6 &lt;- gam(ABUND ~ s(L.AREA, k = 4, fx = TRUE) + fGRAZE, data = bird) anova(bird.gam5, bird.gam6, test = &quot;F&quot;) ## Analysis of Deviance Table ## ## Model 1: ABUND ~ s(L.AREA, k = 4, fx = TRUE) + GRAZE ## Model 2: ABUND ~ s(L.AREA, k = 4, fx = TRUE) + fGRAZE ## Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) ## 1 51 1869.0 ## 2 48 1543.1 3 325.93 3.3796 0.02565 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Both AIC and the \\(F\\)-test suggest that the model with GRAZE as a factor provides a significantly better fit than the model with a linear effect of GRAZE (\\(F_{3,48} = 3.38, p = 0.026\\)). As a final note, Zuur et al. (p.550) observe that “the non-linear L.AREA effect is mainly due to two large patches. It would be useful to sample more of this type of patch in the future.” (Note the rug plots in any of the plots of the area effect above.) Bibliography "],["bibliography.html", "Bibliography", " Bibliography "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
