# One-factor layout and the analysis of variance

```{r echo = FALSE}
setwd("C:/Users/krgross/Documents/GitHub/st512-course-notes/")
```

## One-factor Analysis of Variance (ANOVA)}

### One-factor ANOVA for testing equality of group means}

*Example.*  The data below provide calorie content for 20 randomly selected beef hotdogs, 17 randomly selected poultry hotdogs, and 17 randomly selected meat (!) hotdogs.
```{r fig.cap = "Strip chart of calorie content for hot dog data." echo = FALSE}
hotdog <- read.table("data/hotdog.txt", head = T)
with(hotdog, stripchart(calories ~ type, method = "jitter", pch = 16))
```
(Note: The above plot is an example of a strip chart.  Only left-to-right variation is meaningful.  Vertical variation within male or female groups is just random scatter added to make sure the data points do not lie directly on top of one another.)  

Let $\mu_B$, $\mu_M$ and $\mu_P$ denote the average calorie content for beef, meat and poultry hotdogs, respectively.  Suppose we are interested in asking if the data above contain evidence that $\mu_B$, $\mu_M$ and $\mu_P$ differ.  The strip plot above certainly seems to suggest that these means differ, but we seek a formal statistical procedure for evaluating the strength of evidence.  In particular, we seek a statistical test of the null hypothesis $H_0$: $\mu_B = \mu_M = \mu_P$ vs.\ the alternative hypothesis that at least two of these means differ.  The statistical procedure that will allow us to do so is a one-factor analysis of variance (ANOVA).

Before we begin, we need to develop some notation.  ANOVA notation differs somewhat from regression notation.  The first type of ANOVA that we will consider is a one-factor ANOVA, so called because there is one factor that distinguishes the groups that we are comparing.  In a one-factor ANOVA, $g$ different populations are compared.  In the hot dog example, $g = 3$.  Let $i = 1,\ldots,g$ be an index that distinguishes the different populations.  The size of the random sample drawn from population $i$ is written $n_i$. When the sample sizes are the same in every group, we say that the data are balanced, and sometimes replace the individual sample sizes $n_i$  by a common sample size $n$.  Let $n_T = \sum_{i = 1}^g n_i$  denote the total number of data points in the data set.  Let $j=1, \ldots,n_i$ be an index that distinguishes the different data points within each sample; that is, $y_{ij}$  is observation $j$ from population $i$.  Finally, let $\mu_i$ denote the population mean for group $i$.

We will also use the subscript ``+'' to indicate summation over the values of an index.  (Oelhert uses large dots instead of plus signs.)  For example, if we wanted to add together all the data points from the sample from group $i$, we could write
\[
y_{i+} = \sum_{j=1}^{n_{ij}} y_{ij}
\]
Or, if we wanted to add up all the data points in the data set, we could write
\[
y_{++} = \sum_{i=1}^g \sum_{j=1}^{n_{ij}} y_{ij}
\]
Lastly, we use bars to denote sample averages.  The sample mean from population $i$ is written
\[
\bar{y}_{i+} = \frac{1}{n_i}\sum_{j=1}^{n_{ij}} y_{ij}
\]
and the “grand mean” is written
\[
\bar{y}_{++} = \frac{1}{n_T}\sum_{i=1}^g \sum_{j=1}^{n_{ij}} y_{ij}.
\]

A word about subscripting: As we progress, the subscripting that we use will become increasingly complicated.  Remember that the basic rule for selecting a subscripting scheme is that each unique combination of subscripts must identify a unique data point.  In regression, one subscript was sufficient (i.e., $i = 1, \ldots, n$), because the value of $i$ was sufficient to specify a unique data point.  In ANOVA, we need one subscript to distinguish the different groups, and a second subscript to distinguish the individual observations within each group.  

The basic hypothesis test of interest in a one-factor ANOVA is a test of $H_0$: $\mu_1 = \mu_2 = \ldots = \mu_g$  vs.\ the alternative that at least two group means differ.  As we will see below, this is identical to a model utility test in regression with indicators, and so we shouldn't be surprised that this is an $F$-test.  In the context of ANOVA, however, it is traditional to represent this test via a sums-of-squares decomposition.  To be fluent in the language of ANOVA, it is important to understand this representation also.  We begin by partitioning the variation in the data into two pieces: one quantifying the variation among populations and a second quantifying variation within populations.  
\[
\mbox{Total variation: } SS_{Total} = \sum_{i=1}^g \sum_{j=1}^{n_{ij}} \left( y_{ij} - \bar{y}_{++} \right)^2
\]
\[
\mbox{Variation among groups: } SS_{Groups} = \sum_{i=1}^g \sum_{j=1}^{n_{ij}} \left( \bar{y}_{i+} - \bar{y}_{++} \right)^2  = \sum_{i=1}^g n_i \left( \bar{y}_{i+} - \bar{y}_{++} \right)^2
\]
\[
\mbox{Variation within groups: } SS_{Error} = \sum_{i=1}^g \sum_{j=1}^{n_{ij}} \left( y_{ij} - \bar{y}_{i+} \right)^2  
\]
Although it is not obvious, the $SS_{Groups}$ and $SS_{Error}$ add together to give $SS_{Total}$, that is,
\[
SS_{Total}= SS_{Groups} + SS_{Error}
\]
Heuristically, we want to compare the variation among groups to the variation within groups.  However, we cannot compare the $SS_{Groups}$ to the $SS_{Error}$ directly, because these sums of squares are based on a different number of free differences, or degrees of freedom.  Thus, we must standardize each sum of squares by dividing through by the number of free differences on which the sum of squares is based.

\begin{center}
\begin{tabular}{ll}
Source & df \\ \hline
Groups & $g - 1$ \\
Error & $n_T - g$ \\ \hline
Total & $n_T - 1$
\end{tabular}
\end{center}

We divide both $SS_{Groups}$ and $SS_{Error}$ by their respective df to obtain the corresponding mean squares.  Mean squares can be directly compared, and the ratio of the $MS_{Groups}$ to the $MS_{Error}$ yields our $F$-statistic:
\[
F = \frac{MS_{Groups}}{MS_{Error}} = \frac{SS_{Groups} / (g - 1)}{SS_{Error} / (n_T - g)}.
\]
Mathematically, it can be shown that if the null hypothesis is true, then $MS_{Groups} \approx MS_{Error}$, and so $F \approx 1$.  If the null is false and the alternative is true, then $MS_{Groups} > MS_{Error}$, and so $F > 1$.  Of course, both mean squares have some inherent randomness, so we measure the degree of evidence against the null by comparing the $F$ statistic to the appropriate reference distribution.  If the null is true, then the $F$ ratio has an $F$-distribution with numerator df equal to $g - 1$ and denominator df equal to $n_T - g$.  Large values of the $F$ statistic provide evidence against the null and in favor of the alternative, and so we compute the $p$-value with a one-tailed test.

It is customary to arrange all the information that goes into an $F$-test into an ANOVA table.  Although the layout of this table may differ slightly from one text to the next, the basic pieces are all the same.  One possible arrangement of an ANOVA table is:

\begin{center}
	\begin{tabular}{llllll}
		Source & df & SS & MS & $F$ & $p$\\ \hline
		Groups & $g - 1$ & $SS_{Groups}$ & $MS_{Groups}$ & $MS_{Groups} / MS_{Error}$ &  \\
		Error & $n_T - g$ & $SS_{Error}$ & $MS_{Error}$ &  &  \\ \hline
		Total & $n_T - 1$ & $SS_{Total}$ & & & 
	\end{tabular}
\end{center}

Let's compare ANOVA tables for the hot-dog calorie data from both R and SAS.  Note that although the formatting differs slightly between the two software packages, the components of the $F$-test are identical.

In R, the ANOVA table is produced by fitting a linear model using lm(), and then passing that model fit to the command anova().
```{r}
fm1 <- lm(Calories ~ Type, data = hotdog)
anova(fm1)
```
For comparison, here is output from PROC GLM in SAS (we will discuss SAS implementation below):
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
The GLM Procedure

                                        Sum of
Source                      DF         Squares     Mean Square    F Value    Pr > F
Model                        2     17692.19510      8846.09755      16.07    <.0001
Error                       51     28067.13824       550.33604
Corrected Total             53     45759.33333
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
Thankfully, we get the same output regardless of which computer package we use.  We interpret this output in the following way: There is strong evidence that the average calorie content of meat, beef, and poultry hot dogs are not all equal ($F_{2,51}=16.07$, $p<.001$).  

{\em Example 2.} We also have sodium content (in mg) available for each hotdog.  A strip-plot of the sodium data is shown below.
\begin{center}
	\includegraphics*[width=2.5in]{figures/hotdog-sodium}
\end{center}
Here is partial SAS PROC GLM output: 
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
The GLM Procedure
Dependent Variable: sodium

Sum of
Source                      DF         Squares     Mean Square    F Value    Pr > F
Model                        2      31738.7147      15869.3574       1.78    0.1793
Error                       51     455248.7853       8926.4468
Corrected Total             53     486987.5000
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
There is no evidence that the average sodium content of meat, beef, and poultry hot dogs differ ($F_{2,51}=1.78$, $p=0.18$).

\subsection{Connections between one-factor ANOVA and other statistical procedures}

{\em Regression with indicators.}   It may have occurred to you that we could also have analyzed the hot dog data by using a regression model with indicator variables.   In fact, ANOVA and regression with indicators are the same statistical model, a point which we will illustrate more explicitly now.  For the hot-dog data, let’s consider the regression model
\[
y=\beta_0 +\beta_1 x_1 +\beta_2 x_2 + \epsilon 
\] 
where $x_1$ is an indicator for beef, and $x_2$ is an indicator for meat.  That is, 
\begin{displaymath}
x_1 = \begin{cases} 1 & \texttt{type = beef} \\ 0 & \texttt{otherwise} \end{cases}
\end{displaymath}
and
\begin{displaymath}
x_2 = \begin{cases} 1 & \texttt{type = meat} \\ 0 & \texttt{otherwise.} \end{cases}
\end{displaymath}
We can ask if the data contain evidence that the different types of hotdogs have different calorie contents on average by testing $H_0$: $\beta_1 = \beta_2 = 0$ vs.\ the alternative that at least one of $\beta_1$ or $\beta_2$ does not equal 0. In regression, we know that we can execute this test with an $F$-test.  In this particular case, the reduced model would just be $y = \beta_0 + \epsilon$, and so the $F$-test that we need is the model utility test.  We'll just find this test directly in the R output.  
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
> full.model <- lm(Calories ~ beef.id + meat.id, data = hotdog)

Coefficients:
Estimate Std. Error t value Pr(>|t|)    
(Intercept)  118.765      5.690  20.874  < 2e-16 ***
beef.id       38.085      7.739   4.921 9.39e-06 ***
meat.id       39.941      8.046   4.964 8.11e-06 ***

Residual standard error: 23.46 on 51 degrees of freedom
Multiple R-squared: 0.3866,     Adjusted R-squared: 0.3626 
F-statistic: 16.07 on 2 and 51 DF,  p-value: 3.862e-06
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
Note that the test statistic and $p$-value for the model utility test are exactly the same as the test statistic and $p$-value from the ANOVA $F$-test.   

{\em Two sample $t$-test.}  One-factor ANOVA can also be thought of as a generalization of a two-sample $t$-test.   To illustrate, consider the data below, which show body temperatures for 65 randomly selected women and 65 randomly selected men.
\begin{center}
	\includegraphics*[width=2.5in]{figures/bodytemp}
\end{center}
Suppose we are interested in asking if these data provide evidence that average body temperature differs between men and women.  In ST511 or an equivalent course, you might have conducted this analysis with a two-sample $t$-test.  Here is an implementation of such a test in R.
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
> t.test(bodytemp ~ gender, var.equal = TRUE)

Two Sample $t$-test

data:  bodytemp by gender 
t = 2.2854, df = 128, p-value = 0.02393
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
The $p$-value of 0.0239 suggests that there is reasonably strong evidence of a difference in average body temperature for males and females.  Compare this output with a one-factor ANOVA analysis in SAS: 
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
The SAS System                                                                           
The GLM Procedure

Sum of
Source                      DF         Squares     Mean Square    F Value    Pr > F
Model                        1      2.71876923      2.71876923       5.22    0.0239
Error                      128     66.62615385      0.52051683
Corrected Total            129     69.34492308
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
The $p$-values are exactly the same.  Coincidence?  No!  One-factor ANOVA is an extension of the two-sample $t$-test (with a pooled variance) to more than two populations.

\subsection{Assumptions in ANOVA}

Just like regression, ANOVA is based on assumptions about the distributions of the observations within each group.  If these assumptions are badly violated, then the $F$-test is not trustworthy.  In ANOVA, the errors are assumed to be independent, normally distributed, and to have equal variance.  Because we have seen that ANOVA is tantamount to regression with indicators, it should not surprise you that these are the same assumptions that we encountered in regression.  (The linearity assumption from regression is less pertinent to ANOVA because there are no continuously valued predictors.)

As with regression, we could use the ``residuals'' $e_{ij} = y_{ij} - \bar{y}_{i+}$   to construct diagnostic plots that allow us to assess these assumptions.  With a one-factor ANOVA, however, it typically suffices just to plot the data (either using dotplots, histograms, or boxplots).  Troublesome violations of assumptions should be apparent.

If assumptions are violated, what can we do?  As with a two-sample $t$-test, there are several options.  A first option is to transform the data, just as we transformed the response in regression.  Transformations are particularly useful for right-skewed data (in which case one may use a log transformation), count data (square-root transformation), or proportion data (eg, percenteges, in which case an arcsin-square root transformation is appropriate).

{\em Example: Cloud seeding.} Fifty-two clouds were randomly divided into two equally sized groups.  The 26 clouds in one group were seeded with silver nitrate, and the 26 clouds in the other group were left as controls.  Rainfall from each cloud was recorded in acre-feet.  The data are shown below.  (The original source for these data is Simpson et al., 1975.  ``A Bayesian analysis of a multiplicative treatment effect in weather modification.'' {\em Technometrics} 17:161-166.)
\begin{center}
	\includegraphics*[width=2.5in]{figures/rain-raw}
\end{center}
Clearly, the data are not normally distributed within the groups.  Taking a log-transformation of the data eliminates most of the skew:
\begin{center}
	\includegraphics*[width=2.5in]{figures/rain-log}
\end{center}
Now, a one-factor ANOVA using the log-transformed data shows that these data contain strong evidence that seeded clouds produced more rainfall, on average, than unseeded clouds:
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
The GLM Procedure

                                        Sum of
Source                      DF         Squares     Mean Square    F Value    Pr > F
Model                        1      17.0070591      17.0070591       6.47    0.0141
Error                       50     131.3526732       2.6270535
Corrected Total             51     148.3597324
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}

A second option with non-normal data is to try a non-parametric procedure.  A parametric method assumes that the data come from a population with a (so-called) parametric probability distribution.  A non-parametric method does not make any particular assumption about the distribution of the population(s) from which the data were collected.  The non-parametric equivalent to a one-factor ANOVA is the Kruskal-Wallis test.

\subsection{Design of experiments (DoE) and the one-factor ANOVA model}

To this point, we have been blurring the distinction between observational and experimental studies.  Now we make this distinction explicit.   Observational studies entail sampling from and comparing pre-existing populations (e.g., sampling vegetation in different community types, comparing smokers vs. non-smokers).  Observational studies are contrasted to experimental studies that assign subjects to one of two (or more) treatments and compare outcomes among treatments (e.g., feeding livestock different diets, clinical trials that give some patients treatments and others a placebo, etc.)

Often, we use the same statistical methodology to analyze data from observational and experimental studies.  However, the distinction between observational and experimental studies dictates the interpretation of these analyses.  Observational studies only allow us to make inferences about association.  Experimental studies allow us to make inferences about causation.

We will now develop a vocabulary that allows us to discuss the different types of designs used for experimental studies.  Here are some basic definitions. In an experimental study, the experimental unit (EU) is the physical entity to which the treatment is assigned. The measurement unit (MU) is the physical entity on which a measurement is taken.  The measurement unit is often the same as the EU, but not always.  Experimental error is the variation in the response among EUs assigned to the same treatment.

An experiment can be classified by three aspects:
\begin{enumerate}
	\item The treatment structure of an experimental design describes how the different levels of the factors in an experiment are combined to generate different treatments.
	\item  The randomization structure describes how different treatments are assigned to EUs.
	\item Replication describes how many EUs receive each individual treatment.  Replication is necessary to distinguish treatment effects from experimental error.
\end{enumerate}
The simplest randomization structure is a completely randomized design (CRD), in which treatments are randomly assigned to EUs.  CRDs are appropriate when EUs do not differ in ways that are known to affect the response.  Another way to say this is that the EUs are homogeneous.  When the EUs do differ in ways known to affect the response, there are other randomization structures that are preferable to a CRD.  A balanced CRD is one in which the same number of replicates are used for each treatment.

The simplest type of treatment structure is a one-factor classification (also known as a one-factor layout, or single-factor design).  This is a treatment structure where there is a single (categorical) treatment factor with 2 or more levels of that factor.  A one-factor layout with a CRD is a design in which the different levels of the experimental treatment are assigned randomly to EUs.  The one-factor ANOVA is the appropriate statistical model for analyzing data from a one-factor layout with a CRD.

{\em Example: Reading data}  Here is an example that shows how a one-factor ANOVA can be used to analyze the data from a one-factor classification with a CRD.  In this example, 66 students were randomly assigned to one of 3 different methods of teaching reading in a balanced CRD.  Reading comprehension was tested after students received instruction.  These data are from Moore \& McCabe, {\em Introduction to the Practice of Statistics} (1989).
\begin{center}
	\includegraphics*[width=2.5in]{figures/reading}
\end{center}
A one-factor ANOVA analysis (output from R shown below) suggests that there is strong evidence that teaching method affects reading comprehension ($F_{2,63}=4.48$, $p=0.015$).
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
> fm3 <- lm(POST3 ~ Group)
> anova(fm3)
Analysis of Variance Table

Response: POST3
          Df  Sum Sq Mean Sq F value  Pr(>F)  
Group      2  357.30  178.65  4.4811 0.01515 *
Residuals 63 2511.68   39.87
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}

\subsection{Linear contrasts: The basics}

If we reject the null hypothesis that all group means are equal in a one-factor ANOVA, we usually want to go further and characterize how the group means differ.  The approach that we use for characterizing differences among group means hinges on whether we wish to pursue pre-planned comparisons of the group means, or if we wish to pursue comparisons that are suggested by the data.  The distinction here is that a pre-planned comparison is one that you planned to investigate before having collected the data.  Pre-planned comparisons can be analyzed using linear contrasts, which are the topic of this section.  On the other hand, comparisons suggested by the data should be analyzed with multiple-comparisons procedures, which we will study later.

For pre-planned comparisons, the appropriate tools are linear contrasts.  A linear contrast generalizes the idea of a pairwise difference between two means.  A linear contrast is a special type of linear combination.  A linear combination of the group means is any expression that can be written as
\[
\theta = w_1 \mu_1 + w_2 \mu_2 + \ldots + w_g \mu_g.
\]
We can treat a linear combination just like any other statistical parameter.  A linear contrast is a special case of a linear combination in which the $w$ coefficients add to 0:
\[
\sum_{i=1}^g w_i = 0.
\]

{\em Example.}  Consider the hotdog data.  Number the groups alphabetically, such that $\mu_1$, $\mu_2$, and $\mu_3$ are the average calorie content for beef, meat, and poultry hotdogs, respectively.  A linear contrast that compares the average calorie content of beef hotdogs vs. poultry hotdogs is
\[
\theta_1 = \mu_1 - \mu_3
\]
If all the group means are equal, then the true value of any contrast will be 0.  (A little algebra will verify this claim.)  For this reason, we are almost always interested in testing the null hypothesis that a contrast is equal to 0, vs. the two-sided alternative that it is not equal to 0.

We can estimate a linear contrast (or any linear combination) simply by plugging in sample means for population means.  Example:
\[
\hat{\theta}_1 = \bar{y}_{1+} - \bar{y}_{3+} = 156.9 - 118.8 = 38.1
\]
The standard error for a linear contrast (or any linear combination) is given by the formula
\[
s_{\hat{\theta}} = \sqrt{\left\{\frac{w_1^2}{n_1} + \frac{w_2^2}{n_2} + \ldots + \frac{w_g^2}{n_g} \right\} MS_{Error}}
\]
(Note: Consider the trivial linear combination $\theta = \mu_1$ and derive the formula for the standard error.  You will notice that you recover the familiar formula for the standard error of a sample mean.)  Confidence intervals for $\theta$ can be found by using critical values from a $t$-distribution with $n_T - g$ df.  (We use $n_T - g$ df because this is the number of df for the $MS_{Error}$.)  We can also test $H_0: \theta = \theta_0$ vs.\ any appropriate alternative hypothesis by calculating the $t$-statistic
\[
t = \frac{\hat{\theta} - \theta_0}{s_{\hat{\theta}}}
\]  
and comparing the value of the test statistic to a $t$-distribution with $n_T - g$ df.

{\em Example:} Suppose we wanted to investigate the difference between the average calorie content of beef hotdogs and the average calorie content of poultry hotdogs.  We have already expressed this difference as $\theta_1 = \mu_1 - \mu_3$, and estimated this difference as $\hat{\theta}_1 = 38.1$.  Now, to draw statistical inferences, we calculate the standard error of $\hat{\theta}_1$:
\[
s_{\hat{\theta}_1} = \sqrt{\left\{\frac{1^2}{20} + \frac{0^2}{17} + \frac{(-1)^2}{17} \right\} 550.3} =7.74.
\]
By finding the appropriate critical value from a $t$-distribution with $n_T - g = 54 - 3 = 51$ df, a 95\% CI for $\theta_1$  is $38.1 \pm 2.008 \times 7.74 = (22.6, 53.6)$.  Finally, to test $H_0: \theta_1 = 0$ vs.\ $H_0: \theta_1 \ne 0$, calculate the $t$-statistic
\[
t = \frac{38.1 - 0}{7.74} = 4.92
\]
and compare to a $t$-distribution with 51 df to generate a $p$-value of $9.4 \times 10^{-6}$.  Thus, there is overwhelming evidence that beef and poultry hotdogs differ in their average calorie content.

\subsection{Using SAS: The effects parameterization of the one-factor ANOVA}

We will see that we can use SAS PROC GLM to do calculations for linear contrasts.  In preparation for doing so, we must learn about the model parameterization that PROC GLM uses for ANOVA models --- a parameterization called the ``effects parameterization'' (or ``effects model'').  This parameterization is not unique to SAS, and is commonly found in many statistics texts.  The effects parameterization seems like overkill for a one-factor ANOVA, but it will be more useful when we contemplate multi-factor ANOVA later.  

\subsubsection{Effects-model parameterization of the one-factor ANOVA model}

The effects model simply re-expresses the mean of each treatment group as the sum of an overall reference level and a unique ``effect'' for that treatment group.  That is,
\[
\mu_i = \mu + \alpha_i.
\]
Here, $\mu$ is a reference level (alternatively called an intercept), and $\alpha_i$ is the “effect” of the group $i$.  For the moment, we will be intentionally vague about what we mean by a ``reference level'' and an ``effect'', and we will clarify later.  We can go one step further and write the one-factor ANOVA model for the entire data set as
\[
y_{ij} = \mu_i + \epsilon_{ij} = \mu + \alpha_i + \epsilon_{ij}
\]
where the $\epsilon_{ij}$'s are residual errors that take the usual assumptions of normality, independence, and constant variance.  In the effects representation, the standard ANOVA null hypothesis of equality of the group means can be re-written as $H_0$: $\alpha_1 = \alpha_2 = \ldots \ \alpha_g = 0$.

The problem with the effects model is that it is {\em overparameterized}.  For example, consider the calorie data from the hot dog data set.  The average calorie content for each type of hotdog is:
\begin{eqnarray*}
	\bar{y}_{1+} & = & 156.9 \mbox{   beef}\\
	\bar{y}_{2+} & = & 158.7 \mbox{   meat}\\
	\bar{y}_{3+} & = & 118.8 \mbox{   poultry.}\\
\end{eqnarray*}
In the effects model, there is no unique way to estimate both the reference level and the individual effect parameters for each hot dog type.  For example, we could choose 
\[
\hat{\mu} = 0, \ \ \hat{\alpha}_1 = 156.9, \ \ \hat{\alpha_2} = 158.7, \ \ \hat{\alpha_3} = 118.8
\]
or we could choose \\
\[
\hat{\mu} = 100, \ \ \hat{\alpha}_1 = 56.9, \ \ \hat{\alpha_2} = 58.7, \ \  \hat{\alpha_3} = 18.8.
\]
There is no way to discriminate between these two (or many other) possibilities.  Thus, there is no unique set of best parameter estimates in the effects model.  In order to estimate the parameters in the effects model, we need to impose a constraint.  There are two commonly used constraints.  The first is the sum-to-zero constraint:
\[
\sum_{i=1}^g \alpha_i = 0.
\]
Under the sum-to-zero constraint, our parameter estimates for the hotdog example are:
\[
\hat{\mu} = 144.8, \ \ \hat{\alpha}_1 = 12.1, \ \ \hat{\alpha_2} = 13.9, \ \ \hat{\alpha_3} = -26.0.
\]
Under the sum-to-zero constraint, $\mu$ is the average of the individual group means, and the effects parameters are the differences between the individual group means and $\mu$.

When the data are not balanced, another choice for the sum-to-zero constraint is to set $\mu$ equal to the grand mean, where the grand mean is weighted by the differences in sample sizes among the groups:
\[
\mu = \frac{1}{n_T} \sum_{i=1}^g n_i \mu_i
\]
With this definition of $\mu$, the weighted sum of the $\alpha_i$'s will equal zero: $\sum_{i=1}^g n_i \alpha_i = 0$.  With the hotdog data, there are 20 beef hotdogs, and 17 of each of the meat and poultry hotdogs, so that under this constraint
\[
\hat{\mu} = 145.4, \hat{\alpha}_1 = 11.5, \hat{\alpha_2} = 13.3, \hat{\alpha_3} = -26.6.
\]
The second commonly used constraint is the set-to-zero constraint.  In the set-to-zero constraint, we choose one of the groups to serve as a reference and constrain its effect to be 0.  If one of the treatment groups is a control, it makes sense to constrain the effect of the control group to 0.  If there is no control group, then the choice of which effect to constrain = 0 is arbitrary. 

For the hotdog example, suppose we constrain $\alpha_3 = 0$.  The remaining parameter estimates are:
\[
\hat{\mu} = 118.8, \hat{\alpha}_1 = 38.1, \hat{\alpha_2} = 39.9, \hat{\alpha_3} = 0.
\]
The set-to-zero constraint is exactly like choosing a reference level in regression with indicators.  Under the set-to-zero constraint, $\mu$ is the average response of the reference group, and the effects parameters $\alpha_i$ are the differences between the other individual group means and the reference group.

\subsubsection{SAS implementation of the one-factor ANOVA model in PROC GLM}

SAS has several procedures (or PROCs) that can be used for a one-factor ANOVA analysis.  We'll be using PROC GLM, where GLM stands for General Linear Model.  The term ``general linear model'' encompasses both regression and ANOVA.  Thus, PROC GLM can be used for regression modeling as well. Note, however, that in statistical terminology ``general linear model'' is distinct from ``generalized linear model''.  The latter is a term that refers specifically to models with non-normal errors (such as logistic regression), and is a type of model that we will encounter later in ST512.

At a minimum, PROC GLM code for a one-factor ANOVA model requires two components.  First, a CLASS statement is used to declare which of the variables in the data set are categorical variables.  Second, a MODEL statement is used to identify the response variable (placed on the left-hand side of the equals sign) and the categorical variable that identifies the factor that distinguishes the groups (placed on the right-hand side of the equals sign).  For example, the hot-dog data contains three variables: a categorical variable called `type' that specifies the type of hot dog, and then two response variables labeled `calorie' and `sodium'.  PROC GLM code for a one-factor ANOVA model for the calorie data is:
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
proc glm data=hotdog;
  class type; /* declare categorical variables here */
  model calorie = type;
run;
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
Note that in SAS, all lines must end with a semicolon (;).  Also, comments can be placed between the symbols /* and */.

When PROC GLM fits a one-factor ANOVA model, it uses the effects parameterization with a set-to-zero constraint.  We can ask SAS to display least-squares parameter estimates for this model by adding a SOLUTION option to the MODEL statement in PROC GLM.  Here's an example with the hotdog data:
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
proc glm;
  class type;
  model calorie = type / solution;
run;  

Dependent Variable: calorie
                                            Standard
Parameter                 Estimate             Error    t Value    Pr > |t|

Intercept              118.7647059 B      5.68970197      20.87      <.0001
type      Beef          38.0852941 B      7.73883135       4.92      <.0001
type      Meat          39.9411765 B      8.04645369       4.96      <.0001
type      Poultry        0.0000000 B       .                .         .

NOTE: The X'X matrix has been found to be singular, and a generalized inverse was used to
solve the normal equations.  Terms whose estimates are followed by the letter 'B'
are not uniquely estimable.
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
Thus, SAS has chosen 'poultry' as the reference group.  (SAS's default is to designate as the reference the group that is last alphabetically.)  The B's that appear by the parameter estimates and the NOTE that SAS provides warn us that the parameter estimates would be different under a different constraint. 

\subsubsection{Using the ESTIMATE and CONTRAST statements for linear contrasts in PROC GLM}

PROC GLM includes two facilities for linear contrasts: the ESTIMATE statement for any linear combination, and the CONTRAST statement specifically for contrasts.  The ESTIMATE statement calculates an estimate, a standard error, and a $t$-test of $H_0$: $\theta = 0$ vs.\ $H_a$: $\theta \ne 0$.  The CONTRAST statement is restricted to contrasts, but has the capacity to test several linearly independent contrasts simultaneously.  (We haven't discussed testing several contrasts simultaneously yet, though we will.)

To use either the ESTIMATE or CONTRAST statement, we must re-code our contrast in terms of the effects parameterization.  Consider the contrast $\theta_1 = \mu_1 - \mu_3$.  Re-expressing this contrast in terms of the effects parameterization yields
\begin{eqnarray*}
  \theta_1 & = & \mu_1 - \mu_3	\\
    & = & (\mu + \alpha_1) - (\mu + \alpha_3) \\
    & = & \alpha_1 - \alpha_3 
\end{eqnarray*}
Once the linear contrasts have been re-expressed in terms of the effects model, we simply pass the coefficients of the effects parameters to SAS in the ESTIMATE or CONTRAST statement.  In the examples above, the only terms included are effects associated with the classification factor 'type'.  (This will get more complicated for multi-factor ANOVA models.)  Here is an example of how we might use ESTIMATE statements for $\theta_1$:
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
proc glm data = hotdog; 
  class type;
  model calorie = type;
  estimate 'Beef vs. Poultry' type 1 0 -1 / e;
run;

The GLM Procedure

Dependent Variable: calorie

                                             Standard
Parameter                    Estimate           Error    t Value    Pr > |t|
Beef vs. Poultry           38.0852941      7.73883135       4.92      <.0001

Coefficients for Estimate Beef vs. Poultry

Intercept                    0

type      Beef               1
type      Meat               0
type      Poul              -1
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
Notes:
\begin{enumerate}
\item The title that appears in single quotes is just a label that helps you remember which linear combination is which.
\item SAS orders the effects parameters alphabetically.  Be careful!  Check to make sure that you’ve specified the order of the coefficients correctly.  The `e' option at the end of the estimate statement provides a listing of the coefficients in tabular form, lined up against the model parameters.  This provides a good way to double check whether your code is correct.
\item For each linear combination, SAS provides a test of $H_0: \theta = 0$ vs.\ $H_0: \theta \ne 0$.  Of course, you can use the SAS output to calculate CIs or execute other hypothesis tests of interest.
\end{enumerate}
As another example, consider the linear combination $\theta_2 = \mu_1$.  Expressed in terms of the effects-model parameterization, this combination is simply $\theta_2 = \mu + \alpha_1$.  SAS calls the reference level the 'intercept', so we could find this linear combination using the ESTIMATE statement as:
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
proc glm data=hotdog;
  class type;
  model calorie = type;
  estimate 'Beef mean' intercept 1 type 1 0 0;
run;

                                            Standard
Parameter                   Estimate           Error    t Value    Pr > |t|
Beef mean                 156.850000      5.24564602      29.90      <.0001
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}

{\em Technical note:}  An unfortunate consequence of using the effects-model parameterization is that it is possible to write down linear combinations of the effects parameters that do not correspond to a linear combination of the group means.  We say that these functions are ``not estimable''.  For example, consider the function $\mu + \alpha_1 + \alpha_2$.  (An even simpler example of a function that is not estimable is just $\alpha_1$.) Even though we can write this function down as a mathematical formula, its value will depend on the constraint that we choose to estimate the model parameters.  (You can try this for yourself.  Choose two of the constraints for the hotdog data, and evaluate this function based on those constraints.  You'll see that the value of this function differs depending on the constraint that is chosen.)  However, we have said that the choice of a constraint is arbitrary.  Therefore, we are only interested in functions whose values are the same, regardless of the constraint chosen.  If you try to estimate a function that is not estimable in SAS, SAS will produce an error message.

Of course, if we start with a linear combination of group means, and then convert that combination to its representation with the effects model parameters, we’ll be guaranteed to have an estimable function.  So, it’s best to always start with the group means, and then work from the group means to the parameters of the effect model, instead of starting directly with the parameters of the effects model.

\subsection{Linear contrasts revisited: Testing multiple simultaneous contrasts}

We can do more with linear contrasts than just test whether single contrasts are equal to zero.  This discussion becomes technical, but it will pay off down the road.

{\em Technical idea 1: Linear independence.}
Several contrasts are linearly independent if they are not redundant.  Conversely, a collection of contrasts contains a linear dependence if several of the contrasts equaling 0 dictates that one of the other contrasts must equal 0.  For example, consider the hot-dog data, and consider the three contrasts
\begin{eqnarray*}
\theta_1 & = & \mu_1 - \mu_2 \\
\theta_2 & = & \mu_1 - \mu_3 \\
\theta_3 & = & \mu_2 - \mu_3 \\
\end{eqnarray*}
These contrasts are linearly dependent, because if $\theta_1 = 0$ and $\theta_2 = 0$, then it must be true that $\theta_3 = 0$.  In other words, $\theta_3$ is redundant with $\theta_1$ and $\theta_2$.  

The power of contrasts is that we are not just limited to testing one contrast at a time, but can test whether several linearly independent contrasts are simultaneously equal to 0.  For example, in the hotdog data, and using the contrasts as defined above, we can test $H_0$: $\theta_1 = \theta_2 = 0$, vs.\ the alternative that at least one of $\theta_1$ and $\theta_2$ is not 0.  Of course, $H_0$: $\theta_1 = \theta_2 = 0$ is equivalent to $H_0$: $\mu_1 = \mu_2 = \mu_3$, the usual null hypothesis of equality of group means.  Thus, contrasts give us an alternative route to understanding the ANOVA $F$-test.  This alternative route will become useful when we consider ANOVA for factorial designs later.

Suppose we have a collection of $k$ linearly independent contrasts, which we denote as $\theta_1, \theta_2, \ldots, \theta_k$.  We can use an $F$-test to test the null hypothesis that all $k$ contrasts are equal to zero.  The $F$-test involves finding a sum of squares for the collection of contrasts, which we will write as $SS(\theta_1, \theta_2, \ldots, \theta_k)$, and has $k$ df.  Unfortunately, the formula for $SS(\theta_1, \theta_2, \ldots, \theta_k)$ isn’t one that we can calculate by hand, but thankfully PROC GLM can do the computation for us.  To test the null hypothesis that all $k$ contrasts are equal to zero, we construct an $F$ test in the usual way:
\[
F = \frac{MS(\theta_1, \theta_2, \ldots, \theta_k)}{MSE} = \frac{SS(\theta_1, \theta_2, \ldots, \theta_k) / k}{MSE}
\]
In PROC GLM, we can use the CONTRAST statement to execute this $F$-test.  To do so, we must re-code all of the contrasts in terms of the effects model, and then include all the contrasts in a single CONTRAST statement.  For example, to test $H_0$: $\theta_1 = \theta_2 = 0$ with the hotdog data, we first re-write each contrast in terms of the effect parameters:
\begin{eqnarray*}
\theta_1 & = & \mu_1 - \mu_2 \\
& = & \alpha_1 - \alpha_2 \\
\theta_2 & = & \mu_1 - \mu_3 \\
& = & \alpha_1 - \alpha_3
\end{eqnarray*}
Now we include both contrasts in a single CONTRAST statement
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
proc glm data=hotdog;
  class type;
  model calorie = type;
  contrast 'B vs M, B vs P' type 1 -1 0,
                            type 1 0 -1;
run;

                                        Sum of
Source                      DF         Squares     Mean Square    F Value    Pr > F
Model                        2     17692.19510      8846.09755      16.07    <.0001
Error                       51     28067.13824       550.33604
Corrected Total             53     45759.33333


Contrast                    DF     Contrast SS     Mean Square    F Value    Pr > F
B vs M, B vs P               2     17692.19510      8846.09755      16.07    <.0001
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
The results of the test of $H_0$: $\theta_1 = \theta_2 = 0$ are included in the portion of the output labeled 'Contrast'.  Note that, of course, the $F$-test of $H_0$: $\theta_1 = \theta_2 = 0$ yields exactly the same result as the $F$-test of $H_0$: $\mu_1 = \mu_2 = \mu_3$, which should not surprise us.

Let us collect what we have learned so far.  With $g$ groups, the ANOVA $F$-test that all the group means are equal, that is, of $H_0$: $\mu_1 = \mu_2 =  \ldots = \mu_g$ is equivalent to a test of $g-1$ linearly independent contrasts among the group means.  In other words, the differences among $g$ group means are completely captured by $g-1$ linearly independent contrasts of those group means.  Note that there are many possible sets of $g-1$ linearly independent contrasts that can be used to test the equality of group means.  For example, in the hot-dog data, we could also define the contrasts
\begin{eqnarray*}
	\theta_3 & = & \mu_2 - \mu_3 \\
	\theta_4 & = & \mu_1 - \frac{\mu_2 + \mu_3}{2}.
\end{eqnarray*}
Because $\theta_3$ and $\theta_4$ are linearly independent contrasts, a test of $H_0$: $\theta_3 = \theta_4 = 0$ should also yield the same $F$-statistic, and in fact it does:
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
proc glm data=hotdog;
  class type;
  model calorie = type;
  contrast 'two more contrasts' type 0 1 -1,
                                type 1 -.5 -.5;
run;

Contrast                    DF     Contrast SS     Mean Square    F Value    Pr > F
two more contrasts           2     17692.19510      8846.09755      16.07    <.0001
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}

So what is the point, if all we have done is to recreate the familiar $F$-test for equality of means?  We will see that contrasts actually allow us to go much further.  This is illustrated in the next example.

{\em Example.} An experiment is conducted to investigate the effects of five different sugar treatments on the length (in mm) of pea sections grown in tissue culture.  The five treatments are: a control (no sugar added), +2\% sucrose, +2\% fructose, +2\% glucose, and +1\% fructose +1\% glucose.  The data are balanced, with $n = 10$ replicates per treatment.  A stripchart of the data is shown below.
\begin{center}
	\includegraphics*[width=2.5in]{figures/pea}
\end{center}
The usual one-factor ANOVA allows us to reject the null hypothesis of no difference among group means:
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
                      Sum of       Mean
Source         DF    Squares     Square    F Value    Pr > F

Model           4     14.001      3.500      49.37    <.0001
Error          45      3.190      0.071
Total          49     17.191
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
Suppose that we wanted to compare the means for the four non-control treatments.  Because there are four such treatments, we can find three linearly independent contrasts that capture the differences among these four treatments.  One possible choice of contrasts is
\begin{eqnarray*}
	\theta_1 & = & \mu_f - \mu_{gf} \\
	\theta_2 & = & \mu_g - \mu_{gf} \\
	\theta_3 & = & \mu_s - \mu_{gf} 
\end{eqnarray*}
where the subscripts indicate the particular treatment.  We can test for whether the four treatment means differ by testing $H_0$: $\theta_1 = \theta_2 = \theta_3 = 0$, which we can test using the CONTRAST statement, and include the E option to make sure we have coded the contrasts correctly:
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
proc glm data = pea;
  class trt;
  model length = trt;
  contrast 'Differences among non-controls' trt -1 1 0 0 0,
                                            trt -1 0 1 0 0,
                                            trt -1 0 0 1 0 / e;
run;

Coefficients for Contrast Differences among non-controls

                            Row 1           Row 2           Row 3

Intercept                       0               0               0

trt       1g1f                 -1              -1              -1
trt       2f                    1               0               0
trt       2g                    0               1               0
trt       2s                    0               0               1
trt       control               0               0               0


Contrast                              DF     Contrast SS     Mean Square    F Value    Pr > F
Differences among non-controls         3      3.18402000      1.06134000      14.97    <.0001
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
Suppose we are also interested in whether the sugar-addition treatments (averaged together) differ from the control.  We can test for this difference using the single df contrast:
\[
\theta_4 = \mu_c - \frac{\mu_{gf} + \mu_{f} + \mu{g} + \mu_{s}}{4}.
\]
We add this test to our PROC GLM code as follows:
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
proc glm data = pea;
  class trt;
  model length = trt;
  contrast 'Differences among non-controls' trt -1 1 0 0 0,
                                            trt -1 0 1 0 0,
                                            trt -1 0 0 1 0;
  contrast 'Control vs. non-controls' trt -0.25 -0.25 -0.25 -0.25 1 / e;
run;

Coefficients for Contrast Control vs. non-controls

Intercept                       0

trt       1g1f              -0.25
trt       2f                -0.25
trt       2g                -0.25
trt       2s                -0.25
trt       control               1


Contrast                              DF     Contrast SS     Mean Square    F Value    Pr > F
Differences among non-controls         3      3.18402000      1.06134000      14.97    <.0001
Control vs. non-controls               1     10.81683072     10.81683072     152.56    <.0001
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
Thus, we conclude that there is strong evidence that the non-control means are not all equal, and there is strong evidence that the average of the non-control groups is different from the control group.

We make two notes before moving on:
\begin{enumerate}
\item In the pea example, you might notice that $SS(\theta_1, \theta_2, \theta_3) + SS(\theta_4) = SS_{Groups}$.  This is not always guaranteed to be true, but happens in this case because the contrasts in the first group are all orthogonal to $\theta_4$.  Orthogonality of contrasts is discussed in the additional material at the end of this installment of the notes.

\item It is a bit unsatisfying to not have any sense of how the sums of squares for a set of contrasts is calculated.  The best we can do here is to point out that a test that several contrasts are simultaneously equal to zero can alternatively be formulated as an $F$-test, using the machinery of ``full'' and ``reduced'' models that we studied in the context of multiple regression.  Indeed, it is exactly the same idea, and if we wrote the ANOVA model as a regression with indicator variables, we could test a set of contrasts among the group means using exactly the same approach.  Thus, it is no surprise that the test statistic for testing several simultaneous contrasts is an $F$-statistic.
\end{enumerate}

\subsection{Multiple comparisons}
In contrast to pre-planned comparisons, when we just want to describe how population means differ, we need tools for multiple comparisons.  Multiple comparisons are a concern any time we conduct several hypothesis tests in the context of a single analysis or experiment.  We will introduce the general ideas behind multiple comparisons first, before proceeding to a specific discussion of how these ideas apply in the context of comparing means from a one-factor ANOVA.

\subsubsection{Multiple testing in general}

Recall that in the context of hypothesis testing, a Type I error occurs when we erroneously reject a true null hypothesis.  (In contrast, a Type II error occurs when we fail to reject a false null hypothesis.)   Suppose we conducted 10 independent hypothesis tests, each of which has a 5\% Type I error rate.  Suppose also that $H_0$ is true for each of these 10 tests.  Then the probability of correctly failing to reject $H_0$ on all 10 tests is $0.95^{10} = 0.60$, and so the probability of committing a Type I error on at least one test is $1- 0.60 = 0.40$.  Thus, the overall probability of falsely rejecting a true null at least once is much greater than the 5\% Type I error rate for any individual test.

This is the essence of the multiple comparisons problem.  To structure our thinking, it is helpful to define several different Type I error rates.  Oehlert (section 5.1) describes several different Type I error rates.  The {\em experimentwise} (or {\em familywise}) error rate is the probability of falsely rejecting at least one null hypothesis, if all of the null hypotheses are true.  The {\em comparisonwise} (or {\em individual}) error rate is the probability of falsely rejecting the null in a single hypothesis test.  The {\em strong familywise} error rate is the probability of falsely rejecting at least one null hypothesis, regardless of how many of the null hypotheses are actually true, and how many are actually false.  Finally, the {\em false discovery rate} (FDR) is the proportion of rejected null hypotheses that are rejected incorrectly.  Which multiple testing procedure we use depends on which error rate we want to control.  Some multiple testing procedures can be applied to any collection of hypothesis tests, while others are designed specifically for detecting significant differences among a collection of treatment means.  

Below, we will follow Oehlert’s lead and denote the error rate that we wish to control as $\mathcal{E}$. The specific error rate to which this refers will depend on context.

\subsubsection{Bonferroni and Bonferroni-like procedures}
The simplest way to control the strong familywise error rate is with a Bonferroni correction.  Suppose we intend to perform $K$ individual hypothesis tests, and want to fix the experiment-wise error rate at $\mathcal{E}$.  For each individual test, we reject $H_0$ if and only if 	
\[
p \leq \frac{\mathcal{E}}{K}.
\]
An interesting variation of the Bonferroni procedure was developed by Holm (1979).  The steps of the Holm procedure are:
\begin{enumerate}
	\item Sort the $p$-values from smallest to largest.  Use subscripts with parentheses to denote the sorted $p$-values, so that $p_{(1)} \leq p_{(2)} \leq \ldots \leq p_{(K)}$.
	\item Starting with the smallest $p$-value, reject the associated null hypothesis if
	\[
	p_{(j)} \leq \frac{\mathcal{E}}{K-j+1}.
	\]
	\item Continue until encountering a $p$-value for which the associated null hypothesis cannot be rejected.  Then stop: Neither the null hypothesis associated with this $p$-value, nor the null hypotheses associated with any larger $p$-values, are rejected.
\end{enumerate}
The Holm procedure also controls the strong familywise error rate.

Another interesting variation of the Bonferroni procedure is due to Benjamini and Hochberg (1995).  This procedure only works with independent hypotheses, and it controls the FDR.  The steps of Benjamini \& Hochberg’s FDR procedure are:
\begin{enumerate}
	\item Sort the $p$-values from smallest to largest.  Use subscripts with parentheses to denote the sorted $p$-values, so that $p_{(1)} \leq p_{(2)} \leq \ldots \leq p_{(K)}$.
	\item Find the largest $p$-value for which
	\[
	p_{(j)} \leq \frac{j\mathcal{E}}{K}.
	\]
	\item Once the largest $p$-value for which the above is true has been found, reject both the null hypothesis associated with this $p$-value, and the null hypotheses associated with all smaller $p$-values.
\end{enumerate}

\subsubsection{Multiple comparisons in ANOVA}
In the context of ANOVA, our goal is to simultaneously compare all possible pairs of means.  Thus, if there are $g$ means to be compared, then there are $K = g \times (g-1) / 2$   different comparisons to be performed.  As $g$ becomes larger, the number of comparisons grows quickly.  For example, if  $g = 3$, then there are only $K = 3$ different comparisons (ex., beef vs.\ meat, beef vs.\ poultry, and meat vs.\ poultry).  However, if $g = 10$, then there are $K = 45$ different comparisons.  

Also, in the context of ANOVA, many (but not all) multiple comparisons procedures boil down to comparing the difference between each pair of means to a minimum significant difference.  That is, two means will be declared significantly different from one another if the (absolute) difference between them equals or exceeds some minimum level.  When the data are balanced, there is one common minimum significant difference for all pairs of means.  When the data are not balanced, the minimum significant difference can itself differ for different pairs of means (because of the differences in sample sizes).  

{\em Bonferroni correction.}

To implement a Bonferroni correction, we can simply use a linear contrast for each pairwise comparison, and test the null hypothesis that the pairwise difference is equal to 0.  To control the strong familywise error rate, we would reject the null hypothesis of no difference if $p \leq \mathcal{E} / K$. 

{\em Example.}  Sokal \& Rohlff {\em Biometry} give the following data.  H.\ L.\ Willis measured the head width of tiger beetles at $g=8$ sites across the central U.S.  The data are balanced, with are $n=15$ tiger beetles measured at each site.  The data are shown below.
\begin{center}
	\includegraphics*[width=2.5in]{figures/beetle}
\end{center}
With $g=8$ groups, there are $K = (8 \times 7)/ 2 = 28$ pairwise comparisons.  Thus, for a strong familywise error rate of $\mathcal{E} = 0.05$, we could perform contrasts for all pairwise comparisons manually, and declare two group means to be different using a $p$-value threshold of $p \leq 0.05 / 28 = 0.0018$.  Alternatively, we can use the MEANS statement in PROC GLM.  In PROC GLM, the MEANS statement computes the mean for each treatment group, and then clusters the group means if a multiple comparisons procedure is specified.  For example, to perform all pairwise comparisons using a Bonferroni correction, use the BON option on the MEANS statement:
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
proc glm data=beetle;
  class site;
  model head_width = site;
  means site / bon;
run;

The GLM Procedure
Dependent Variable: head_width

                                        Sum of
Source                      DF         Squares     Mean Square    F Value    Pr > F
Model                        7      0.99607399      0.14229628       6.46    <.0001
Error                      112      2.46675560      0.02202460
Corrected Total            119      3.46282959

Bonferroni (Dunn) t Tests for head_width

NOTE: This test controls the Type I experimentwise error rate, but it generally has a higher 
Type II error rate than REGWQ.

Alpha                              0.05
Error Degrees of Freedom            112
Error Mean Square              0.022025
Critical Value of t             3.20042
Minimum Significant Difference   0.1734

Means with the same letter are not significantly different.

Bon
Grouping   Mean      N    site

A       3.88053     15    Stafford.K
A       3.77353     15    Mayfield.O
A       3.76373     15    Okeene.OK
A       3.75353     15    Kackley.KS
A       3.75247     15    Talmo.KS
A       3.73260     15    NE
A       3.70753     15    Roswell.NM
B       3.53120     15    Barnard.KS
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
PROC GLM summarizes the outcome of the multiple comparisons procedure by assigning letters to groups (A, B, ...).  Groups that do not share a letter in common have statistically distinguishable means.  Thus, we again conclude that Barnard, KS is significantly different from all the other sites, but that none of the other sites are significantly different from one another.

With balanced data, two means will be significantly different if they differ by more than a threshold difference.  PROC GLM reports this difference as the `minimum significant difference', which for the beetle data is 0.1734.  Several popular multiple comparisons procedures give rise to a minimum significant difference with balanced data, although the size of the minimum significant difference will depend on the procedure being used.

With unbalanced data, the minimum significant difference depends on the sample sizes of the two groups being compared.  Thus, SAS doesn't report a common minimum significant difference, and instead reports significance levels for each possible pairwise comparison.  We can see this with the hotdog data:
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
proc glm;
  class type;
  model calorie = type;
  means type / bon;
run;  

Bonferroni (Dunn) t Tests for calorie

NOTE: This test controls the Type I experimentwise error rate, but it generally has a
higher Type II error rate than Tukey's for all pairwise comparisons.


Alpha                        0.05
Error Degrees of Freedom       51
Error Mean Square         550.336
Critical Value of t       2.47551

Comparisons significant at the 0.05 level are indicated by ***.

                     Difference
type                    Between     Simultaneous 95%
Comparison                Means    Confidence Limits
Meat    - Beef            1.856     -17.302   21.013
Meat    - Poultry        39.941      20.022   59.860  ***
Beef    - Meat           -1.856     -21.013   17.302
Beef    - Poultry        38.085      18.928   57.243  ***
Poultry - Meat          -39.941     -59.860  -20.022  ***
Poultry - Beef          -38.085     -57.243  -18.928  ***
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
Thus, using Bonferroni, we would declare that meat and poultry hotdogs have a significantly different average calorie content, as do beef and poultry hotdogs.  However, the difference between beef and meat hotdogs is not statistically significant.

Another option for unbalanced data is to calculate a common BSD using the harmonic mean of the group sizes.  The harmonic mean is defined as
\[
\tilde{n} = \left[ \left(\frac{1}{n_1} + \frac{1}{n_2} + \cdots + \frac{1}{n_g}\right) / g \right]^{-1}.
\]
In PROC GLM, we implement this by asking for the LINES option in the MEANS statement.  Here's an example with the hotdog data:
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
proc glm;
  class type;
  model calorie = type;
  means type / bon lines;
run;

Bonferroni (Dunn) t Tests for calorie

NOTE: This test controls the Type I experimentwise error rate, but it generally has a
higher Type II error rate than REGWQ.


Alpha                              0.05
Error Degrees of Freedom             51
Error Mean Square               550.336
Critical Value of t             2.47551
Minimum Significant Difference   19.415
Harmonic Mean of Cell Sizes    17.89474

NOTE: Cell sizes are not equal.

Means with the same letter are not significantly different.

           Mean      N    type
A       158.706     17    Meat
A       156.850     20    Beef
B       118.765     17    Poultry
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
Here, the harmonic mean of the sample sizes is 17.9, resulting in a common minimum significant difference of 19.4.  In general, using a harmonic mean as a common sample size is okay if the imbalance is mild, but not if the imbalance is severe.

The advantage to a Bonferroni correction is that it is easy, quick, and broadly applicable.  The disadvantage is that it is extremely conservative, and thus has very little power to find differences between means.  Because of its extremely low power, Bonferroni is not often used in practice.

{\em Fisher's protected Least Significant Difference (LSD)}

Fisher's protected LSD is a two-stage procedure that controls the experimentwise error rate.  In the first stage, use an ANOVA $F$-test to test $H_0$: $\mu_1 = \mu_2 = \ldots = \mu_g$.  If we fail to reject $H_0$ at the $\mathcal{E}$ significance level, we will stop.  (This is the ``protection'').   If $H_0$ is rejected, then the second state entails comparing each pair of means using a two-sample $t$-test, using $MS_{Error}$ as the pooled variance estimate.

Fisher's protected LSD tends to be anti-conservative.  Here's an example with the beetle data:
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
proc glm data=beetle;
  class site;
  model head_width = site;
  means site / lsd;
run;

The GLM Procedure
t Tests (LSD) for head_width
NOTE: This test controls the Type I comparisonwise error rate, not the experimentwise error rate.

Alpha                            0.05
Error Degrees of Freedom          112
Error Mean Square            0.022025
Critical Value of t           1.98137
Least Significant Difference   0.1074

Means with the same letter are not significantly different.

t
Grouping        Mean      N    site

A            3.88053     15    Stafford.K
B    A       3.77353     15    Mayfield.O
B            3.76373     15    Okeene.OK
B            3.75353     15    Kackley.KS
B            3.75247     15    Talmo.KS
B            3.73260     15    NE
B            3.70753     15    Roswell.NM
C            3.53120     15    Barnard.KS
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
Note that PROC GLM does not actually implement the first-stage of Fisher's procedure; instead, it's the user's responsibility to stop if the overall $F$-test does not reject the null hypothesis that all group means are equal.  Note that Fisher's least significant difference (0.1074) is substantially smaller than Bonferroni's minimum significant difference, and thus a greater number of significant pairwise differences have been detected.  In general, Fisher's LSD suffers the opposite sin of Bonferroni, in that it tends to be too anti-conservative.  (If you use LSD, you tend to see differences that aren’t there.)

{\em Tukey's Honest Significant Difference (HSD)}

For balanced data, if all group means are equal, then it can be shown that the (standardized) difference between the largest sample mean and the smallest sample mean has a known statistical distribution (a so-called studentized range distribution).  The idea of Tukey's HSD is to compare the difference between each pair of sample means from a critical value of this known distribution.  If the difference between two sample means is greater than this critical value (Tukey's HSD), then we declare the means different from one another.  Tukey's HSD tends to be more conservative than Fisher's LSD, though not as conservative as Bonferroni.  Here's an example of Tukey's HSD with the beetle data:
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
proc glm data=beetle;
  class site;
  model head_width = site;
  means site / tukey;
run;

The GLM Procedure

Tukey's Studentized Range (HSD) Test for head_width

NOTE: This test controls the Type I experimentwise error rate, but it generally has a higher Type II error rate than
REGWQ.

Alpha                                   0.05
Error Degrees of Freedom                 112
Error Mean Square                   0.022025
Critical Value of Studentized Range  4.36851
Minimum Significant Difference        0.1674

Means with the same letter are not significantly different.

Tukey
Grouping        Mean      N    site

A            3.88053     15    Stafford.K
B    A       3.77353     15    Mayfield.O
B    A       3.76373     15    Okeene.OK
B    A       3.75353     15    Kackley.KS
B    A       3.75247     15    Talmo.KS
B    A       3.73260     15    NE
B            3.70753     15    Roswell.NM
C            3.53120     15    Barnard.KS
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
Note that Tukey's HSD with the beetle data is 0.1674 -- not as big a difference as Bonferroni's minimum significant difference, but bigger than Fisher's LSD.  As with Bonferroni and Fisher, a common HSD is only available when data are balanced.  When data are unbalanced, an adjustment to Tukey's method is available called the Tukey-Kramer adjustment.  Tukey-Kramer tends to be slightly conservative.

{\em Comparisons to a control: Dunnett's procedure}

Dunnett's procedure is useful when one group serves as a control, and the comparisons of interest involve comparing the remaining groups to that control.  Here is an example using the beetle data, assuming that 'Stafford.K' is the control group:
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
proc glm data=beetle;
  class site;
  model head_width = site;
  means site / dunnett('Stafford.K');
run;

The GLM Procedure

Dunnett's t Tests for head_width

NOTE: This test controls the Type I experimentwise error for comparisons of all treatments
      against a control.

Alpha                              0.05
Error Degrees of Freedom            112
Error Mean Square              0.022025
Critical Value of Dunnett's t   2.65419
Minimum Significant Difference   0.1438

Comparisons significant at the 0.05 level are indicated by ***.

                           Difference
         site                 Between     Simultaneous 95%
      Comparison                Means    Confidence Limits

Mayfield.O - Stafford.K      -0.10700    -0.25083  0.03683
Okeene.OK  - Stafford.K      -0.11680    -0.26063  0.02703
Kackley.KS - Stafford.K      -0.12700    -0.27083  0.01683
Talmo.KS   - Stafford.K      -0.12807    -0.27190  0.01577
NE         - Stafford.K      -0.14793    -0.29177 -0.00410  ***
Roswell.NM - Stafford.K      -0.17300    -0.31683 -0.02917  ***
Barnard.KS - Stafford.K      -0.34933    -0.49317 -0.20550  ***
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}

\subsection{General strategy for analyzing data from a CRD with a one-factor treatment structure}

\begin{enumerate}
	\item Plot the data.  Determine if assumptions of normality and constant variance within treatment groups are appropriate.
	\item If ANOVA assumptions are appropriate, use an $F$-test to test for differences among treatments.
	\item If the $F$-test in step 2 provides evidence of a treatment effect, use linear contrasts to investigate pre-planned comparisons, and/or use a multiple comparisons procedure to characterize differences among treatments.
	\item Report the following:
	\begin{enumerate}
		\item Sample sizes.
		\item Estimates of treatment means, and a measure of the statistical uncertainty in the treatment mean (either a standard error or a CI).
		\item Outcome of the $F$-test for a treatment effect.
		\item If the $F$-test provides evidence of a treatment effect, also report tests of contrasts or multiple comparisons.
	\end{enumerate}
\end{enumerate}

\subsection{Advanced topic: Power and sample-size determination in ANOVA}
In the course of proposing or designing an experiment, it is often helpful to have some guidance regarding how many data points are needed to obtain the statistical precision that we desire.  This sort of information comes from a power calculation.  As we’ll see, power calculations require specifying parameter values that are unknown.  (If we knew the parameter values, we wouldn’t need to do the experiment!).  Often, even having some small amount of pilot data that can be used to make educated guesses about these unknown parameters is superior to taking blind stabs at those values.  As a last resort, blind stabs are better than foregoing a power analysis altogether.

A power analysis can be focused on one of two particular questions:
\begin{enumerate}
	\item How many samples are needed for the margin of error around a group mean (or a contrast between group means) to be less than a certain desired threshold?
	\item How many samples are needed for the power to reject a false null hypothesis to be at least as big as a certain desired threshold?content...
\end{enumerate}

Here, we will focus on the second of these two questions.  For power analysis built around the margin of error, see Oehlert sections 7.2 and 7.4

The power associated with rejecting the null hypothesis that all group means are equal depends on all of the following quantities:

\begin{itemize}
\item The Type I error, $\alpha$ (i.e., the tolerable false-positive rate; higher $\alpha$ gives more power)
\item How different the group means actually are (larger differences among group means give more power)
\item The sample size (bigger sample sizes give more power)
\item The number of groups being compared, although this is often dictated by the experiment
\item The variance in observations within the same group, $\sigma^2_{\epsilon}$ (greater variation within a group reduces power)
\end{itemize}

A usual approach is to choose a tolerable Type I error rate (often 0.05 or 0.01), and to make educated guesses (ideally based on pilot data) about the actual values of the group means and the error variance , and then to calculate power from either an on-line calculator or a textbook chart.  Oehlert provides a few such charts in his Table D.10.  If values of the group means and the within-group variance are unknown, the conservative approach is to assume the smallest group-differences that one might hope to detect, and to assume a large (but still reasonable) value for the within-group.  Of course, a conservative calculation will suggest a larger sample size, but many scientists prefer to err on the side of caution.

To make sense of these charts, one needs to be able to calculate something called a non-centrality parameter.  Basically, the non-centrality parameter is a single quantity that pulls together the combined effects of the differences in the group means, the sample sizes, and the error variance on the power.  Fortunately, the non-centrality parameter is easy to calculate, even if the formula is somewhat mysterious.  NB: In the formula below, the $\alpha_i$’s refer to the effect parameters in the ANOVA model, not to the significance level. The non-centrality parameter will be the same regardless of the particular constraint that one chooses to define the effect parameters.  The formula for the non-centrality parameter is:
\[
\zeta = \dfrac{\sum_{i=1}^g n_i \alpha_i^2}{\sigma^2_{\epsilon}}
\]
As the non-centrality parameter increases, power also increases.  Thus, we see that power increases as either the sample sizes increase, the differences among the group means increase, or the error variance decreases.

\subsection{Advanced topic: Orthogonal contrasts}

We have already seen that if several contrasts are not (completely) redundant, they are called {\em linearly independent}.  Linearly independent contrasts might still share some information, however.  For example, the contrasts $\theta_1 = \mu_1 - \mu_2$ and $\theta_2 = \mu_1 - \mu_3$ are linearly independent, but they both depend in the same way on $\mu_1$.  Thus, if we happen to draw a sample from group 1 that is unusual in some regard, the event of drawing that sample will affect our tests of both $H_0$: $\theta_1 = 0$ and $H_0$: $\theta_2 = 0$ in the same way.

There is a stronger notion of independence among contrasts called {\em orthogonality}.  (You might remember that {\em orthogonal} is a synonym for {\em perpendicular}.)   Here is how orthogonality is defined.  Two contrasts $\theta_1 = \sum_{i=1}^g c_i \mu_i$ and $\theta_2 = \sum_{i=1}^g d_i \mu_i$ are orthogonal if and only if
\[
\sum_{i=1}^g \frac{c_i d_i}{n_i} = 0.
\]
Note that orthogonality is a stronger condition that linear independence.  If two contrasts are orthogonal, then they must also be linearly independent.  However, two contrasts that are linearly independent may or may not be orthogonal.\footnote{To run with the geometric idea for a bit, if we think about a contrasts as vectors in a plane, two contrasts are orthogonal if they are perpendicular.  Two contrasts are linearly independent as long as they are not parallel.  Thus, two contrasts that are orthogonal (perpendicular) are necessarily linearly independent (not parallel), but two contrasts that are linearly independent (not parallel) may or may not be orthogonal (perpendicular).}

Groups of linear contrasts can also be orthogonal.  Consider two groups of contrasts, where we denote the first group as $\theta_1, \theta_2, \ldots, \theta_k$, and the second group as $\phi_1, \phi_2, \ldots, \phi_l$.  These two groups are orthogonal if every contrast in the first group is orthogonal to every contrast in the second group.  Note that the contrasts within each group may or may not be orthogonal to each other; whether or not they are orthogonal with each other is irrelevant.  All we care about is whether the contrasts in the first group are orthogonal to the contrasts in the second group. 

The major upshot is that if two groups of contrasts are orthogonal, then their sums of squares are additive.  That is,
\[
SS(\theta_1, \theta_2, \ldots, \theta_k, \phi_1, \phi_2, \ldots, \phi_l) = SS(\theta_1, \theta_2, \ldots, \theta_k) + SS(\phi_1, \phi_2, \ldots, \phi_l).
\]
That is, the sum-of-squares for the entire collection can be partitioned into the sum-of-squares for the first group plus the sum-of-squares for the second group.

In the pea example above, note that the four contrasts $\theta_1, \theta_2, \theta_3, \theta_4$ provide four linearly independent contrasts that completely capture the differences among the 5 treatment groups.  Thus, we have $SS(\theta_1, \theta_2, \theta_3, \theta_4) = SS_{Groups}$.  Second, it turns out that the group $\theta_1$, $\theta_2$, $\theta_3$ is orthogonal to the singleton group $\theta_4$, and thus we have $SS(\theta_1, \theta_2, \theta_3) + SS(\theta_4) = SS_{Groups}$.  When the $SS_{Groups}$ can be partitioned into SS for two or more groups of orthogonal contrasts, it makes for a particularly nice touch to add this partition to our ANOVA table:
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
Source         DF    Squares     Square    F Value    Pr > F

Model           4     14.001      3.500      49.37    <.0001
  Non-controls    3      3.184    1.061      14.97    <.0001
  Ctrl vs. non    1     10.817   10.817     152.56    <.0001
Error          45      3.190      0.071
Total          49     17.191
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
Following convention, we indent the tests that partition the overall test of the treatment groups.

The natural question that arises is: When should we expect orthogonality?  One doesn't just stumble onto sets of orthogonal contrasts by chance.  There are three types of experiments where sets of orthogonal contrasts can be expected.  If you are able to recognize these types of experiments, you can use orthogonal contrasts to create an especially thorough and satisfying analysis.  The three types of experiments that support sets of orthogonal contrasts are:
\begin{enumerate}
	\item Factorial experiments.  By a large margin, factorial experiments (in which treatment groups are formed by crossing multiple experimental factors) are the most important context for orthogonal contrasts.  We will study factorial experiments in depth in the next installment of the notes, and thus will say no more about them here.
	\item Experiments in which the treatment groups cluster into a hierarchy, and the data are balanced.  The pea experiment above provides an example.  The five treatment groups naturally cluster into a group of sugar-addition treatments and a singleton control group.  In this type of experiment, we can find a set of contrasts to test for differences among the treatment means within each cluster, and another set of contrasts to test for differences among the clusters, exactly as we have done with the pea data. 
	
	As a second example, suppose that we have a balanced experiment with six treatment groups: one control group, three treatments that involve adding different types of sugars, and two treatments that involve adding different types of starches.   With 6 treatment groups, we can find 5 linearly independent contrasts to capture the differences among the treatment groups.  These 5 contrasts can be broken down into the following groups:
	\begin{itemize}
		\item A set of two linearly independent contrasts that compare the means of the three sugar treatments.
		\item A single contrast that compares the means of the two starch treatments.
		\item A set of two linearly independent contrasts that compare (i) the control group, (ii) the average of the sugar treatments, and (iii) the average of the starch treatments.
	\end{itemize}
	Alternatively, we might take the two contrasts in the last group above and break those down further into one contrast that compares the control group with the average of the five non-control groups, and a second contrast that compares the average of the sugar treatments to the average of the starch treatments.
	\item Experiments in which the treatment groups are formed by different levels of a quantitative variable.  We study this scenario in more depth below.
\end{enumerate}

\subsection{Advanced topic: Polynomial trends}

% Note: These data were in a lab, so I have taken the figure from there, without reproducing the R code to generate it.

Consider the following data set.  Pott (1992) conducted an experiment to investigate the effect of added dietary molybdenum (Mo) on the molybdenum concentrations of sheep kidneys.  Twenty (20) sheep were randomly assigned to one of four diets, where the diets differed only in the amount of supplementary molybdenum (0, 15, 30 and 45 ppm).  Five sheep were assigned to each diet, and the Mo concentration in the kidney was measured after 14 days.  The data are shown below:
\begin{center}
	\includegraphics*[width=2.5in]{figures/molybdenum}
\end{center}
Should we model these data with an ANOVA or a regression?  In this case, the treatment groups are formed by different values of a quantitative variable --- in this case, the amount of supplementary molybdenum in the diet.  Many experiments form treatment groups in this way.  Often, the data exhibit a clear trend with respect to the quantitative variable, as the sheep example does.  When this happens, we can essentially have our cake and eat it too, by using polynomial contrasts to characterize the underlying trend within the context of an ANOVA.

Our strategy is to decompose the $SS_{Groups}$ into sums-of-squares that capture the polynomial component of the underlying trend.  That is, with four treatment groups (such as we have with the sheep data), we can partition the $SS_{Groups}$ as
\[
SS_{Groups} = SS(\theta_1) + SS(\theta_2) + SS(\theta_3)
\]

where $\theta_1$ captures the linear trend in the group means, $\theta_2$ captures the quadratic trend in the group means, and $\theta_3$ captures the cubic trend in the group means.  Additionally, we want all of these contrasts to be orthogonal to one another.  

There are two different computational approaches to calculating the sums of squares for orthogonal polynomial contrasts, but they will both yield the same answer.  If the data are balanced, and if the treatment groups are equally spaced  with respect to the quantitative variable used to construct them, then coefficients for orthogonal polynomial contrasts can be found in a table, such as Table D.6 of Oehlert.  The sheep data satisfy these special conditions, so we can go ahead and use Table D.6 in Oehlert to find that the orthogonal polynomial contrasts are:
\begin{eqnarray*}
\theta_1 & = & -3 \mu_1 - \mu_2 + \mu_3 + 3 \mu_4 \\
\theta_2 & = & \mu_1 - \mu_2 - \mu_3 + \mu_4 \\
\theta_3 & = & - \mu_1 + 3 \mu_2 - 3 \mu_3 + \mu_4 
\end{eqnarray*}
where $\mu_1$, $\mu_2$, $\mu_3$, and $\mu_4$ are the means for the treatment groups with 0, 15, 30 and 45 ppm supplementary molybdenum, respectively.
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
proc glm data = sheep;
  class diet;
  model conc = diet;
  contrast 'Linear' diet -3 -1 1 3;
  contrast 'Quadratic' diet 1 -1 -1 1;
  contrast 'Cubic' diet -1 3 -3 1;
run; 

The GLM Procedure
Dependent Variable: conc

                                        Sum of
Source                      DF         Squares     Mean Square    F Value    Pr > F
Model                        3     119.6508950      39.8836317       8.62    0.0012
Error                       16      74.0601600       4.6287600
Corrected Total             19     193.7110550

Contrast                    DF     Contrast SS     Mean Square    F Value    Pr > F
Linear                       1     100.7012250     100.7012250      21.76    0.0003
Quadratic                    1      17.6156450      17.6156450       3.81    0.0688
Cubic                        1       1.3340250       1.3340250       0.29    0.5988
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
Thus we see that the linear trend is statistically significant (i.e., we can reject $H_0$: $\theta_1 = 0$), but neither the quadratic nor the cubic trends are statistically significant.

Our usual hope is that we can explain the trend in the group means parsimoniously with a small number of low-order polynomial contrasts.  In this case, it is common to pool the higher-order, non-significant contrasts together into a single sum-of-squares that we will call the ``lack-of-fit''.  With the sheep data, we can pool the quadratic and cubic contrasts together into a lack-of-fit term, as shown below:
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
proc glm data = sheep;
  class diet;
  model conc = diet;
  contrast 'Linear' diet -3 -1 1 3;
  contrast 'Lack-of-fit' diet 1 -1 -1 1,
                         diet -1 3 -3 1;
run; 

The GLM Procedure
Dependent Variable: conc

                                        Sum of
Source                      DF         Squares     Mean Square    F Value    Pr > F
Model                        3     119.6508950      39.8836317       8.62    0.0012
Error                       16      74.0601600       4.6287600
Corrected Total             19     193.7110550

Contrast                    DF     Contrast SS     Mean Square    F Value    Pr > F
Linear                       1     100.7012250     100.7012250      21.76    0.0003
Lack-of-fit                  2      18.9496700       9.4748350       2.05    0.1616
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
We conclude that there is strong evidence that the treatment groups differ, and the differences among the treatment groups are parsimoniously explained by a linear trend.  As before, it makes for a nice touch to combine this all into one ANOVA table:
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
                                        Sum of
Source                      DF         Squares     Mean Square    F Value    Pr > F
Groups                       3     119.6508950      39.8836317       8.62    0.0012
  Linear                       1     100.7012250   100.7012250      21.76    0.0003
  Lack-of-fit                  2      18.9496700     9.4748350       2.05    0.1616
Error                       16      74.0601600       4.6287600
Total                       19     193.7110550
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}

Tables of orthogonal polynomial contrasts are nice, but the requirements for balanced data and equally spaced groups seem quite restrictive.  What can we do when these conditions are not met?  It is possible to use specialized software to find the coefficients for orthogonal polynomial contrasts when the data are not balanced, and/or when the treatment groups are not equally spaced.  However, a {\em much} easier alternative is to find the sums-of-squares for each contrasts by fitting polynomial regression models.  The key here is that the sum-of-squares for a polynomial regression model is equal to the sum of the sums-of-squares for the respective polynomial contrasts.  That is, with four or more treatment groups,
\begin{eqnarray*}
	SS(\mbox{Linear regression}) & = & SS(\theta_1) \\
	SS(\mbox{Quadratic regression}) & = & SS(\theta_1) + SS(\theta_2) \\
	SS(\mbox{Cubic regression}) & = & SS(\theta_1) + SS(\theta_2) + SS(\theta_3) 
\end{eqnarray*}
and so on.  So, let's suppose we've misplaced our table of orthogonal polynomial contrasts.  We can still calculate the sum-of-squares for each polynomial contrast by fitting a series of polynomial regression models.  For example, to find $SS(\theta_1)$, we fit a linear regression to the sheep data:
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
proc glm data = sheep;
  model conc = diet;
run; 

The GLM Procedure
Dependent Variable: conc

                                        Sum of
Source                      DF         Squares     Mean Square    F Value    Pr > F
Model                        1     100.7012250     100.7012250      19.49    0.0003
Error                       18      93.0098300       5.1672128
Corrected Total             19     193.7110550
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
(Note the absence of a CLASS statement in the PROC GLM program above.  Without a CLASS statement, PROC GLM interprets `conc' as a quantitative predictor.)   To find $SS(\theta_1)$, we simply use $SS(\theta_1) = SS(\mbox{Linear regression}) = 100.70$.  Now, to find $SS(\theta_2)$, we need to fit a quadratic regression:
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
proc glm data = sheep;
  model conc = diet diet*diet;
run; 

The GLM Procedure
Dependent Variable: conc

                                        Sum of
Source                      DF         Squares     Mean Square    F Value    Pr > F
Model                        2     118.3168700      59.1584350      13.34    0.0003
Error                       17      75.3941850       4.4349521
Corrected Total             19     193.7110550
\end{verbatim}
\renewcommand{\baselinestretch}{1.5}
Now, $SS(\theta_2) = SS(\mbox{Quadratic regression}) - SS(\mbox{Linear regression}) = 118.32 - 100.70 = 17.62$.  Finally, to find $SS(\theta_3)$, we could fit a cubic regression:
\renewcommand{\baselinestretch}{1}
\begin{verbatim}
proc glm data = sheep;
  model conc = diet diet*diet diet*diet*diet;
run; 
